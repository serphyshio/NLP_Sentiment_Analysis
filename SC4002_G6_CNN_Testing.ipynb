{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["lcPlSrAjGuKV","AIVMZWdHJILR","XI7NufrKL3Hj","S2UFjASulwCb"]},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9720296,"sourceType":"datasetVersion","datasetId":5937679}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["%%capture\n","!pip install datasets\n","!pip install optuna"],"metadata":{"id":"BWnbpQJT0Vy7","execution":{"iopub.status.busy":"2024-11-04T07:24:22.310345Z","iopub.execute_input":"2024-11-04T07:24:22.310733Z","iopub.status.idle":"2024-11-04T07:24:45.708376Z","shell.execute_reply.started":"2024-11-04T07:24:22.310681Z","shell.execute_reply":"2024-11-04T07:24:45.707115Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# The value for the seed and paths\n","SEED = 168\n","PATH_TO_W2V_MODEL_DRIVE = '/content/drive/MyDrive/NLP 24-25 Sem1/GoogleNews-vectors-negative300.bin'\n","PATH_TO_W2V_MODEL_LOCAL = r'C:/Users/60178/Downloads/GoogleNews-vectors-negative300.bin'\n","PATH_TO_FASTTEXT_MODEL_DRIVE = '/content/drive/MyDrive/NLP 24-25 Sem1/cc.en.300.bin'\n","PATH_TO_FASTTEXT_MODEL_LOCAL = r'C:/Users/60178/Downloads/cc.en.300.bin/cc.en.300.bin'"],"metadata":{"id":"6ogYH4cv385k","execution":{"iopub.status.busy":"2024-11-04T07:24:45.709845Z","iopub.execute_input":"2024-11-04T07:24:45.710151Z","iopub.status.idle":"2024-11-04T07:24:45.714998Z","shell.execute_reply.started":"2024-11-04T07:24:45.710112Z","shell.execute_reply":"2024-11-04T07:24:45.714074Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Standard library imports\n","from collections import Counter\n","\n","# General third-party libraries import\n","import copy\n","from gensim.models import KeyedVectors\n","from gensim.models.fasttext import load_facebook_model\n","from datasets import load_dataset\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","import string\n","import sys\n","\n","# Import optuna for hyperparams tuning\n","import optuna\n","\n","# Import nltk\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","# Download necessary nltk resources\n","nltk.download('wordnet')\n","nltk.download('punkt')  # Ensure punkt is downloaded for tokenization\n","nltk.download('stopwords')  # Ensure stopwords are also downloaded if not already\n","\n","# Import pytorch\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch import nn\n","from torch.nn.utils.rnn import pad_sequence"],"metadata":{"id":"L5t53ajjwAIQ","executionInfo":{"status":"ok","timestamp":1729855642791,"user_tz":-480,"elapsed":7871,"user":{"displayName":"colin yifei","userId":"02963454452622984612"}},"outputId":"6ebd0d01-aabe-4d97-dffa-8aec9474cc51","execution":{"iopub.status.busy":"2024-11-04T07:24:45.717690Z","iopub.execute_input":"2024-11-04T07:24:45.718052Z","iopub.status.idle":"2024-11-04T07:24:56.564807Z","shell.execute_reply.started":"2024-11-04T07:24:45.718020Z","shell.execute_reply":"2024-11-04T07:24:56.563902Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["# Method to set random seed to ensure consistency\n","def set_seed(seed = SEED):\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","# Factory to create the dataloader\n","def dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device):\n","    # Create Tensor datasets\n","    train_dataset = TensorDataset(X_train_sequence.to(device), torch.tensor(Y_train, dtype=torch.long, device=device))\n","    val_dataset = TensorDataset(X_val_sequence.to(device), torch.tensor(Y_val, dtype=torch.long, device=device))\n","    test_dataset = TensorDataset(X_test_sequence.to(device), torch.tensor(Y_test, dtype=torch.long, device=device))\n","\n","    # Create data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","    return train_loader, val_loader, test_loader\n","\n","# Early Stopper\n","class EarlyStopper:\n","    def __init__(self, patience=5, delta=0):\n","        self.patience = patience\n","        self.delta = delta\n","        self.counter = 0\n","        self.max_validation_accuracy = 0\n","\n","    def early_stop(self, validation_accuracy):\n","        if validation_accuracy > self.max_validation_accuracy:\n","            self.max_validation_accuracy = validation_accuracy\n","            self.counter = 0\n","        elif validation_accuracy < (self.max_validation_accuracy + self.delta):\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                return True\n","        return False\n","\n","# Factory to provide required optimizer which specified parameters\n","def optimizer_factory(optimizer_name, model, learning_rate, momentum = 0.8):\n","    if optimizer_name == 'Adam':\n","        return torch.optim.Adam(model.parameters(), lr = learning_rate)\n","    if optimizer_name == 'SGD':\n","        return torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = momentum)\n","    if optimizer_name == 'RMSprop':\n","        return torch.optim.RMSprop(model.parameters(), lr = learning_rate)\n","\n","# Method to train the model\n","def train_loop(dataloader, model, loss_fn, optimizer):\n","    model.train()  # Set model to training mode\n","    train_loss = 0  # To accumulate loss for this epoch\n","\n","    for inputs, labels in dataloader:\n","        optimizer.zero_grad()  # Zero the gradients\n","        outputs = model(inputs)  # Forward pass\n","\n","        # Calculate loss\n","        loss = loss_fn(outputs.squeeze(), labels.float())  # Squeeze to match dimensions\n","        loss.backward()  # Backpropagation\n","\n","        # # Apply gradient clipping\n","        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n","\n","        optimizer.step()  # Update parameters\n","\n","        train_loss += loss.item()  # Accumulate loss\n","\n","    # Calculate average loss for the epoch\n","    train_loss /= len(dataloader)\n","    return train_loss\n","\n","# Method to evaluate the model\n","def test_loop(dataloader, model):\n","    model.eval() # Set model to evaluation mode\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():  # Disable gradient computation\n","        for inputs, labels in dataloader:\n","            outputs = model(inputs)  # Forward pass\n","            predicted = (outputs.squeeze() > 0.5).float()  # Apply threshold\n","\n","            total += labels.size(0)\n","            correct += (predicted == labels.float()).sum().item()  # Count correct predictions\n","\n","    accuracy = correct / total\n","    return accuracy\n","\n","# Method to plot the performance graph\n","def plot_performance(train_losses, val_accuracies, num_epochs):\n","    # Visualization\n","    plt.figure(figsize=(12, 5))\n","\n","    # Plotting Loss\n","    plt.subplot(1, 2, 1)\n","    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss', color='blue')\n","    plt.title('Training Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.xticks(range(1, num_epochs + 1, 10))\n","    plt.legend()\n","\n","    # Plotting Accuracy\n","    plt.subplot(1, 2, 2)\n","    plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='orange')\n","    plt.title('Validation Accuracy')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.xticks(range(1, num_epochs + 1, 10))\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"uze1mEr6wZ5C","execution":{"iopub.status.busy":"2024-11-04T07:24:56.566294Z","iopub.execute_input":"2024-11-04T07:24:56.567093Z","iopub.status.idle":"2024-11-04T07:24:56.588782Z","shell.execute_reply.started":"2024-11-04T07:24:56.567055Z","shell.execute_reply":"2024-11-04T07:24:56.587930Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":["set_seed()\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_dataset = dataset['train']\n","validation_dataset = dataset['validation']\n","test_dataset = dataset['test']"],"metadata":{"id":"CP6--aDd0rN6","execution":{"iopub.status.busy":"2024-11-04T07:24:56.589948Z","iopub.execute_input":"2024-11-04T07:24:56.590254Z","iopub.status.idle":"2024-11-04T07:25:02.178217Z","shell.execute_reply.started":"2024-11-04T07:24:56.590222Z","shell.execute_reply":"2024-11-04T07:25:02.177081Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# **Part 0. Dataset Preparation**"],"metadata":{"id":"EsVKHy3K1njx"}},{"cell_type":"markdown","source":["Part 0. Dataset Preparation We will be using the movie review dataset introduced in https://www.cs.cornell.edu/people/ pabo/movie-review-data/rt-polaritydata.README.1.0.txt. To load this dataset, you need to install the “datasets” library via pip install datasets. Then you can use the following code snippet: 1 from datasets import load_dataset 2 dataset = load_dataset (\" rotten_tomatoes \") 3 train_dataset = dataset [’train ’] 4 validation_dataset = dataset [’validation ’] 5 test_dataset = dataset [’test ’] Using the original train-valid-test split provided in the above code, you will perform model training on the training dataset, configure your model (e.g., learning rate, batch size, number of training epochs) on the validation dataset, and conduct evaluation on the test dataset."],"metadata":{"id":"Xb01WBWa1kOH"}},{"cell_type":"markdown","source":["# **Load Dataset**"],"metadata":{"id":"jTxJ0zFgAzou"}},{"cell_type":"code","source":["set_seed()\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_dataset = dataset['train']\n","validation_dataset = dataset['validation']\n","test_dataset = dataset['test']"],"metadata":{"execution":{"iopub.status.busy":"2024-11-04T07:25:02.179464Z","iopub.execute_input":"2024-11-04T07:25:02.179794Z","iopub.status.idle":"2024-11-04T07:25:05.342825Z","shell.execute_reply.started":"2024-11-04T07:25:02.179759Z","shell.execute_reply":"2024-11-04T07:25:05.341984Z"},"trusted":true,"id":"SZ3OxMKjAnVy"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["print(f\"train_dataset size: {len(train_dataset)}\")\n","print(f\"validation_dataset size: {len(validation_dataset)}\")\n","print(f\"test_dataset size: {len(test_dataset)}\")\n","print(f\"data format: {train_dataset[0]}\")"],"metadata":{"id":"Mg0GeOL9hJde","executionInfo":{"status":"ok","timestamp":1729855647285,"user_tz":-480,"elapsed":4,"user":{"displayName":"colin yifei","userId":"02963454452622984612"}},"outputId":"9168193c-f249-43a8-9137-952eb8c56d60","execution":{"iopub.status.busy":"2024-11-04T07:25:05.343974Z","iopub.execute_input":"2024-11-04T07:25:05.344286Z","iopub.status.idle":"2024-11-04T07:25:05.349977Z","shell.execute_reply.started":"2024-11-04T07:25:05.344252Z","shell.execute_reply":"2024-11-04T07:25:05.349043Z"},"trusted":true},"outputs":[{"name":"stdout","text":"train_dataset size: 8530\nvalidation_dataset size: 1066\ntest_dataset size: 1066\ndata format: {'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    print(\"CUDA is available!\")\n","    print(f\"The name of the CUDA device is: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n","else:\n","    print(\"CUDA is not available.\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"QrOunSyr2V--","executionInfo":{"status":"ok","timestamp":1729855647286,"user_tz":-480,"elapsed":4,"user":{"displayName":"colin yifei","userId":"02963454452622984612"}},"outputId":"00be97b2-3990-440c-875c-981ca83db740","execution":{"iopub.status.busy":"2024-11-04T07:25:05.351384Z","iopub.execute_input":"2024-11-04T07:25:05.351738Z","iopub.status.idle":"2024-11-04T07:25:05.408914Z","shell.execute_reply.started":"2024-11-04T07:25:05.351705Z","shell.execute_reply":"2024-11-04T07:25:05.407944Z"},"trusted":true},"outputs":[{"name":"stdout","text":"CUDA is available!\nThe name of the CUDA device is: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["# **Load word2vec Model**"],"metadata":{"id":"SukwM-3EA4ix"}},{"cell_type":"code","source":["# Load the pre-trained Word2Vec model\n","if 'google.colab' in sys.modules:\n","    print(\"Running on Google Colab's hosted runtime\")\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    w2v_model = KeyedVectors.load_word2vec_format(PATH_TO_W2V_MODEL_DRIVE, binary=True)\n","else:\n","    print(\"Running on a local runtime\")\n","    w2v_model = KeyedVectors.load_word2vec_format(PATH_TO_W2V_MODEL_LOCAL, binary=True)\n","\n","# Get the dimension of the embeddings\n","vector_dim = w2v_model.vector_size\n","print(f\"Word2Vec Dimension: {vector_dim}\")\n"],"metadata":{"id":"CS5XdHuN0rQe","executionInfo":{"status":"ok","timestamp":1729855797685,"user_tz":-480,"elapsed":86758,"user":{"displayName":"colin yifei","userId":"02963454452622984612"}},"outputId":"02bad16f-7c6f-4c2f-f837-fe9c08d743ea","execution":{"iopub.status.busy":"2024-11-04T07:25:05.410337Z","iopub.execute_input":"2024-11-04T07:25:05.410672Z","iopub.status.idle":"2024-11-04T07:25:47.403877Z","shell.execute_reply.started":"2024-11-04T07:25:05.410637Z","shell.execute_reply":"2024-11-04T07:25:47.402920Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Running on a local runtime\nWord2Vec Dimension: 300\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["# **Data Preprocessing**"],"metadata":{"id":"H_xsrQoCA-Eg"}},{"cell_type":"code","source":["# this code was previously used when running file on kaggle\n","#!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"],"metadata":{"id":"uEKcOy__AnV3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the lemmatizer and stopwords\n","lemmatizer = WordNetLemmatizer()\n","punctuation = set(string.punctuation)\n","stop_words = set(stopwords.words('english'))\n","\n","# Dataset Preprocessing to tokenize and/or lemmatize\n","def preprocess_dataset(data, lemmatization = True):\n","    processed_sentences = []\n","\n","    for entry in data:\n","        text = entry['text']\n","        # Tokenize the sentence\n","        tokens = word_tokenize(text)\n","        if lemmatization:\n","          # With case folding, punctuation and stop words removal\n","          #tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if word not in punctuation and word.lower() not in stop_words]\n","          tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if word not in punctuation]\n","\n","        processed_sentences.append(tokens)\n","\n","    return processed_sentences\n","\n","# Get vocab and oov words\n","def get_vocab_OOV(sentences, w2v_model):\n","    vocabulary = set()\n","    oov_words = set()\n","\n","    for sentence in sentences:\n","        for word in sentence:\n","            vocabulary.add(word)\n","            if word not in w2v_model:\n","                oov_words.add(word)\n","\n","    return vocabulary, oov_words\n","\n","vocabulary, oov_words = get_vocab_OOV(preprocess_dataset(train_dataset, False), w2v_model)\n","lemmatized_vocabulary, lemmatized_oov_words = get_vocab_OOV(preprocess_dataset(train_dataset), w2v_model)\n","\n","print(f\"Before lemmatization: Vocabulary size: {len(vocabulary)}, OOV words: {len(oov_words)}\")\n","print(f\"After lemmatization: Vocabulary size: {len(lemmatized_vocabulary)}, OOV words: {len(lemmatized_oov_words)}\")"],"metadata":{"id":"HVLzP4kh0rX4","executionInfo":{"status":"ok","timestamp":1729855805621,"user_tz":-480,"elapsed":7953,"user":{"displayName":"colin yifei","userId":"02963454452622984612"}},"outputId":"4bed229e-5010-4640-e8f8-52c2895ef3a5","execution":{"iopub.status.busy":"2024-11-04T07:25:47.405238Z","iopub.execute_input":"2024-11-04T07:25:47.405641Z","iopub.status.idle":"2024-11-04T07:25:56.632547Z","shell.execute_reply.started":"2024-11-04T07:25:47.405574Z","shell.execute_reply":"2024-11-04T07:25:56.631577Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Before lemmatization: Vocabulary size: 18035, OOV words: 3619\nAfter lemmatization: Vocabulary size: 16571, OOV words: 3590\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["\n","\n","1.   Lemmatize, case folding and remove stop words for train, validation and test set\n","2.   Drop words not found in train dataset for validation and test set\n","\n","\n"],"metadata":{"id":"hbMJLXI8jNuf"}},{"cell_type":"code","source":["preprocessed_train_dataset = preprocess_dataset(train_dataset)\n","preprocessed_validation_dataset = preprocess_dataset(validation_dataset)\n","preprocessed_test_dataset = preprocess_dataset(test_dataset)"],"metadata":{"id":"103baxlbkPdk","execution":{"iopub.status.busy":"2024-11-04T07:25:56.633988Z","iopub.execute_input":"2024-11-04T07:25:56.634393Z","iopub.status.idle":"2024-11-04T07:26:01.391138Z","shell.execute_reply.started":"2024-11-04T07:25:56.634339Z","shell.execute_reply":"2024-11-04T07:26:01.390149Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":["def drop_oov(preprocessed_dataset, w2v_model):\n","    return [[word for word in sentence if word in w2v_model] for sentence in preprocessed_dataset]\n","\n","def drop_non_train_vocab(dataset, train_vocab):\n","    return [[word for word in sentence if word in train_vocab] for sentence in dataset]\n","\n","X_train = preprocessed_train_dataset\n","Y_train = [entry['label'] for entry in train_dataset]\n","\n","X_val = drop_non_train_vocab(preprocessed_validation_dataset, lemmatized_vocabulary)\n","Y_val = [entry['label'] for entry in validation_dataset]\n","\n","X_test = drop_non_train_vocab(preprocessed_test_dataset, lemmatized_vocabulary)\n","Y_test = [entry['label'] for entry in test_dataset]\n"],"metadata":{"id":"DAm3fk7PjTyU","execution":{"iopub.status.busy":"2024-11-04T07:26:01.395409Z","iopub.execute_input":"2024-11-04T07:26:01.395720Z","iopub.status.idle":"2024-11-04T07:26:01.780599Z","shell.execute_reply.started":"2024-11-04T07:26:01.395687Z","shell.execute_reply":"2024-11-04T07:26:01.779525Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# **Generate Embedding**"],"metadata":{"id":"SoQccpkdlogO"}},{"cell_type":"code","source":["def get_word2index(train_vocab):\n","    word2index = {}\n","    for idx, word in enumerate(train_vocab):\n","        word2index[word] = idx + 1  # +1 to offset 0 for padding\n","    return word2index\n","\n","word2index = get_word2index(lemmatized_vocabulary)"],"metadata":{"id":"NCycQdn5M4w-","execution":{"iopub.status.busy":"2024-11-04T07:26:01.781856Z","iopub.execute_input":"2024-11-04T07:26:01.782191Z","iopub.status.idle":"2024-11-04T07:26:01.793415Z","shell.execute_reply.started":"2024-11-04T07:26:01.782158Z","shell.execute_reply":"2024-11-04T07:26:01.792410Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":["print(f\"word2index size = vocab size: {len(word2index)}\")"],"metadata":{"id":"Mjn-EoF4XRm2","executionInfo":{"status":"ok","timestamp":1729844661145,"user_tz":-480,"elapsed":14,"user":{"displayName":"Hong Yi Leong","userId":"10727536068644076426"}},"outputId":"64dd669d-afb1-4919-a541-16c48557026a","execution":{"iopub.status.busy":"2024-11-04T07:26:01.794711Z","iopub.execute_input":"2024-11-04T07:26:01.795006Z","iopub.status.idle":"2024-11-04T07:26:01.803256Z","shell.execute_reply.started":"2024-11-04T07:26:01.794975Z","shell.execute_reply":"2024-11-04T07:26:01.802361Z"},"trusted":true},"outputs":[{"name":"stdout","text":"word2index size = vocab size: 16571\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["def create_embedding_matrix(w2v_model, word2index, embedding_dim=300):\n","    \"\"\"\n","    Creates an embedding matrix based on the word2index mapping and Word2Vec model.\n","\n","    Args:\n","    - w2v_model: Pre-trained Word2Vec model.\n","    - word2index: Dictionary mapping words to their indices.\n","    - embedding_dim: Dimension of the Word2Vec word vectors (default: 300).\n","\n","    Returns:\n","    - embedding_matrix: Embedding matrix where each row corresponds to the vector of a word in the vocabulary.\n","    \"\"\"\n","    vocab_size = len(word2index)  # Number of words in the training vocabulary\n","\n","    embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n","    mean = np.mean(w2v_model.vectors, axis=0)\n","    std = np.std(w2v_model.vectors, axis=0)\n","\n","    for word, idx in word2index.items():\n","        if word in w2v_model:\n","            embedding_matrix[idx] = w2v_model[word]\n","        else:\n","            embedding_matrix[idx] = np.random.normal(\n","                loc=mean, scale=std, size=(embedding_dim,)\n","            )\n","\n","    return embedding_matrix\n","\n","embedding_dim = w2v_model.vector_size  # Get the dimension of the word vectors in Word2Vec\n","embedding_matrix = create_embedding_matrix(w2v_model, word2index, embedding_dim)\n","print(f\"embedding_matrix shape: {embedding_matrix.shape}\")\n"],"metadata":{"id":"ooT2n-xNOQNy","executionInfo":{"status":"ok","timestamp":1729844661145,"user_tz":-480,"elapsed":10,"user":{"displayName":"Hong Yi Leong","userId":"10727536068644076426"}},"outputId":"34e2e3b6-a0ab-4f56-a692-ed46c2ad7a9a","execution":{"iopub.status.busy":"2024-11-04T07:26:01.804570Z","iopub.execute_input":"2024-11-04T07:26:01.804904Z","iopub.status.idle":"2024-11-04T07:26:04.909855Z","shell.execute_reply.started":"2024-11-04T07:26:01.804871Z","shell.execute_reply":"2024-11-04T07:26:04.908797Z"},"trusted":true},"outputs":[{"name":"stdout","text":"embedding_matrix shape: (16572, 300)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["# **Sentence2sequence + Padding**"],"metadata":{"id":"rBeJ7A9AjUj9"}},{"cell_type":"code","source":["# Get maximum sentence length\n","def get_max_sentence_length(*datasets):\n","    \"\"\"\n","    Get the maximum sentence length from the provided datasets.\n","\n","    Args:\n","    - datasets: A variable number of lists containing tokenized sentences.\n","\n","    Returns:\n","    - max_length: The maximum sentence length across all datasets.\n","    \"\"\"\n","    max_length = 0\n","    for index, dataset in enumerate(datasets):\n","        # Update max_length if a longer sentence is found\n","        max_length = max(max_length, max(len(sentence) for sentence in dataset))\n","        min_length = min(len(sentence) for sentence in dataset)\n","        print(f\"Min sentence length in dataset {index}: {min_length}\")\n","    return max_length\n","\n","# Example usage\n","max_length = get_max_sentence_length(X_train, X_val, X_test)\n","print(f\"Maximum sentence length of three datasets: {max_length}\")"],"metadata":{"id":"UhNxEvMJg3z6","executionInfo":{"status":"ok","timestamp":1729844760263,"user_tz":-480,"elapsed":364,"user":{"displayName":"Hong Yi Leong","userId":"10727536068644076426"}},"outputId":"99722c5c-e01c-4814-ea7e-137abd3d1ace","execution":{"iopub.status.busy":"2024-11-04T07:26:04.911144Z","iopub.execute_input":"2024-11-04T07:26:04.911483Z","iopub.status.idle":"2024-11-04T07:26:04.921781Z","shell.execute_reply.started":"2024-11-04T07:26:04.911446Z","shell.execute_reply":"2024-11-04T07:26:04.920938Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Min sentence length in dataset 0: 1\nMin sentence length in dataset 1: 0\nMin sentence length in dataset 2: 1\nMaximum sentence length of three datasets: 53\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["def convert_to_sequence_data(tokenized_data, word2index, max_length=max_length):\n","    \"\"\"\n","    Preprocess the tokenized data for RNN input.\n","\n","    Args:\n","    - tokenized_data: List of tokenized sentences.\n","    - word2index: Dictionary mapping words to indices.\n","    - max_length: Maximum length for padding (default is 100).\n","\n","    Returns:\n","    - padded_sequences: Padded tensor of sequences.\n","    \"\"\"\n","    # Convert tokens to indices\n","    indexed_sequences = []\n","    for sentence in tokenized_data:\n","        indexed_sentence = [word2index.get(word, 0) for word in sentence]  # Use 0 for OOV words\n","\n","        # Truncate or pad the indexed sentence to the specified max_length\n","        if len(indexed_sentence) > max_length:\n","            indexed_sentence = indexed_sentence[:max_length]  # Truncate\n","        else:\n","            indexed_sentence += [0] * (max_length - len(indexed_sentence))  # Pad with zeros\n","\n","        indexed_sequences.append(torch.tensor(indexed_sentence, dtype=torch.long))\n","\n","    # Convert list of tensors to a padded tensor\n","    padded_sequences = torch.stack(indexed_sequences)\n","\n","    return padded_sequences\n","\n","# Convert to sequence\n","X_train_sequence = convert_to_sequence_data(X_train, word2index)\n","X_val_sequence = convert_to_sequence_data(X_val , word2index)\n","X_test_sequence = convert_to_sequence_data(X_test, word2index)\n"],"metadata":{"id":"9NIEsB6inUbF","execution":{"iopub.status.busy":"2024-11-04T07:26:04.922978Z","iopub.execute_input":"2024-11-04T07:26:04.923291Z","iopub.status.idle":"2024-11-04T07:26:05.163375Z","shell.execute_reply.started":"2024-11-04T07:26:04.923257Z","shell.execute_reply":"2024-11-04T07:26:05.162554Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# **CNN Model Architecture**"],"metadata":{"id":"BQWy6CTWQcVK"}},{"cell_type":"code","source":["class SimpleCNN(nn.Module):\n","    def __init__(self, embedding_matrix, num_filters=64, num_conv_layers=1, filter_size=3, aggregation='mean_pool', attn = False, num_heads=1):\n","        super(SimpleCNN, self).__init__()\n","        embedding_dim = embedding_matrix.shape[1]\n","        self.aggregation = aggregation\n","        self.attn = attn\n","\n","        self.embedding = nn.Embedding.from_pretrained(\n","            torch.tensor(embedding_matrix, dtype=torch.float32),\n","            padding_idx=0,  # Assuming 0 is the padding index\n","            freeze=False  # Freezing the embeddings\n","        )\n","\n","        self.conv1d_layers = nn.ModuleList([\n","            nn.Sequential(*[\n","                nn.Conv1d(in_channels=(embedding_dim if i == 0 else num_filters),  # Use embedding_dim for the first layer\n","                          out_channels=num_filters,\n","                          kernel_size=filter_size),\n","                nn.ReLU(),\n","            ]) for i in range(num_conv_layers)\n","        ])\n","        if self.attn:\n","            self.attn_head = nn.MultiheadAttention(num_filters, num_heads, batch_first = True)\n","\n","        self.dropout = nn.Dropout(0.2)\n","        self.fc = nn.Linear(num_filters, 1)  # Output size is 1 for binary classification\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        embedded = embedded.permute(0, 2, 1)\n","\n","        conv_out = embedded\n","        for conv1d_layer in self.conv1d_layers:\n","            conv_out = conv1d_layer(conv_out)\n","\n","        if self.aggregation == 'max_pool':\n","            agg_out, _ = torch.max(conv_out, dim=2)  # Max-pooling over the sequence length\n","        elif self.aggregation == 'mean_pool':\n","            agg_out = torch.mean(conv_out, dim=2)  # Mean-pooling over the sequence length\n","        if self.attn:\n","            agg_out, _ = self.attn_head(agg_out, agg_out, agg_out)\n","        out = self.dropout(agg_out)\n","        out = self.fc(out)\n","        out = torch.sigmoid(out)\n","\n","        return out"],"metadata":{"id":"OYeiOXgv6pyX","execution":{"iopub.status.busy":"2024-11-04T07:26:05.164858Z","iopub.execute_input":"2024-11-04T07:26:05.165337Z","iopub.status.idle":"2024-11-04T07:26:05.177127Z","shell.execute_reply.started":"2024-11-04T07:26:05.165289Z","shell.execute_reply":"2024-11-04T07:26:05.176208Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# **CNN Best Hyperparameter Test**"],"metadata":{"id":"3TMqCVczGxwt"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# Reset the seed to ensure fairness when comparing performances\n","set_seed()\n","\n","# Best parameter for CNN with Max Pool\n","num_filters = 256\n","num_conv_layers = 1\n","filter_size = 7\n","learning_rate = 0.0005\n","batch_size = 32\n","num_epochs = 40\n","aggregation = 'max_pool'\n","optimizer_name = 'Adam'\n","\n","'''\n","# Best parameter for CNN with Mean Pool\n","num_filters = 128\n","num_conv_layers = 2\n","filter_size = 3\n","learning_rate = 0.001\n","batch_size = 32\n","num_epochs = 50\n","aggregation = 'mean_pool'\n","optimizer = 'RMSprop'\n","'''\n","\n","'''\n","# Best parameter for CNN + Attention\n","num_filters = 64\n","num_conv_layers = 1\n","filter_size = 5\n","learning_rate = 0.0001\n","batch_size = 32\n","num_epochs = 50\n","num_heads = 1\n","aggregation = 'max_pool'\n","optimizer = 'RMSprop'\n","'''\n","\n","\n","# Create data loaders\n","train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","\n","# Initialize the model\n","model = SimpleCNN(embedding_matrix, num_filters, num_conv_layers, filter_size, aggregation, False)\n","model.to(device)\n","criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","# Choose optimizer based on trial suggestion\n","optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","early_stopper = EarlyStopper(patience = 20, delta = 0.05)\n","# Lists to store loss and accuracy for visualization\n","train_losses = []\n","val_accuracies = []\n","\n","best_val_accuracy = 0\n","best_model = None\n","\n","for epoch in range(num_epochs):\n","    loss = train_loop(train_loader, model, criterion, optimizer)\n","    train_losses.append(loss)\n","\n","    # Evaluate on validation set\n","    val_accuracy = test_loop(val_loader, model)\n","    val_accuracies.append(val_accuracy)\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        best_model = copy.deepcopy(model)\n","\n","    if early_stopper.early_stop(val_accuracy):\n","        print(\"Early stopping triggered!\")\n","        break\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","# Evaluate on test data\n","test_accuracy = test_loop(test_loader, best_model)\n","\n","# Print out the performance\n","print(f\"Using Best Model, Test Accuracy: {test_accuracy:.4f}\")"],"metadata":{"id":"dvagD9mmNEXx","execution":{"iopub.status.busy":"2024-11-04T07:26:05.178231Z","iopub.execute_input":"2024-11-04T07:26:05.178512Z","iopub.status.idle":"2024-11-04T07:26:28.038522Z","shell.execute_reply.started":"2024-11-04T07:26:05.178480Z","shell.execute_reply":"2024-11-04T07:26:28.037513Z"},"trusted":true,"outputId":"eb061f66-5d58-479e-e8e7-008d4c038aa4"},"outputs":[{"name":"stdout","text":"Epoch [1/40], Train Loss: 0.5344, Validation Accuracy: 0.7814\nEpoch [2/40], Train Loss: 0.3094, Validation Accuracy: 0.7720\nEpoch [3/40], Train Loss: 0.1515, Validation Accuracy: 0.7964\nEpoch [4/40], Train Loss: 0.0604, Validation Accuracy: 0.7861\nEpoch [5/40], Train Loss: 0.0228, Validation Accuracy: 0.7777\nEpoch [6/40], Train Loss: 0.0094, Validation Accuracy: 0.7861\nEpoch [7/40], Train Loss: 0.0047, Validation Accuracy: 0.7795\nEpoch [8/40], Train Loss: 0.0030, Validation Accuracy: 0.7767\nEpoch [9/40], Train Loss: 0.0019, Validation Accuracy: 0.7833\nEpoch [10/40], Train Loss: 0.0013, Validation Accuracy: 0.7777\nEpoch [11/40], Train Loss: 0.0010, Validation Accuracy: 0.7786\nEpoch [12/40], Train Loss: 0.0007, Validation Accuracy: 0.7805\nEpoch [13/40], Train Loss: 0.0006, Validation Accuracy: 0.7805\nEpoch [14/40], Train Loss: 0.0005, Validation Accuracy: 0.7739\nEpoch [15/40], Train Loss: 0.0004, Validation Accuracy: 0.7739\nEpoch [16/40], Train Loss: 0.0003, Validation Accuracy: 0.7739\nEpoch [17/40], Train Loss: 0.0003, Validation Accuracy: 0.7749\nEpoch [18/40], Train Loss: 0.0002, Validation Accuracy: 0.7739\nEpoch [19/40], Train Loss: 0.0002, Validation Accuracy: 0.7739\nEpoch [20/40], Train Loss: 0.0001, Validation Accuracy: 0.7758\nEpoch [21/40], Train Loss: 0.0001, Validation Accuracy: 0.7758\nEpoch [22/40], Train Loss: 0.0001, Validation Accuracy: 0.7730\nEarly stopping triggered!\nUsing Best Model, Test Accuracy: 0.7852\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":[],"metadata":{"trusted":true,"id":"tlAAS9Q-AnV-"},"outputs":[],"execution_count":null},{"cell_type":"code","source":[],"metadata":{"trusted":true,"id":"dV8dn1g_AnV-"},"outputs":[],"execution_count":null}]}