{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"BWnbpQJT0Vy7"},"outputs":[],"source":["%%capture\n","!pip install datasets\n","!pip install optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ogYH4cv385k"},"outputs":[],"source":["# The value for the seed and paths\n","SEED = 168\n","PATH_TO_W2V_MODEL_DRIVE = '/content/drive/My Drive/NLP Project/GoogleNews-vectors-negative300.bin'\n","PATH_TO_W2V_MODEL_LOCAL = r'GoogleNews-vectors-negative300.bin'\n","PATH_TO_FASTTEXT_MODEL_DRIVE = '/content/drive/My Drive/NLP Project/cc.en.300.bin'\n","PATH_TO_FASTTEXT_MODEL_LOCAL = r'cc.en.300.bin/cc.en.300.bin'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18038,"status":"ok","timestamp":1730614566263,"user":{"displayName":"alvin khoo","userId":"02395113757594492728"},"user_tz":-480},"id":"L5t53ajjwAIQ","outputId":"a66b83b5-72ea-4e3f-d100-766f88fb75b5"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["# Standard library imports\n","from collections import Counter\n","\n","# General third-party libraries import\n","import copy\n","from gensim.models import KeyedVectors\n","from gensim.models.fasttext import load_facebook_model\n","from datasets import load_dataset\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","import string\n","import sys\n","\n","# Import optuna for hyperparams tuning\n","import optuna\n","\n","# Import nltk\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","# Download necessary nltk resources\n","nltk.download('wordnet')\n","nltk.download('punkt')  # Ensure punkt is downloaded for tokenization\n","nltk.download('stopwords')  # Ensure stopwords are also downloaded if not already\n","\n","# Import pytorch\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch import nn\n","from torch.nn.utils.rnn import pad_sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uze1mEr6wZ5C"},"outputs":[],"source":["# Method to set random seed to ensure consistency\n","def set_seed(seed = SEED):\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","# Factory to create the dataloader\n","def dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device):\n","    # Create Tensor datasets\n","    train_dataset = TensorDataset(X_train_sequence.to(device), torch.tensor(Y_train, dtype=torch.long, device=device))\n","    val_dataset = TensorDataset(X_val_sequence.to(device), torch.tensor(Y_val, dtype=torch.long, device=device))\n","    test_dataset = TensorDataset(X_test_sequence.to(device), torch.tensor(Y_test, dtype=torch.long, device=device))\n","\n","    # Create data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","    return train_loader, val_loader, test_loader\n","\n","# Early Stopper\n","class EarlyStopper:\n","    def __init__(self, patience=30, delta=0):\n","        self.patience = patience\n","        self.delta = delta\n","        self.counter = 0\n","        self.max_validation_accuracy = 0\n","\n","    def early_stop(self, validation_accuracy):\n","        if validation_accuracy > self.max_validation_accuracy:\n","            self.max_validation_accuracy = validation_accuracy\n","            self.counter = 0\n","        elif validation_accuracy < (self.max_validation_accuracy + self.delta):\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                return True\n","        return False\n","\n","# Factory to provide required optimizer which specified parameters\n","def optimizer_factory(optimizer_name, model, learning_rate, momentum = 0.8):\n","    if optimizer_name == 'Adam':\n","        return torch.optim.Adam(model.parameters(), lr = learning_rate)\n","    if optimizer_name == 'SGD':\n","        return torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = momentum)\n","    if optimizer_name == 'RMSprop':\n","        return torch.optim.RMSprop(model.parameters(), lr = learning_rate)\n","\n","# Method to train the model\n","def train_loop(dataloader, model, loss_fn, optimizer):\n","    model.train()  # Set model to training mode\n","    train_loss = 0  # To accumulate loss for this epoch\n","\n","    for inputs, labels in dataloader:\n","        optimizer.zero_grad()  # Zero the gradients\n","        outputs = model(inputs)  # Forward pass\n","\n","        # Calculate loss\n","        loss = loss_fn(outputs.squeeze(), labels.float())  # Squeeze to match dimensions\n","        loss.backward()  # Backpropagation\n","\n","        # # Apply gradient clipping\n","        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n","\n","        optimizer.step()  # Update parameters\n","\n","        train_loss += loss.item()  # Accumulate loss\n","\n","    # Calculate average loss for the epoch\n","    train_loss /= len(dataloader)\n","    return train_loss\n","\n","# Method to evaluate the model\n","def test_loop(dataloader, model):\n","    model.eval() # Set model to evaluation mode\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():  # Disable gradient computation\n","        for inputs, labels in dataloader:\n","            outputs = model(inputs)  # Forward pass\n","            predicted = (outputs.squeeze() > 0.5).float()  # Apply threshold\n","\n","            total += labels.size(0)\n","            correct += (predicted == labels.float()).sum().item()  # Count correct predictions\n","\n","    accuracy = correct / total\n","    return accuracy\n","\n","# Method to plot the performance graph\n","def plot_performance(train_losses, val_accuracies, num_epochs):\n","    # Visualization\n","    plt.figure(figsize=(12, 5))\n","\n","    # Plotting Loss\n","    plt.subplot(1, 2, 1)\n","    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss', color='blue')\n","    plt.title('Training Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.xticks(range(1, num_epochs + 1, 10))\n","    plt.legend()\n","\n","    # Plotting Accuracy\n","    plt.subplot(1, 2, 2)\n","    plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='orange')\n","    plt.title('Validation Accuracy')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.xticks(range(1, num_epochs + 1, 10))\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"EsVKHy3K1njx"},"source":["# **Part 0. Dataset Preparation**"]},{"cell_type":"markdown","metadata":{"id":"Xb01WBWa1kOH"},"source":["Part 0. Dataset Preparation We will be using the movie review dataset introduced in https://www.cs.cornell.edu/people/ pabo/movie-review-data/rt-polaritydata.README.1.0.txt. To load this dataset, you need to install the “datasets” library via pip install datasets. Then you can use the following code snippet: 1 from datasets import load_dataset 2 dataset = load_dataset (\" rotten_tomatoes \") 3 train_dataset = dataset [’train ’] 4 validation_dataset = dataset [’validation ’] 5 test_dataset = dataset [’test ’] Using the original train-valid-test split provided in the above code, you will perform model training on the training dataset, configure your model (e.g., learning rate, batch size, number of training epochs) on the validation dataset, and conduct evaluation on the test dataset."]},{"cell_type":"markdown","metadata":{"id":"jTxJ0zFgAzou"},"source":["# **Load Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363,"referenced_widgets":["ca26c7d60b8340619b72dcc095234522","4c060e5b775d49cf97fa43d8cdacfc1c","7cf56f239dcc40f29a1f01b335a90d58","0b8a07c019ef4bd4a2c18e87f328a5d6","b0d73abc371d4d9b98412535af6af040","1463ad0101de4ebdbbc75aefebc4f423","3ca89dd8e3e341e38e83aa642fe77acb","366ebc5f241c4cf38406ab9c93280b80","93320f7e8a864d5bb5aa78dfa42597fa","3773abf300dd42a9a1d1f94989aa72ab","be75af6ddff54a0088da1453ebc75c7b","c0e216c195e347cd97f395a66ade2dc3","47ea00e33aba43b0bc906adf4256ef8b","ffcb6889ef8749ce8db231526ff088ef","668aea027e4a4e5382b89464ea88104c","7085e3d659ac41809963e05d35aeb41d","5f61b2a37c294a5a9bbf3ed0ede587ab","f34f7432ec4d459cb03cacdc85db437a","0dff2f4e567a4fb398b56c16e199b3f8","f78bfa03d5f04ad397050e25cf07246f","2584ee548125431a9ec7a60ee385d342","ef5b7c15d9454955b21c7387510d54d6","4621ac9eb58f462686688936bcbdea53","bd5ec82a5ee646659add5e9ba1b0e76a","5fc163d9e89e4baeb85d3aa16257c42b","7ac82e55bb4e4868b67b2177b42d28ba","c2164308609d4decacd82fc888625780","2a90bb6951dd4da5a84a4b26b737fb84","c1c906e0106746fbaf921178ea8cbc66","3717b79035744bfc9bbdbd5f9df5380f","ee68977fb5634e5d95004a7689686e31","55353da4c2a141dfa5c978d307396f1f","395124e7a7c54beb89a98fd542bdb627","645be1bd8bb54812a2df0ecb447882cb","6d8bd3534cbe4bcb9f90394526df6862","558d6df7b71d44ae880d518c99a1f1f5","ac59d255e29049868449c3413bff4d2d","cca0901277064588b527facd74a53467","cf9d82a268cc40f2bad4eb07b0e92252","04f6c6ba85f44387953a1c662424ad82","ff389610170749e2aaaf7bd3fa9c35e8","0f73887fedf4402c943b8c8a050bd0a7","246bc2c957e44828b2dbd0255a5591d0","f78ebff877a4450c968080425ab05183","95ac2d742074490d85094aaeafbc41d1","d3613dc7a94d4fbda121d28e255424c5","86ac5db6431340c8a6e74bb969043ae3","766dd2fbf5f1410ca4011be0f6f737a3","77ea8f679b64453190d939de6d2fe81c","976816239b5342de97846df39a66294c","df33277871204e6a9a4a3ccba3ca8045","20e03d4e40d34ff2a6cea59eb7c2281d","02ad52ed3bbc4ba6b8750aa4a2565c0c","57cebd549e394a7387d4e6f8d067bf51","478045f86d604fc1bac9d267ca5ad7f1","b39178f5784b4f67a174e5c957c07673","b81d898e794743ea82e549581da457ae","6875518f241f4a31be532e63a34f5f5e","8248e0447764401a9851e40e3c626354","60a8db739f3442a48e69e41fa2cc02a0","4ffefb0779fd47c9935887b37afc0c08","4111f13dccc74ddfb66c416dd0ce9fd1","ceefff4198314b9b9ff797fca58919e0","42a4ea758bcc4983b28a8d4140879616","c34e46a333724e759604b94a2ae19997","59f4af2615c5408d8b06838747facdfc","f7c3d20305014a6e9745276f02d396c2","ed549a4bb4784f72827df85c5d290be6","bcf1bc6748d14056beecb71c395bc97e","3ec88fc8995249f99e24a6a7b8399102","769bc507904044f9ad42c776573d1237","44a6473408e246b990c1e377a3703c03","68b74e4f668f46fd818835af2457571f","62dd83f5fdff457dafe66216fa897934","070f7012059d46e9b3dc250405a354b8","ab4e414fd6194f7ab76712fd71eba09e","f6470914dfc7463b90664712d3b946c1"]},"executionInfo":{"elapsed":11005,"status":"ok","timestamp":1730614577266,"user":{"displayName":"alvin khoo","userId":"02395113757594492728"},"user_tz":-480},"id":"CP6--aDd0rN6","outputId":"2b02d8a2-4a03-4489-bec2-9dbb77eb7f7b"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca26c7d60b8340619b72dcc095234522","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/7.46k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0e216c195e347cd97f395a66ade2dc3","version_major":2,"version_minor":0},"text/plain":["train.parquet:   0%|          | 0.00/699k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4621ac9eb58f462686688936bcbdea53","version_major":2,"version_minor":0},"text/plain":["validation.parquet:   0%|          | 0.00/90.0k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"645be1bd8bb54812a2df0ecb447882cb","version_major":2,"version_minor":0},"text/plain":["test.parquet:   0%|          | 0.00/92.2k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"95ac2d742074490d85094aaeafbc41d1","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b39178f5784b4f67a174e5c957c07673","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7c3d20305014a6e9745276f02d396c2","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["set_seed()\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_dataset = dataset['train']\n","validation_dataset = dataset['validation']\n","test_dataset = dataset['test']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1730614577266,"user":{"displayName":"alvin khoo","userId":"02395113757594492728"},"user_tz":-480},"id":"Mg0GeOL9hJde","outputId":"3d124faa-3179-4846-e758-36bf2e29987c"},"outputs":[{"name":"stdout","output_type":"stream","text":["train_dataset size: 8530\n","validation_dataset size: 1066\n","test_dataset size: 1066\n","data format: {'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n"]}],"source":["print(f\"train_dataset size: {len(train_dataset)}\")\n","print(f\"validation_dataset size: {len(validation_dataset)}\")\n","print(f\"test_dataset size: {len(test_dataset)}\")\n","print(f\"data format: {train_dataset[0]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1730614577266,"user":{"displayName":"alvin khoo","userId":"02395113757594492728"},"user_tz":-480},"id":"QrOunSyr2V--","outputId":"a58b5626-a598-44d8-ca4f-c932505e8595"},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA is available!\n","The name of the CUDA device is: Tesla T4\n"]}],"source":["if torch.cuda.is_available():\n","    print(\"CUDA is available!\")\n","    print(f\"The name of the CUDA device is: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n","else:\n","    print(\"CUDA is not available.\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"SukwM-3EA4ix"},"source":["# **Load word2vec Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93001,"status":"ok","timestamp":1730614670264,"user":{"displayName":"alvin khoo","userId":"02395113757594492728"},"user_tz":-480},"id":"CS5XdHuN0rQe","outputId":"6f696e27-ce0c-4649-a492-56fe65bb3eda"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on Google Colab's hosted runtime\n","Mounted at /content/drive\n","Word2Vec Dimension: 300\n"]}],"source":["# Load the pre-trained Word2Vec model\n","if 'google.colab' in sys.modules:\n","    print(\"Running on Google Colab's hosted runtime\")\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    w2v_model = KeyedVectors.load_word2vec_format(PATH_TO_W2V_MODEL_DRIVE, binary=True)\n","else:\n","    print(\"Running on a local runtime\")\n","    w2v_model = KeyedVectors.load_word2vec_format(PATH_TO_W2V_MODEL_LOCAL, binary=True)\n","\n","# Get the dimension of the embeddings\n","vector_dim = w2v_model.vector_size\n","print(f\"Word2Vec Dimension: {vector_dim}\")\n"]},{"cell_type":"markdown","metadata":{"id":"H_xsrQoCA-Eg"},"source":["# **Data Preprocessing**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7260,"status":"ok","timestamp":1730614677515,"user":{"displayName":"alvin khoo","userId":"02395113757594492728"},"user_tz":-480},"id":"HVLzP4kh0rX4","outputId":"2a7811d4-2b6f-4cb3-8667-4a77bb7a6b8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Before lemmatization: Vocabulary size: 18030, OOV words: 3613\n","After lemmatization: Vocabulary size: 16565, OOV words: 3584\n"]}],"source":["# Initialize the lemmatizer and stopwords\n","lemmatizer = WordNetLemmatizer()\n","punctuation = set(string.punctuation)\n","stop_words = set(stopwords.words('english'))\n","\n","# Dataset Preprocessing to tokenize and/or lemmatize\n","def preprocess_dataset(data, lemmatization = True):\n","    processed_sentences = []\n","\n","    for entry in data:\n","        text = entry['text']\n","        # Tokenize the sentence\n","        tokens = word_tokenize(text)\n","        if lemmatization:\n","          # With case folding, punctuation and stop words removal\n","          tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if word not in punctuation]# and word.lower() not in stop_words]\n","\n","        processed_sentences.append(tokens)\n","\n","    return processed_sentences\n","\n","# Get vocab and oov words\n","def get_vocab_OOV(sentences, w2v_model):\n","    vocabulary = set()\n","    oov_words = set()\n","\n","    for sentence in sentences:\n","        for word in sentence:\n","            vocabulary.add(word)\n","            if word not in w2v_model:\n","                oov_words.add(word)\n","\n","    return vocabulary, oov_words\n","\n","vocabulary, oov_words = get_vocab_OOV(preprocess_dataset(train_dataset, False), w2v_model)\n","lemmatized_vocabulary, lemmatized_oov_words = get_vocab_OOV(preprocess_dataset(train_dataset), w2v_model)\n","\n","print(f\"Before lemmatization: Vocabulary size: {len(vocabulary)}, OOV words: {len(oov_words)}\")\n","print(f\"After lemmatization: Vocabulary size: {len(lemmatized_vocabulary)}, OOV words: {len(lemmatized_oov_words)}\")"]},{"cell_type":"markdown","metadata":{"id":"hbMJLXI8jNuf"},"source":["\n","\n","1.   Lemmatize, case folding and remove stop words for train, validation and test set\n","2.   Drop words not found in train dataset for validation and test set\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"103baxlbkPdk"},"outputs":[],"source":["preprocessed_train_dataset = preprocess_dataset(train_dataset)\n","preprocessed_validation_dataset = preprocess_dataset(validation_dataset)\n","preprocessed_test_dataset = preprocess_dataset(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DAm3fk7PjTyU"},"outputs":[],"source":["def drop_oov(preprocessed_dataset, w2v_model):\n","    return [[word for word in sentence if word in w2v_model] for sentence in preprocessed_dataset]\n","\n","def drop_non_train_vocab(dataset, train_vocab):\n","    return [[word for word in sentence if word in train_vocab] for sentence in dataset]\n","\n","#X_train = drop_oov(preprocessed_train_dataset, w2v_model)\n","X_train = preprocessed_train_dataset\n","Y_train = [entry['label'] for entry in train_dataset]\n","\n","#X_val = drop_non_train_vocab(drop_oov(preprocessed_validation_dataset, w2v_model), lemmatized_vocabulary)\n","X_val = drop_non_train_vocab(preprocessed_validation_dataset, lemmatized_vocabulary)\n","Y_val = [entry['label'] for entry in validation_dataset]\n","\n","#X_test = drop_non_train_vocab(drop_oov(preprocessed_test_dataset, w2v_model), lemmatized_vocabulary)\n","X_test = drop_non_train_vocab(preprocessed_test_dataset, lemmatized_vocabulary)\n","Y_test = [entry['label'] for entry in test_dataset]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1730614680881,"user":{"displayName":"alvin khoo","userId":"02395113757594492728"},"user_tz":-480},"id":"AJybg97IrraL","outputId":"ceb46b49-8e79-46f2-b9f3-6fad7dda73d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}],"source":["print(train_dataset['label'])"]},{"cell_type":"markdown","metadata":{"id":"J4YrSe76AlMa"},"source":["Sanity check: If preprocessed data is wrong"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1730614680882,"user":{"displayName":"alvin khoo","userId":"02395113757594492728"},"user_tz":-480},"id":"J_GAiqg4W27s","outputId":"32c74c91-21ad-4cea-a7df-8a42043f4964"},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train size: 8530\n","Y_train size: 8530\n","X_val size: 1066\n","Y_val size: 1066\n","X_test size: 1066\n","Y_test size: 1066\n"]}],"source":["print(f\"X_train size: {len(X_train)}\")\n","print(f\"Y_train size: {len(Y_train)}\")\n","\n","print(f\"X_val size: {len(X_val)}\")\n","print(f\"Y_val size: {len(Y_val)}\")\n","\n","print(f\"X_test size: {len(X_test)}\")\n","print(f\"Y_test size: {len(Y_test)}\")"]},{"cell_type":"markdown","metadata":{"id":"SoQccpkdlogO"},"source":["# **Generate Embedding**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NCycQdn5M4w-"},"outputs":[],"source":["def get_word2index(train_vocab):\n","    word2index = {}\n","    for idx, word in enumerate(train_vocab):\n","        word2index[word] = idx + 1  # +1 to offset 0 for padding\n","    return word2index\n","\n","word2index = get_word2index(lemmatized_vocabulary)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1730614680885,"user":{"displayName":"alvin khoo","userId":"02395113757594492728"},"user_tz":-480},"id":"Mjn-EoF4XRm2","outputId":"8dee37ed-5247-41c6-a923-48c3d27087af"},"outputs":[{"name":"stdout","output_type":"stream","text":["word2index size = vocab size: 16565\n"]}],"source":["print(f\"word2index size = vocab size: {len(word2index)}\")"]},{"cell_type":"markdown","metadata":{"id":"_aoHg0KYx4te"},"source":["#Statistical Normal Distribution for Embedding Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7630,"status":"ok","timestamp":1730614788667,"user":{"displayName":"alvin khoo","userId":"02395113757594492728"},"user_tz":-480},"id":"j_kJ2FwF-PTq","outputId":"be24a975-beee-477c-e8d0-fdb2ac3ce127"},"outputs":[{"name":"stdout","output_type":"stream","text":["embedding_matrix shape: (16566, 300)\n"]}],"source":["def create_embedding_matrix(w2v_model, word2index, embedding_dim=300):\n","    \"\"\"\n","    Creates an embedding matrix based on the word2index mapping and Word2Vec model.\n","\n","    Args:\n","    - w2v_model: Pre-trained Word2Vec model.\n","    - word2index: Dictionary mapping words to their indices.\n","    - embedding_dim: Dimension of the Word2Vec word vectors (default: 300).\n","\n","    Returns:\n","    - embedding_matrix: Embedding matrix where each row corresponds to the vector of a word in the vocabulary.\n","    \"\"\"\n","\n","    mean = np.mean(w2v_model.vectors, axis=0)\n","    std = np.std(w2v_model.vectors, axis=0)\n","\n","\n","    vocab_size = len(word2index)  # Number of words in the training vocabulary\n","\n","    # Initialize the embedding matrix with zeros (or any other value)\n","    embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n","    num_oov = 0\n","\n","    for word, idx in word2index.items():\n","        if word in w2v_model:\n","            # If the word exists in the Word2Vec model, use its pre-trained embedding\n","            embedding_matrix[idx] = w2v_model[word]\n","        else:\n","            num_oov += 1\n","            embedding_matrix[idx] = np.random.normal(\n","                loc=mean, scale=std, size=(embedding_dim,)\n","            )\n","\n","    return embedding_matrix\n","\n","embedding_dim = w2v_model.vector_size  # Get the dimension of the word vectors in Word2Vec\n","embedding_matrix = create_embedding_matrix(w2v_model, word2index, embedding_dim)\n","print(f\"embedding_matrix shape: {embedding_matrix.shape}\")"]},{"cell_type":"code","source":[],"metadata":{"id":"TpG75hvQQTLb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rBeJ7A9AjUj9"},"source":["# **Sentence2sequence + Padding**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1730614788667,"user":{"displayName":"alvin khoo","userId":"02395113757594492728"},"user_tz":-480},"id":"UhNxEvMJg3z6","outputId":"662ad8bc-0d34-489c-87de-a6bf1c4e1466"},"outputs":[{"name":"stdout","output_type":"stream","text":["Min sentence length in dataset 0: 1\n","Min sentence length in dataset 1: 0\n","Min sentence length in dataset 2: 1\n","Maximum sentence length of three datasets: 53\n"]}],"source":["# Get maximum sentence length\n","def get_max_sentence_length(*datasets):\n","    \"\"\"\n","    Get the maximum sentence length from the provided datasets.\n","\n","    Args:\n","    - datasets: A variable number of lists containing tokenized sentences.\n","\n","    Returns:\n","    - max_length: The maximum sentence length across all datasets.\n","    \"\"\"\n","    max_length = 0\n","    for index, dataset in enumerate(datasets):\n","        # Update max_length if a longer sentence is found\n","        max_length = max(max_length, max(len(sentence) for sentence in dataset))\n","        min_length = min(len(sentence) for sentence in dataset)\n","        print(f\"Min sentence length in dataset {index}: {min_length}\")\n","    return max_length\n","\n","# Example usage\n","max_length = get_max_sentence_length(X_train, X_val, X_test)\n","print(f\"Maximum sentence length of three datasets: {max_length}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9NIEsB6inUbF"},"outputs":[],"source":["def convert_to_sequence_data(tokenized_data, word2index, max_length=max_length):\n","    \"\"\"\n","    Preprocess the tokenized data for RNN input.\n","\n","    Args:\n","    - tokenized_data: List of tokenized sentences.\n","    - word2index: Dictionary mapping words to indices.\n","    - max_length: Maximum length for padding (default is 100).\n","\n","    Returns:\n","    - padded_sequences: Padded tensor of sequences.\n","    \"\"\"\n","    # Convert tokens to indices\n","    indexed_sequences = []\n","    for sentence in tokenized_data:\n","        indexed_sentence = [word2index.get(word, 0) for word in sentence]  # Use 0 for OOV words\n","\n","        # Truncate or pad the indexed sentence to the specified max_length\n","        if len(indexed_sentence) > max_length:\n","            indexed_sentence = indexed_sentence[:max_length]  # Truncate\n","        else:\n","            indexed_sentence += [0] * (max_length - len(indexed_sentence))  # Pad with zeros\n","\n","        indexed_sequences.append(torch.tensor(indexed_sentence, dtype=torch.long))\n","\n","    # Convert list of tensors to a padded tensor\n","    padded_sequences = torch.stack(indexed_sequences)\n","\n","    return padded_sequences\n","\n","# Convert to sequence\n","X_train_sequence = convert_to_sequence_data(X_train, word2index)\n","X_val_sequence = convert_to_sequence_data(X_val , word2index)\n","X_test_sequence = convert_to_sequence_data(X_test, word2index)\n"]},{"cell_type":"markdown","metadata":{"id":"UZ21miFC_7Y8"},"source":["Sanity check: if the sequenced tokens mapped correctly to embedding matrix and w2v_model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1730614789420,"user":{"displayName":"alvin khoo","userId":"02395113757594492728"},"user_tz":-480},"id":"3pyohagSZBpR","outputId":"627daa8d-06f8-4095-e434-7a0ea5258dad"},"outputs":[{"data":{"text/plain":["array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True])"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# Check if the sequenced tokens mapped correctly to embedding matrix and w2v_model\n","embedding_matrix[X_train_sequence[0][0]] == w2v_model[X_train[0][0]]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1730614789420,"user":{"displayName":"alvin khoo","userId":"02395113757594492728"},"user_tz":-480},"id":"oOQlYEtdZSpp","outputId":"4654723b-158e-4252-95bf-174bfde85861"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X_train_sequence: torch.Size([8530, 53])\n","Shape of X_valn_sequence: torch.Size([1066, 53])\n","Shape of X_test_sequence: torch.Size([1066, 53])\n"]}],"source":["print(f\"Shape of X_train_sequence: {X_train_sequence.shape}\")\n","print(f\"Shape of X_valn_sequence: {X_val_sequence.shape}\")\n","print(f\"Shape of X_test_sequence: {X_test_sequence.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"Uw7KZcp0peCk"},"source":["# Create Tensor datasets\n","train_dataset = TensorDataset(X_train_sequence, torch.tensor(Y_train, dtype=torch.long))\n","val_dataset = TensorDataset(X_val_sequence, torch.tensor(Y_val, dtype=torch.long))\n","test_dataset = TensorDataset(X_test_sequence, torch.tensor(Y_test, dtype=torch.long))\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    max_accuracy = 0\n","\n","    # Reset the seed to ensure fairness when comparing performances\n","    set_seed()\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])  # Hidden size choices\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])  # Number of RNN layers choices\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])  # Learning rate choices\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])  # Batch size choices\n","    num_epochs = trial.suggest_categorical('num_epochs', [10, 20, 30, 40, 50])  # num epoch choices\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])  # Optimizer choices\n","\n","\n","    # Create data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","    # Initialize the model\n","    model = SimpleRNN(embedding_matrix, hidden_size, num_layers)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","    \n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Lists to store loss and accuracy for visualization\n","    train_losses = []\n","    val_accuracies = []\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()  # Set model to evaluation mode\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():  # Disable gradient computation\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)  # Forward pass\n","                predicted = (outputs.squeeze() > 0.5).float()  # Apply threshold\n","\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()  # Count correct predictions\n","\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    for epoch in range(num_epochs):\n","        model.train()  # Set model to training mode\n","        epoch_loss = 0  # To accumulate loss for this epoch\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()  # Zero the gradients\n","            outputs = model(inputs)  # Forward pass\n","\n","            # Calculate loss\n","            loss = criterion(outputs.squeeze(), labels.float())  # Squeeze to match dimensions\n","            loss.backward()  # Backpropagation\n","            optimizer.step()  # Update parameters\n","\n","            epoch_loss += loss.item()  # Accumulate loss\n","\n","        # Calculate average loss for the epoch\n","        avg_loss = epoch_loss / len(train_loader)\n","        train_losses.append(avg_loss)\n","\n","        # Evaluate on validation set\n","        val_accuracy = evaluate_model(val_loader)\n","        val_accuracies.append(val_accuracy)\n","        if val_accuracy>max_accuracy:\n","          max_accuracy = val_accuracy\n","\n","        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    return max_accuracy  # Return validation accuracy for optimization\n","\n","# Create a study object for Optuna\n","study = optuna.create_study(direction='maximize')  # We want to maximize validation accuracy\n","study.optimize(objective, n_trials=100)  # Run 100 trials"]},{"cell_type":"markdown","source":["#**Simple BiLSTM Architecture**"],"metadata":{"id":"QqvVVoFSRRVm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4yWQmjfVyLis"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class SimpleBiLSTM(nn.Module):\n","    def __init__(self, embedding_matrix, hidden_size, num_layers, aggregation='last_hidden_state', freeze_embedding=True):\n","        super(SimpleBiLSTM, self).__init__()\n","        embedding_dim = embedding_matrix.shape[1]\n","\n","        self.aggregation = aggregation\n","        self.hidden_size = hidden_size  # Store hidden size for reference\n","\n","        # Embedding layer using a pretrained embedding matrix\n","        self.embedding = nn.Embedding.from_pretrained(\n","            torch.tensor(embedding_matrix, dtype=torch.float32),\n","            padding_idx=0,  # Using 0 for padding index\n","            freeze=freeze_embedding,  # Freeze embeddings\n","        )\n","\n","        # BiLSTM layer with bidirectional=True\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","        self.dropout = nn.Dropout(0.2)\n","\n","        # Output layer dimensions adjusted for bidirectional LSTM (2 * hidden_size)\n","        if self.aggregation == \"max_mean_pool\":\n","            self.fc = nn.Linear(4 * hidden_size, 1)  # Output size is 1 for binary classification\n","        else:\n","            self.fc = nn.Linear(2 * hidden_size, 1)\n","\n","\n","    def forward(self, x):\n","        # Pass through the embedding layer\n","        embedded = self.embedding(x)\n","\n","        # Initialize hidden and cell states for both directions (2*num_layers for bidirectional)\n","        h0 = torch.zeros(2 * self.lstm.num_layers, embedded.size(0), self.hidden_size).to(embedded.device)\n","        c0 = torch.zeros(2 * self.lstm.num_layers, embedded.size(0), self.hidden_size).to(embedded.device)\n","\n","        # Forward pass through BiLSTM\n","        out, (hn, cn) = self.lstm(embedded, (h0, c0))\n","\n","        # Aggregation method\n","        if self.aggregation == 'last_hidden_state':\n","            # Concatenate the last hidden states from both directions\n","            out = torch.cat((hn[-2], hn[-1]), dim=1)  # -2 and -1 refer to the last forward and backward states\n","        elif self.aggregation == 'max_pool':\n","            # Mask out padding tokens for max pooling\n","            mask = (x != self.embedding.padding_idx).unsqueeze(-1)\n","            out = out.masked_fill(~mask, -100000)\n","            out, _ = torch.max(out, dim=1)\n","        elif self.aggregation == 'mean_pool':\n","            # Mask out padding tokens for mean pooling\n","            mask = (x != self.embedding.padding_idx).unsqueeze(-1)\n","            out = out * mask\n","            summed = out.sum(dim=1)\n","            valid_counts = mask.sum(dim=1).clamp(min=1)\n","            out = summed / valid_counts\n","\n","        elif self.aggregation == 'max_mean_pool':\n","            # Mask out padding tokens for pooling\n","            mask = (x != self.embedding.padding_idx).unsqueeze(-1)\n","\n","            # Max pooling\n","            max_pooled_out = out.masked_fill(~mask, -100000)  # Mask padding tokens with a large negative value\n","            max_pooled_out, _ = torch.max(max_pooled_out, dim=1)\n","\n","            # Mean pooling\n","            mean_pooled_out = out * mask\n","            summed = mean_pooled_out.sum(dim=1)\n","            valid_counts = mask.sum(dim=1).clamp(min=1)\n","            mean_pooled_out = summed / valid_counts\n","\n","            # Concatenate max and mean pooling outputs\n","            out = torch.cat((max_pooled_out, mean_pooled_out), dim=1)\n","\n","        out = self.dropout(out)  # Apply dropout\n","        out = self.fc(out)  # Final fully connected layer\n","\n","        # Sigmoid activation for binary classification\n","        out = torch.sigmoid(out)\n","\n","        return out\n"]},{"cell_type":"markdown","metadata":{"id":"Pb5-OrD7_v0W"},"source":["#**BiLSTM - Word2Vec (MaxPool - Stat Normal)**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4HJ3Rthx-pW","outputId":"b6750a59-b66c-4951-aba5-1b6d21d40ec5"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-10-28 01:48:12,675] A new study created in memory with name: no-name-6ad44cba-23c3-4295-8442-1956133eca0d\n","[I 2024-10-28 01:49:23,739] Trial 0 finished with value: 0.775797373358349 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.775797373358349.\n","[I 2024-10-28 01:49:52,164] Trial 1 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 10, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7861163227016885.\n","[I 2024-10-28 01:50:05,928] Trial 2 finished with value: 0.7129455909943715 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 10, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7861163227016885.\n","[I 2024-10-28 01:50:47,674] Trial 3 finished with value: 0.575046904315197 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7861163227016885.\n","[I 2024-10-28 01:54:24,543] Trial 4 finished with value: 0.7908067542213884 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.7908067542213884.\n","[I 2024-10-28 01:56:06,638] Trial 5 finished with value: 0.7954971857410882 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 5 with value: 0.7954971857410882.\n","[I 2024-10-28 01:56:46,092] Trial 6 finished with value: 0.7504690431519699 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 5 with value: 0.7954971857410882.\n","[I 2024-10-28 01:59:27,595] Trial 7 finished with value: 0.49906191369606 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 5 with value: 0.7954971857410882.\n","[I 2024-10-28 01:59:54,398] Trial 8 finished with value: 0.7926829268292683 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 5 with value: 0.7954971857410882.\n","[I 2024-10-28 02:00:33,002] Trial 9 finished with value: 0.7607879924953096 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 5 with value: 0.7954971857410882.\n","[I 2024-10-28 02:01:40,943] Trial 10 finished with value: 0.7954971857410882 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 20, 'optimizer': 'SGD'}. Best is trial 5 with value: 0.7954971857410882.\n","[I 2024-10-28 02:02:49,076] Trial 11 finished with value: 0.7954971857410882 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 20, 'optimizer': 'SGD'}. Best is trial 5 with value: 0.7954971857410882.\n","[I 2024-10-28 02:03:57,133] Trial 12 finished with value: 0.7954971857410882 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 20, 'optimizer': 'SGD'}. Best is trial 5 with value: 0.7954971857410882.\n","[I 2024-10-28 02:05:38,761] Trial 13 finished with value: 0.50093808630394 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 5 with value: 0.7954971857410882.\n","[I 2024-10-28 02:06:46,768] Trial 14 finished with value: 0.7954971857410882 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 20, 'optimizer': 'SGD'}. Best is trial 5 with value: 0.7954971857410882.\n","[I 2024-10-28 02:07:30,333] Trial 15 finished with value: 0.8030018761726079 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:08:20,384] Trial 16 finished with value: 0.7926829268292683 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:09:03,798] Trial 17 finished with value: 0.49906191369606 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:09:47,336] Trial 18 finished with value: 0.8030018761726079 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:11:02,563] Trial 19 finished with value: 0.50093808630394 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:11:46,188] Trial 20 finished with value: 0.8030018761726079 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:12:29,827] Trial 21 finished with value: 0.8030018761726079 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:13:13,360] Trial 22 finished with value: 0.8030018761726079 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:13:56,867] Trial 23 finished with value: 0.8030018761726079 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:14:40,300] Trial 24 finished with value: 0.49906191369606 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:14:54,800] Trial 25 finished with value: 0.8030018761726079 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 10, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:16:07,169] Trial 26 finished with value: 0.7917448405253283 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:16:50,705] Trial 27 finished with value: 0.8030018761726079 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:17:29,218] Trial 28 finished with value: 0.6951219512195121 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:18:05,346] Trial 29 finished with value: 0.774859287054409 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:18:28,181] Trial 30 finished with value: 0.7917448405253283 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:19:11,846] Trial 31 finished with value: 0.8030018761726079 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:19:55,518] Trial 32 finished with value: 0.8030018761726079 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:20:10,080] Trial 33 finished with value: 0.5234521575984991 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 10, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:20:53,685] Trial 34 finished with value: 0.8030018761726079 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:21:07,127] Trial 35 finished with value: 0.7898686679174484 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 10, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:22:26,844] Trial 36 finished with value: 0.7917448405253283 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:22:59,555] Trial 37 finished with value: 0.50093808630394 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:23:43,465] Trial 38 finished with value: 0.5 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:24:25,957] Trial 39 finished with value: 0.7851782363977486 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:24:49,635] Trial 40 finished with value: 0.7842401500938087 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:25:33,309] Trial 41 finished with value: 0.8030018761726079 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:26:17,075] Trial 42 finished with value: 0.8030018761726079 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:27:00,777] Trial 43 finished with value: 0.8030018761726079 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:27:44,185] Trial 44 finished with value: 0.49906191369606 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:28:52,991] Trial 45 finished with value: 0.49906191369606 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:29:36,454] Trial 46 finished with value: 0.6200750469043153 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n","[I 2024-10-28 02:29:50,514] Trial 47 finished with value: 0.7851782363977486 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 20, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079.\n"]}],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Reset the seed for fairness\n","    torch.manual_seed(42)\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [10, 20, 30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation=\"max_pool\",freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","# Start the Optuna study\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)\n","#[I 2024-10-28 02:07:30,333] Trial 15 finished with value: 0.8030018761726079 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SNjdZKTPeKXb"},"outputs":[],"source":["# The Best Hyperparameters\n","hidden_size = 128  # Hidden size choices\n","num_layers = 2 # Number of RNN layers choices\n","learning_rate = 0.1  # Learning rate choices\n","batch_size = 64  # Batch size choices\n","num_epochs = 60  # num epoch choices\n","optimizer_name = 'SGD'  # Optimizer choices\n","#[I 2024-10-28 02:07:30,333] Trial 15 finished with value: 0.8030018761726079 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8030018761726079."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57802,"status":"ok","timestamp":1730603350356,"user":{"displayName":"ALEX KHOO SHIEN HOW","userId":"09575417203520826139"},"user_tz":-480},"id":"wX9h0kLdx-r-","outputId":"55770b3c-1fd3-4f0e-d847-edf875e76201"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/60], Train Loss: 0.6929, Validation Accuracy: 0.4991\n","Epoch [2/60], Train Loss: 0.6865, Validation Accuracy: 0.5797\n","Epoch [3/60], Train Loss: 0.6249, Validation Accuracy: 0.6886\n","Epoch [4/60], Train Loss: 0.5184, Validation Accuracy: 0.7402\n","Epoch [5/60], Train Loss: 0.4826, Validation Accuracy: 0.7495\n","Epoch [6/60], Train Loss: 0.4658, Validation Accuracy: 0.7598\n","Epoch [7/60], Train Loss: 0.4591, Validation Accuracy: 0.7786\n","Epoch [8/60], Train Loss: 0.4478, Validation Accuracy: 0.7711\n","Epoch [9/60], Train Loss: 0.4287, Validation Accuracy: 0.7692\n","Epoch [10/60], Train Loss: 0.4265, Validation Accuracy: 0.7899\n","Epoch [11/60], Train Loss: 0.4166, Validation Accuracy: 0.7777\n","Epoch [12/60], Train Loss: 0.4102, Validation Accuracy: 0.7824\n","Epoch [13/60], Train Loss: 0.3998, Validation Accuracy: 0.8030\n","Epoch [14/60], Train Loss: 0.3859, Validation Accuracy: 0.7889\n","Epoch [15/60], Train Loss: 0.3823, Validation Accuracy: 0.7946\n","Epoch [16/60], Train Loss: 0.3731, Validation Accuracy: 0.7561\n","Epoch [17/60], Train Loss: 0.3630, Validation Accuracy: 0.7833\n","Epoch [18/60], Train Loss: 0.3570, Validation Accuracy: 0.7692\n","Epoch [19/60], Train Loss: 0.3493, Validation Accuracy: 0.7664\n","Epoch [20/60], Train Loss: 0.3466, Validation Accuracy: 0.7871\n","Epoch [21/60], Train Loss: 0.3333, Validation Accuracy: 0.7645\n","Epoch [22/60], Train Loss: 0.3217, Validation Accuracy: 0.7627\n","Epoch [23/60], Train Loss: 0.3130, Validation Accuracy: 0.7730\n","Epoch [24/60], Train Loss: 0.2969, Validation Accuracy: 0.7814\n","Epoch [25/60], Train Loss: 0.2883, Validation Accuracy: 0.7692\n","Epoch [26/60], Train Loss: 0.2818, Validation Accuracy: 0.7805\n","Epoch [27/60], Train Loss: 0.2727, Validation Accuracy: 0.7749\n","Epoch [28/60], Train Loss: 0.2594, Validation Accuracy: 0.7767\n","Epoch [29/60], Train Loss: 0.2465, Validation Accuracy: 0.7692\n","Epoch [30/60], Train Loss: 0.2318, Validation Accuracy: 0.7683\n","Epoch [31/60], Train Loss: 0.2230, Validation Accuracy: 0.7674\n","Epoch [32/60], Train Loss: 0.2061, Validation Accuracy: 0.7767\n","Epoch [33/60], Train Loss: 0.1863, Validation Accuracy: 0.7795\n","Epoch [34/60], Train Loss: 0.1823, Validation Accuracy: 0.7664\n","Epoch [35/60], Train Loss: 0.1732, Validation Accuracy: 0.7777\n","Epoch [36/60], Train Loss: 0.1514, Validation Accuracy: 0.7617\n","Epoch [37/60], Train Loss: 0.1383, Validation Accuracy: 0.7814\n","Epoch [38/60], Train Loss: 0.1214, Validation Accuracy: 0.7702\n","Epoch [39/60], Train Loss: 0.1103, Validation Accuracy: 0.7645\n","Epoch [40/60], Train Loss: 0.1042, Validation Accuracy: 0.7880\n","Epoch [41/60], Train Loss: 0.0779, Validation Accuracy: 0.7598\n","Epoch [42/60], Train Loss: 0.0856, Validation Accuracy: 0.7861\n","Epoch [43/60], Train Loss: 0.0646, Validation Accuracy: 0.7777\n","Early stopping triggered!\n","Using Best Model, Test Accuracy: 0.7974\n"]}],"source":["# Reset the seed to ensure fairness when comparing performances\n","set_seed()\n","\n","# Load Data\n","train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","\n","# Initialize the model\n","model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation='max_pool')\n","model.to(device)\n","# Use BCELoss for binary classification\n","criterion = nn.BCELoss()\n","# Choose optimizer based on trial suggestion\n","optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","# Initialize the early stopper\n","early_stopper = EarlyStopper()\n","\n","# Lists to store loss and accuracy for visualization\n","train_losses = []\n","val_accuracies = []\n","\n","# Training Loop\n","best_val_accuracy = 0\n","best_model = None\n","for epoch in range(num_epochs):\n","    # Train on train set\n","    train_loss = train_loop(train_loader, model, criterion, optimizer)\n","    train_losses.append(train_loss)\n","\n","    # Evaluate on validation set\n","    val_accuracy = test_loop(val_loader, model)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Get the best model\n","    if val_accuracy > best_val_accuracy:\n","      best_val_accuracy = val_accuracy\n","      best_model = copy.deepcopy(model)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    #Early stopping\n","    if early_stopper.early_stop(val_accuracy):\n","       print(\"Early stopping triggered!\")\n","       break\n","\n","# Evaluate on test data\n","test_accuracy = test_loop(test_loader, best_model)\n","\n","# Print out the performance\n","print(f\"Using Best Model, Test Accuracy: {test_accuracy:.4f}\") #0.7974"]},{"cell_type":"markdown","metadata":{"id":"LhA3CF8YAA7C"},"source":["#**BiLSTM - Word2Vec (MeanPool - Stat Normal)**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-NWuzR3jx-wF","outputId":"f1e4992e-ae2a-4869-8c1b-360455ad4623"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-10-28 05:58:41,373] A new study created in memory with name: no-name-0564e1ac-11b6-4250-b76c-2294aca3647e\n","[I 2024-10-28 05:59:21,760] Trial 0 finished with value: 0.7842401500938087 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.7842401500938087.\n","[I 2024-10-28 05:59:39,364] Trial 1 finished with value: 0.6529080675422139 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 10, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.7842401500938087.\n","[I 2024-10-28 06:01:31,378] Trial 2 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:02:23,284] Trial 3 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:03:00,774] Trial 4 finished with value: 0.7626641651031895 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 20, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:03:57,035] Trial 5 finished with value: 0.773921200750469 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 20, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:05:18,532] Trial 6 finished with value: 0.7514071294559099 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:07:36,077] Trial 7 finished with value: 0.7692307692307693 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:08:46,725] Trial 8 finished with value: 0.7617260787992496 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:09:15,240] Trial 9 finished with value: 0.7673545966228893 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:11:07,619] Trial 10 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:13:00,087] Trial 11 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:14:52,661] Trial 12 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:16:45,173] Trial 13 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:16:45,173] Trial 13 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:18:37,610] Trial 14 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:18:37,610] Trial 14 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:19:13,352] Trial 15 finished with value: 0.7654784240150094 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 10, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:19:13,352] Trial 15 finished with value: 0.7654784240150094 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 10, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:20:53,841] Trial 16 finished with value: 0.775797373358349 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:20:53,841] Trial 16 finished with value: 0.775797373358349 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:22:46,310] Trial 17 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:22:46,310] Trial 17 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:23:59,806] Trial 18 finished with value: 0.7804878048780488 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:23:59,806] Trial 18 finished with value: 0.7804878048780488 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:24:05,519] Trial 19 finished with value: 0.5075046904315197 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 10, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:24:05,519] Trial 19 finished with value: 0.5075046904315197 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 10, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:25:20,571] Trial 20 finished with value: 0.7720450281425891 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 20, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:25:20,571] Trial 20 finished with value: 0.7720450281425891 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 20, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:27:13,057] Trial 21 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.7861163227016885.\n","[I 2024-10-28 06:27:13,057] Trial 21 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.7861163227016885.\n"]}],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Reset the seed for fairness\n","    torch.manual_seed(42)\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [10, 20, 30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers,freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","# Start the Optuna study\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)\n","#[I 2024-10-28 05:12:08,917] Trial 19 finished with value: 0.7917448405253283 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 19 with value: 0.7917448405253283."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tIP3Yn85e4El"},"outputs":[],"source":["#[I 2024-10-28 05:12:08,917] Trial 19 finished with value: 0.7917448405253283 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 19 with value: 0.7917448405253283.\n","# The Best Hyperparameters\n","hidden_size = 128  # Hidden size choices\n","num_layers = 2 # Number of RNN layers choices\n","learning_rate = 0.0001  # Learning rate choices\n","batch_size = 64  # Batch size choices\n","num_epochs = 50  # num epoch choices\n","optimizer_name = 'RMSprop'  # Optimizer choices\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59105,"status":"ok","timestamp":1730603450521,"user":{"displayName":"ALEX KHOO SHIEN HOW","userId":"09575417203520826139"},"user_tz":-480},"id":"CaDa7r4fe4G7","outputId":"befc029d-e700-437a-e0a2-4342035be87e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/50], Train Loss: 0.5933, Validation Accuracy: 0.7139\n","Epoch [2/50], Train Loss: 0.4999, Validation Accuracy: 0.7364\n","Epoch [3/50], Train Loss: 0.4796, Validation Accuracy: 0.7617\n","Epoch [4/50], Train Loss: 0.4689, Validation Accuracy: 0.7458\n","Epoch [5/50], Train Loss: 0.4581, Validation Accuracy: 0.7495\n","Epoch [6/50], Train Loss: 0.4551, Validation Accuracy: 0.7608\n","Epoch [7/50], Train Loss: 0.4472, Validation Accuracy: 0.7627\n","Epoch [8/50], Train Loss: 0.4418, Validation Accuracy: 0.7608\n","Epoch [9/50], Train Loss: 0.4366, Validation Accuracy: 0.7758\n","Epoch [10/50], Train Loss: 0.4321, Validation Accuracy: 0.7608\n","Epoch [11/50], Train Loss: 0.4263, Validation Accuracy: 0.7645\n","Epoch [12/50], Train Loss: 0.4220, Validation Accuracy: 0.7355\n","Epoch [13/50], Train Loss: 0.4179, Validation Accuracy: 0.7767\n","Epoch [14/50], Train Loss: 0.4119, Validation Accuracy: 0.7899\n","Epoch [15/50], Train Loss: 0.4082, Validation Accuracy: 0.7749\n","Epoch [16/50], Train Loss: 0.4023, Validation Accuracy: 0.7645\n","Epoch [17/50], Train Loss: 0.3992, Validation Accuracy: 0.7889\n","Epoch [18/50], Train Loss: 0.3928, Validation Accuracy: 0.7467\n","Epoch [19/50], Train Loss: 0.3905, Validation Accuracy: 0.7523\n","Epoch [20/50], Train Loss: 0.3845, Validation Accuracy: 0.7749\n","Epoch [21/50], Train Loss: 0.3831, Validation Accuracy: 0.7786\n","Epoch [22/50], Train Loss: 0.3745, Validation Accuracy: 0.7608\n","Epoch [23/50], Train Loss: 0.3719, Validation Accuracy: 0.7598\n","Epoch [24/50], Train Loss: 0.3663, Validation Accuracy: 0.7871\n","Epoch [25/50], Train Loss: 0.3612, Validation Accuracy: 0.7777\n","Epoch [26/50], Train Loss: 0.3571, Validation Accuracy: 0.7814\n","Epoch [27/50], Train Loss: 0.3532, Validation Accuracy: 0.7674\n","Epoch [28/50], Train Loss: 0.3519, Validation Accuracy: 0.7786\n","Epoch [29/50], Train Loss: 0.3422, Validation Accuracy: 0.7795\n","Epoch [30/50], Train Loss: 0.3375, Validation Accuracy: 0.7749\n","Epoch [31/50], Train Loss: 0.3348, Validation Accuracy: 0.7833\n","Epoch [32/50], Train Loss: 0.3329, Validation Accuracy: 0.7814\n","Epoch [33/50], Train Loss: 0.3282, Validation Accuracy: 0.7842\n","Epoch [34/50], Train Loss: 0.3221, Validation Accuracy: 0.7786\n","Epoch [35/50], Train Loss: 0.3168, Validation Accuracy: 0.7880\n","Epoch [36/50], Train Loss: 0.3149, Validation Accuracy: 0.7824\n","Epoch [37/50], Train Loss: 0.3136, Validation Accuracy: 0.6970\n","Epoch [38/50], Train Loss: 0.3086, Validation Accuracy: 0.7786\n","Epoch [39/50], Train Loss: 0.2993, Validation Accuracy: 0.7702\n","Epoch [40/50], Train Loss: 0.2943, Validation Accuracy: 0.7730\n","Epoch [41/50], Train Loss: 0.2946, Validation Accuracy: 0.7880\n","Epoch [42/50], Train Loss: 0.2913, Validation Accuracy: 0.7889\n","Epoch [43/50], Train Loss: 0.2806, Validation Accuracy: 0.7842\n","Epoch [44/50], Train Loss: 0.2780, Validation Accuracy: 0.7889\n","Early stopping triggered!\n","Using Best Model, Test Accuracy: 0.7730\n"]}],"source":["# Reset the seed to ensure fairness when comparing performances\n","set_seed()\n","\n","# Load Data\n","train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","\n","# Initialize the model\n","model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation='mean_pool')\n","model.to(device)\n","# Use BCELoss for binary classification\n","criterion = nn.BCELoss()\n","# Choose optimizer based on trial suggestion\n","optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","# Initialize the early stopper\n","early_stopper = EarlyStopper()\n","\n","# Lists to store loss and accuracy for visualization\n","train_losses = []\n","val_accuracies = []\n","\n","# Training Loop\n","best_val_accuracy = 0\n","best_model = None\n","for epoch in range(num_epochs):\n","    # Train on train set\n","    train_loss = train_loop(train_loader, model, criterion, optimizer)\n","    train_losses.append(train_loss)\n","\n","    # Evaluate on validation set\n","    val_accuracy = test_loop(val_loader, model)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Get the best model\n","    if val_accuracy > best_val_accuracy:\n","      best_val_accuracy = val_accuracy\n","      best_model = copy.deepcopy(model)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    #Early stopping\n","    if early_stopper.early_stop(val_accuracy):\n","       print(\"Early stopping triggered!\")\n","       break\n","\n","# Evaluate on test data\n","test_accuracy = test_loop(test_loader, best_model)\n","\n","# Print out the performance\n","print(f\"Using Best Model, Test Accuracy: {test_accuracy:.4f}\") #0.7730"]},{"cell_type":"markdown","metadata":{"id":"1TIqEJCwAFug"},"source":["#**BiLSTM - Word2Vec (LHS - Stat Normal)**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4420174,"status":"ok","timestamp":1730170500405,"user":{"displayName":"Alvin Khoo","userId":"12471070573568746260"},"user_tz":-480},"id":"zUDWk5OCe4JT","outputId":"d4434379-64d6-4362-c590-ed888c00f2ae"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-10-29 01:41:20,112] A new study created in memory with name: no-name-ec64ac70-d214-4832-9729-40a23948e006\n","[I 2024-10-29 01:42:07,480] Trial 0 finished with value: 0.775797373358349 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.775797373358349.\n","[I 2024-10-29 01:43:16,494] Trial 1 finished with value: 0.7673545966228893 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.775797373358349.\n","[I 2024-10-29 01:44:54,631] Trial 2 finished with value: 0.5 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.775797373358349.\n","[I 2024-10-29 01:46:12,391] Trial 3 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 01:47:27,331] Trial 4 finished with value: 0.7448405253283302 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 01:47:47,326] Trial 5 finished with value: 0.7157598499061913 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 01:48:25,754] Trial 6 finished with value: 0.625703564727955 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 01:50:24,988] Trial 7 finished with value: 0.7129455909943715 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 01:52:13,180] Trial 8 finished with value: 0.5 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 01:53:10,047] Trial 9 finished with value: 0.7514071294559099 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 01:53:45,785] Trial 10 finished with value: 0.775797373358349 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 01:55:07,338] Trial 11 finished with value: 0.5975609756097561 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 01:55:51,014] Trial 12 finished with value: 0.5440900562851783 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 01:56:46,908] Trial 13 finished with value: 0.5056285178236398 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 01:57:29,106] Trial 14 finished with value: 0.7729831144465291 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 01:58:08,968] Trial 15 finished with value: 0.6022514071294559 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 01:59:14,119] Trial 16 finished with value: 0.525328330206379 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:00:20,181] Trial 17 finished with value: 0.5778611632270169 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:03:05,719] Trial 18 finished with value: 0.5065666041275797 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:03:52,804] Trial 19 finished with value: 0.7729831144465291 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:05:02,703] Trial 20 finished with value: 0.5 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:05:38,947] Trial 21 finished with value: 0.775797373358349 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:06:15,383] Trial 22 finished with value: 0.775797373358349 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:06:50,985] Trial 23 finished with value: 0.775797373358349 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:07:27,399] Trial 24 finished with value: 0.775797373358349 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:07:52,825] Trial 25 finished with value: 0.5694183864915572 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:08:42,827] Trial 26 finished with value: 0.773921200750469 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:09:19,096] Trial 27 finished with value: 0.775797373358349 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:10:43,477] Trial 28 finished with value: 0.7720450281425891 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:12:13,740] Trial 29 finished with value: 0.5 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:13:19,496] Trial 30 finished with value: 0.5572232645403377 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:13:55,723] Trial 31 finished with value: 0.775797373358349 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:14:32,001] Trial 32 finished with value: 0.775797373358349 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:15:07,504] Trial 33 finished with value: 0.775797373358349 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:15:43,716] Trial 34 finished with value: 0.7626641651031895 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:16:09,284] Trial 35 finished with value: 0.7542213883677298 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:19:42,288] Trial 36 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:21:49,259] Trial 37 finished with value: 0.5 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:23:46,258] Trial 38 finished with value: 0.5 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:26:36,543] Trial 39 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:29:26,509] Trial 40 finished with value: 0.774859287054409 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:32:16,820] Trial 41 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:35:06,970] Trial 42 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:37:57,540] Trial 43 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:40:47,868] Trial 44 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:43:38,190] Trial 45 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:46:28,761] Trial 46 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:49:19,000] Trial 47 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:52:09,438] Trial 48 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n","[I 2024-10-29 02:54:59,973] Trial 49 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7833020637898687.\n"]}],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Reset the seed for fairness\n","    torch.manual_seed(42)\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers,freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","# Start the Optuna study\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)\n","\n","#[I 2024-10-28 07:38:51,176] Trial 6 finished with value: 0.7926829268292683 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 6 with value: 0.7926829268292683."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W2mTC8Mbe4Qe"},"outputs":[],"source":["hidden_size = 128  # Hidden size choices\n","num_layers = 2 # Number of RNN layers choices\n","learning_rate = 0.001  # Learning rate choices\n","batch_size = 64  # Batch size choices\n","num_epochs = 50  # num epoch choices\n","optimizer_name = 'Adam'  # Optimizer choices\n","\n","#[I 2024-10-28 07:38:51,176] Trial 6 finished with value: 0.7926829268292683 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 6 with value: 0.7926829268292683."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49261,"status":"ok","timestamp":1730603594758,"user":{"displayName":"ALEX KHOO SHIEN HOW","userId":"09575417203520826139"},"user_tz":-480},"id":"wSlztNQydazy","outputId":"e1f298d9-0cc3-4f86-ddc9-abd5aa4d0d41"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/50], Train Loss: 0.5748, Validation Accuracy: 0.7580\n","Epoch [2/50], Train Loss: 0.4762, Validation Accuracy: 0.7523\n","Epoch [3/50], Train Loss: 0.4509, Validation Accuracy: 0.7758\n","Epoch [4/50], Train Loss: 0.4283, Validation Accuracy: 0.7842\n","Epoch [5/50], Train Loss: 0.4113, Validation Accuracy: 0.7852\n","Epoch [6/50], Train Loss: 0.3887, Validation Accuracy: 0.7908\n","Epoch [7/50], Train Loss: 0.3601, Validation Accuracy: 0.7786\n","Epoch [8/50], Train Loss: 0.3353, Validation Accuracy: 0.7758\n","Epoch [9/50], Train Loss: 0.2949, Validation Accuracy: 0.7889\n","Epoch [10/50], Train Loss: 0.2668, Validation Accuracy: 0.7824\n","Epoch [11/50], Train Loss: 0.2231, Validation Accuracy: 0.7814\n","Epoch [12/50], Train Loss: 0.2009, Validation Accuracy: 0.7833\n","Epoch [13/50], Train Loss: 0.1650, Validation Accuracy: 0.7720\n","Epoch [14/50], Train Loss: 0.1393, Validation Accuracy: 0.7786\n","Epoch [15/50], Train Loss: 0.1128, Validation Accuracy: 0.7664\n","Epoch [16/50], Train Loss: 0.0886, Validation Accuracy: 0.7795\n","Epoch [17/50], Train Loss: 0.0819, Validation Accuracy: 0.7598\n","Epoch [18/50], Train Loss: 0.0733, Validation Accuracy: 0.7674\n","Epoch [19/50], Train Loss: 0.0444, Validation Accuracy: 0.7552\n","Epoch [20/50], Train Loss: 0.0471, Validation Accuracy: 0.7467\n","Epoch [21/50], Train Loss: 0.0362, Validation Accuracy: 0.7636\n","Epoch [22/50], Train Loss: 0.0528, Validation Accuracy: 0.7655\n","Epoch [23/50], Train Loss: 0.0486, Validation Accuracy: 0.7514\n","Epoch [24/50], Train Loss: 0.0176, Validation Accuracy: 0.7683\n","Epoch [25/50], Train Loss: 0.0154, Validation Accuracy: 0.7645\n","Epoch [26/50], Train Loss: 0.0337, Validation Accuracy: 0.7598\n","Epoch [27/50], Train Loss: 0.0149, Validation Accuracy: 0.7617\n","Epoch [28/50], Train Loss: 0.0149, Validation Accuracy: 0.7608\n","Epoch [29/50], Train Loss: 0.0068, Validation Accuracy: 0.7730\n","Epoch [30/50], Train Loss: 0.0183, Validation Accuracy: 0.7627\n","Epoch [31/50], Train Loss: 0.0236, Validation Accuracy: 0.7786\n","Epoch [32/50], Train Loss: 0.0026, Validation Accuracy: 0.7627\n","Epoch [33/50], Train Loss: 0.0006, Validation Accuracy: 0.7645\n","Epoch [34/50], Train Loss: 0.0002, Validation Accuracy: 0.7636\n","Epoch [35/50], Train Loss: 0.0002, Validation Accuracy: 0.7664\n","Epoch [36/50], Train Loss: 0.0001, Validation Accuracy: 0.7683\n","Early stopping triggered!\n","Using Best Model, Test Accuracy: 0.7833\n"]}],"source":["# Reset the seed to ensure fairness when comparing performances\n","set_seed()\n","\n","# Load Data\n","train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","\n","# Initialize the model\n","model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers)\n","model.to(device)\n","# Use BCELoss for binary classification\n","criterion = nn.BCELoss()\n","# Choose optimizer based on trial suggestion\n","optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","# Initialize the early stopper\n","early_stopper = EarlyStopper()\n","\n","# Lists to store loss and accuracy for visualization\n","train_losses = []\n","val_accuracies = []\n","\n","# Training Loop\n","best_val_accuracy = 0\n","best_model = None\n","for epoch in range(num_epochs):\n","    # Train on train set\n","    train_loss = train_loop(train_loader, model, criterion, optimizer)\n","    train_losses.append(train_loss)\n","\n","    # Evaluate on validation set\n","    val_accuracy = test_loop(val_loader, model)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Get the best model\n","    if val_accuracy > best_val_accuracy:\n","      best_val_accuracy = val_accuracy\n","      best_model = copy.deepcopy(model)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    #Early stopping\n","    if early_stopper.early_stop(val_accuracy):\n","       print(\"Early stopping triggered!\")\n","       break\n","\n","# Evaluate on test data\n","test_accuracy = test_loop(test_loader, best_model)\n","\n","# Print out the performance\n","print(f\"Using Best Model, Test Accuracy: {test_accuracy:.4f}\") #0.7833"]},{"cell_type":"markdown","metadata":{"id":"mIfLukDFdb_d"},"source":["#MaxMean Concatentation (Stat-Normal)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4815546,"status":"ok","timestamp":1730347867807,"user":{"displayName":"Alvin Khoo","userId":"12471070573568746260"},"user_tz":-480},"id":"n0ClE7wvdiXS","outputId":"0076879f-8b62-4781-de04-1c80f9ce76e5"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-10-31 02:50:52,004] A new study created in memory with name: no-name-c716f8cf-c290-4fde-9ca6-7b8f09ba1e4e\n","[I 2024-10-31 02:52:06,258] Trial 0 finished with value: 0.7682926829268293 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.7682926829268293.\n","[I 2024-10-31 02:53:18,559] Trial 1 finished with value: 0.7711069418386491 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7711069418386491.\n","[I 2024-10-31 02:54:38,366] Trial 2 finished with value: 0.6397748592870544 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7711069418386491.\n","[I 2024-10-31 02:55:20,606] Trial 3 finished with value: 0.7795497185741088 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7795497185741088.\n","[I 2024-10-31 02:55:44,658] Trial 4 finished with value: 0.7176360225140713 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7795497185741088.\n","[I 2024-10-31 02:56:26,497] Trial 5 finished with value: 0.5093808630393997 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.7795497185741088.\n","[I 2024-10-31 02:57:04,740] Trial 6 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 02:57:52,365] Trial 7 finished with value: 0.773921200750469 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 02:58:09,596] Trial 8 finished with value: 0.7654784240150094 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 02:59:13,550] Trial 9 finished with value: 0.5806754221388368 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 02:59:53,112] Trial 10 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 03:00:31,662] Trial 11 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 03:01:10,078] Trial 12 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 03:01:47,461] Trial 13 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 03:02:35,575] Trial 14 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 03:06:02,962] Trial 15 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 03:06:37,343] Trial 16 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 03:07:15,709] Trial 17 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 03:11:34,793] Trial 18 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 03:12:50,522] Trial 19 finished with value: 0.7926829268292683 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 03:13:25,395] Trial 20 finished with value: 0.50093808630394 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 03:14:02,426] Trial 21 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 03:14:40,072] Trial 22 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 03:15:18,099] Trial 23 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 03:15:57,382] Trial 24 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 03:16:44,107] Trial 25 finished with value: 0.49906191369606 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 03:17:22,271] Trial 26 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 03:18:00,403] Trial 27 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.7936210131332082.\n","[I 2024-10-31 03:20:33,323] Trial 28 finished with value: 0.799249530956848 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 03:23:00,523] Trial 29 finished with value: 0.7879924953095685 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 03:26:21,981] Trial 30 finished with value: 0.7917448405253283 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 03:28:55,309] Trial 31 finished with value: 0.799249530956848 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 03:31:28,314] Trial 32 finished with value: 0.799249530956848 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 03:34:00,980] Trial 33 finished with value: 0.7485928705440901 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 03:36:34,748] Trial 34 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 03:39:14,997] Trial 35 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 03:41:55,732] Trial 36 finished with value: 0.7814258911819888 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 03:44:35,966] Trial 37 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 03:47:53,063] Trial 38 finished with value: 0.5703564727954972 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 03:49:00,206] Trial 39 finished with value: 0.7898686679174484 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 03:51:41,119] Trial 40 finished with value: 0.7551594746716698 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 03:54:21,390] Trial 41 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 03:57:01,812] Trial 42 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 03:59:42,258] Trial 43 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 04:01:43,041] Trial 44 finished with value: 0.7814258911819888 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 04:04:23,476] Trial 45 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 04:05:20,078] Trial 46 finished with value: 0.5234521575984991 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 04:06:39,289] Trial 47 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 04:08:39,353] Trial 48 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 28 with value: 0.799249530956848.\n","[I 2024-10-31 04:11:07,134] Trial 49 finished with value: 0.6754221388367729 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 28 with value: 0.799249530956848.\n"]}],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Reset the seed for fairness\n","    torch.manual_seed(42)\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers,aggregation='max_mean_pool',freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","# Start the Optuna study\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)\n","#[I 2024-10-31 03:20:33,323] Trial 28 finished with value: 0.799249530956848 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 28 with value: 0.799249530956848."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ueSFpaykd0ak"},"outputs":[],"source":["#[I 2024-10-31 03:20:33,323] Trial 28 finished with value: 0.799249530956848 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 28 with value: 0.799249530956848.\n","# The Best Hyperparameters\n","hidden_size = 256  # Hidden size choices\n","num_layers = 2 # Number of RNN layers choices\n","learning_rate = 0.001  # Learning rate choices\n","batch_size = 32  # Batch size choices\n","num_epochs = 40  # num epoch choices\n","optimizer_name = 'RMSprop'  # Optimizer choices"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":126395,"status":"ok","timestamp":1730604384353,"user":{"displayName":"ALEX KHOO SHIEN HOW","userId":"09575417203520826139"},"user_tz":-480},"id":"w5BUlsUcduR8","outputId":"03a51484-3823-4e1f-c5a7-584f200ae975"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/40], Train Loss: 0.5278, Validation Accuracy: 0.7871\n","Epoch [2/40], Train Loss: 0.4313, Validation Accuracy: 0.7852\n","Epoch [3/40], Train Loss: 0.3939, Validation Accuracy: 0.7936\n","Epoch [4/40], Train Loss: 0.3435, Validation Accuracy: 0.7917\n","Epoch [5/40], Train Loss: 0.2899, Validation Accuracy: 0.7767\n","Epoch [6/40], Train Loss: 0.2370, Validation Accuracy: 0.8077\n","Epoch [7/40], Train Loss: 0.1850, Validation Accuracy: 0.7824\n","Epoch [8/40], Train Loss: 0.1393, Validation Accuracy: 0.7992\n","Epoch [9/40], Train Loss: 0.0979, Validation Accuracy: 0.7692\n","Epoch [10/40], Train Loss: 0.0713, Validation Accuracy: 0.7899\n","Epoch [11/40], Train Loss: 0.0563, Validation Accuracy: 0.7852\n","Epoch [12/40], Train Loss: 0.0459, Validation Accuracy: 0.7871\n","Epoch [13/40], Train Loss: 0.0459, Validation Accuracy: 0.7767\n","Epoch [14/40], Train Loss: 0.0303, Validation Accuracy: 0.7833\n","Epoch [15/40], Train Loss: 0.0284, Validation Accuracy: 0.7786\n","Epoch [16/40], Train Loss: 0.0307, Validation Accuracy: 0.7861\n","Epoch [17/40], Train Loss: 0.0248, Validation Accuracy: 0.7749\n","Epoch [18/40], Train Loss: 0.0276, Validation Accuracy: 0.7889\n","Epoch [19/40], Train Loss: 0.0222, Validation Accuracy: 0.7861\n","Epoch [20/40], Train Loss: 0.0216, Validation Accuracy: 0.7767\n","Epoch [21/40], Train Loss: 0.0202, Validation Accuracy: 0.7833\n","Epoch [22/40], Train Loss: 0.0167, Validation Accuracy: 0.7814\n","Epoch [23/40], Train Loss: 0.0157, Validation Accuracy: 0.7711\n","Epoch [24/40], Train Loss: 0.0164, Validation Accuracy: 0.7936\n","Epoch [25/40], Train Loss: 0.0124, Validation Accuracy: 0.7777\n","Epoch [26/40], Train Loss: 0.0175, Validation Accuracy: 0.7889\n","Epoch [27/40], Train Loss: 0.0102, Validation Accuracy: 0.7636\n","Epoch [28/40], Train Loss: 0.0162, Validation Accuracy: 0.7899\n","Epoch [29/40], Train Loss: 0.0128, Validation Accuracy: 0.7824\n","Epoch [30/40], Train Loss: 0.0268, Validation Accuracy: 0.7739\n","Epoch [31/40], Train Loss: 0.0076, Validation Accuracy: 0.7814\n","Epoch [32/40], Train Loss: 0.0131, Validation Accuracy: 0.7824\n","Epoch [33/40], Train Loss: 0.0020, Validation Accuracy: 0.7908\n","Epoch [34/40], Train Loss: 0.0164, Validation Accuracy: 0.7824\n","Epoch [35/40], Train Loss: 0.0153, Validation Accuracy: 0.7889\n","Epoch [36/40], Train Loss: 0.0104, Validation Accuracy: 0.7749\n","Early stopping triggered!\n","Using Best Model, Test Accuracy: 0.7852\n"]}],"source":["# Reset the seed to ensure fairness when comparing performances\n","set_seed()\n","\n","# Load Data\n","train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","\n","# Initialize the model\n","model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation='max_mean_pool')\n","model.to(device)\n","# Use BCELoss for binary classification\n","criterion = nn.BCELoss()\n","# Choose optimizer based on trial suggestion\n","optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","# Initialize the early stopper\n","early_stopper = EarlyStopper()\n","\n","# Lists to store loss and accuracy for visualization\n","train_losses = []\n","val_accuracies = []\n","\n","# Training Loop\n","best_val_accuracy = 0\n","best_model = None\n","for epoch in range(num_epochs):\n","    # Train on train set\n","    train_loss = train_loop(train_loader, model, criterion, optimizer)\n","    train_losses.append(train_loss)\n","\n","    # Evaluate on validation set\n","    val_accuracy = test_loop(val_loader, model)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Get the best model\n","    if val_accuracy > best_val_accuracy:\n","      best_val_accuracy = val_accuracy\n","      best_model = copy.deepcopy(model)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    if early_stopper.early_stop(val_accuracy):\n","       print(\"Early stopping triggered!\")\n","       break\n","\n","# Evaluate on test data\n","test_accuracy = test_loop(test_loader, best_model)\n","\n","# Print out the performance\n","print(f\"Using Best Model, Test Accuracy: {test_accuracy:.4f}\") #0.7852"]},{"cell_type":"markdown","metadata":{"id":"Y-BuFNGKl6yJ"},"source":["#MHSA (Max Pooling)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8J0F8_ZH9Ts"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class SimpleBiLSTM(nn.Module):\n","    def __init__(self, embedding_matrix, hidden_size, num_layers, num_attention_heads=8, aggregation='last_hidden_state', freeze_embedding=True):\n","        super(SimpleBiLSTM, self).__init__()\n","        embedding_dim = embedding_matrix.shape[1]\n","\n","        self.aggregation = aggregation\n","        self.hidden_size = hidden_size  # Store hidden size for reference\n","\n","        # Ensure num_attention_heads is compatible with embedding_dim\n","        if embedding_dim % num_attention_heads != 0:\n","            num_attention_heads = max(1, embedding_dim // 64)  # Adjust to nearest factor if needed\n","\n","        # Embedding layer using a pretrained embedding matrix\n","        self.embedding = nn.Embedding.from_pretrained(\n","            torch.tensor(embedding_matrix, dtype=torch.float32),\n","            padding_idx=0,  # Using 0 for padding index\n","            freeze=freeze_embedding,  # Freeze embeddings\n","        )\n","\n","        # Multi-head self-attention layer\n","        self.mhsa = nn.MultiheadAttention(embedding_dim, num_attention_heads, batch_first=True)\n","\n","        # BiLSTM layer with bidirectional=True\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","        self.dropout = nn.Dropout(0.3)\n","\n","        # Output layer dimensions adjusted for bidirectional LSTM (2 * hidden_size)\n","        self.fc = nn.Linear(300, 1)  # Adjusted for max pooling\n","\n","    def forward(self, x):\n","        # Pass through the embedding layer\n","        embedded = self.embedding(x)\n","\n","        # Create padding mask for the attention layer\n","        device = embedded.device\n","        padding_mask = (x == self.embedding.padding_idx)\n","\n","        # Apply multi-head self-attention\n","        attention_out, _ = self.mhsa(embedded, embedded, embedded, key_padding_mask=padding_mask)\n","\n","        # Forward pass through BiLSTM using attention output as input\n","        h0 = torch.zeros(2 * self.lstm.num_layers, embedded.size(0), self.hidden_size).to(embedded.device)\n","        c0 = torch.zeros(2 * self.lstm.num_layers, embedded.size(0), self.hidden_size).to(embedded.device)\n","        lstm_out, (hn, cn) = self.lstm(attention_out, (h0, c0))\n","\n","        # Aggregation method\n","        if self.aggregation == 'last_hidden_state':\n","            # Concatenate the last hidden states from both directions\n","            out = torch.cat((hn[-2], hn[-1]), dim=1)  # -2 and -1 refer to the last forward and backward states\n","        elif self.aggregation == 'max_pool':\n","            # Apply max pooling on LSTM output, ignoring padding\n","            mask = (x != self.embedding.padding_idx).unsqueeze(-1)\n","            lstm_out_masked = lstm_out.masked_fill(~mask, -float('inf'))\n","            out, _ = torch.max(lstm_out_masked, dim=1)  # Max pooling over sequence length\n","        elif self.aggregation == 'mhsa_max_pool':\n","            # Apply max pooling on the output of MHSA layer\n","            mask = (x != self.embedding.padding_idx).unsqueeze(-1)\n","            attention_out_masked = attention_out.masked_fill(~mask, -float('inf'))\n","            out, _ = torch.max(attention_out_masked, dim=1)  # Max pooling over sequence length\n","\n","        out = self.dropout(out)  # Apply dropout\n","        out = self.fc(out)  # Final fully connected layer\n","\n","        # Sigmoid activation for binary classification\n","        out = torch.sigmoid(out)\n","\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":620292,"status":"ok","timestamp":1730608762512,"user":{"displayName":"ALEX KHOO SHIEN HOW","userId":"09575417203520826139"},"user_tz":-480},"id":"qR7kxnpqIDKU","outputId":"f86314ce-8dfa-4e12-db41-c3a08b533682"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-11-03 03:45:45,188] A new study created in memory with name: no-name-f2f32ef6-7497-4856-9e29-29a2625987dc\n","[I 2024-11-03 03:46:41,216] Trial 0 finished with value: 0.5037523452157598 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.5037523452157598.\n","[I 2024-11-03 03:47:43,487] Trial 1 finished with value: 0.774859287054409 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.774859287054409.\n","[I 2024-11-03 03:48:49,140] Trial 2 finished with value: 0.7795497185741088 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.7795497185741088.\n","[I 2024-11-03 03:49:56,865] Trial 3 finished with value: 0.5112570356472795 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.7795497185741088.\n","[I 2024-11-03 03:50:38,192] Trial 4 finished with value: 0.5121951219512195 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.7795497185741088.\n","[I 2024-11-03 03:51:33,948] Trial 5 finished with value: 0.7851782363977486 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 5 with value: 0.7851782363977486.\n","[I 2024-11-03 03:52:32,489] Trial 6 finished with value: 0.776735459662289 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 5 with value: 0.7851782363977486.\n","[I 2024-11-03 03:53:58,281] Trial 7 finished with value: 0.50187617260788 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 5 with value: 0.7851782363977486.\n","[I 2024-11-03 03:54:53,962] Trial 8 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 03:56:25,687] Trial 9 finished with value: 0.5 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 03:57:21,289] Trial 10 finished with value: 0.5065666041275797 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 03:58:10,270] Trial 11 finished with value: 0.6941838649155723 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 03:59:25,611] Trial 12 finished with value: 0.7814258911819888 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:00:04,969] Trial 13 finished with value: 0.7842401500938087 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:01:20,143] Trial 14 finished with value: 0.774859287054409 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:02:20,232] Trial 15 finished with value: 0.702626641651032 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:03:09,114] Trial 16 finished with value: 0.5919324577861164 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:04:09,556] Trial 17 finished with value: 0.7814258911819888 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:04:56,500] Trial 18 finished with value: 0.7804878048780488 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:05:38,825] Trial 19 finished with value: 0.49906191369606 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:06:54,291] Trial 20 finished with value: 0.7814258911819888 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:07:33,324] Trial 21 finished with value: 0.7842401500938087 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:08:12,660] Trial 22 finished with value: 0.7842401500938087 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:08:51,983] Trial 23 finished with value: 0.7842401500938087 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:09:31,144] Trial 24 finished with value: 0.7842401500938087 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:10:04,168] Trial 25 finished with value: 0.7851782363977486 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:10:37,330] Trial 26 finished with value: 0.7851782363977486 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:11:10,525] Trial 27 finished with value: 0.7851782363977486 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:12:19,935] Trial 28 finished with value: 0.524390243902439 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:13:24,884] Trial 29 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:14:29,761] Trial 30 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:15:34,751] Trial 31 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:16:39,799] Trial 32 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:17:44,630] Trial 33 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:18:49,624] Trial 34 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:19:54,537] Trial 35 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:20:59,349] Trial 36 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:22:04,343] Trial 37 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:23:09,324] Trial 38 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 8 with value: 0.7861163227016885.\n","[I 2024-11-03 04:24:41,283] Trial 39 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 39 with value: 0.7870544090056285.\n","[I 2024-11-03 04:25:36,470] Trial 40 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 39 with value: 0.7870544090056285.\n","[I 2024-11-03 04:27:08,651] Trial 41 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 39 with value: 0.7870544090056285.\n","[I 2024-11-03 04:28:40,630] Trial 42 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 39 with value: 0.7870544090056285.\n","[I 2024-11-03 04:30:12,149] Trial 43 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 39 with value: 0.7870544090056285.\n","[I 2024-11-03 04:31:43,881] Trial 44 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 39 with value: 0.7870544090056285.\n","[I 2024-11-03 04:33:15,498] Trial 45 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 39 with value: 0.7870544090056285.\n","[I 2024-11-03 04:34:47,067] Trial 46 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 39 with value: 0.7870544090056285.\n","[I 2024-11-03 04:36:18,776] Trial 47 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 39 with value: 0.7870544090056285.\n","[I 2024-11-03 04:37:50,367] Trial 48 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 39 with value: 0.7870544090056285.\n","[I 2024-11-03 04:39:21,813] Trial 49 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 39 with value: 0.7870544090056285.\n"]}],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Reset the seed for fairness\n","    torch.manual_seed(42)\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers,aggregation='mhsa_max_pool',freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","# Start the Optuna study\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nGyMQbL3k6TF"},"outputs":[],"source":["#[I 2024-11-03 04:24:41,283] Trial 39 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 39 with value: 0.7870544090056285.\n","\n","# The Best Hyperparameters\n","hidden_size = 128  # Hidden size choices\n","num_layers = 2 # Number of RNN layers choices\n","learning_rate = 1e-05  # Learning rate choices\n","batch_size = 32  # Batch size choices\n","num_epochs = 50  # num epoch choices\n","optimizer_name = 'Adam'  # Optimizer choices"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":76872,"status":"ok","timestamp":1730613711597,"user":{"displayName":"Alex Khoo Shien How","userId":"06086813408012082662"},"user_tz":-480},"id":"TBfADTx9lA6S","outputId":"1406b8e6-6cc3-4534-a03e-35b0312f5bf1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/50], Train Loss: 0.6913, Validation Accuracy: 0.6510\n","Epoch [2/50], Train Loss: 0.6855, Validation Accuracy: 0.6792\n","Epoch [3/50], Train Loss: 0.6771, Validation Accuracy: 0.6848\n","Epoch [4/50], Train Loss: 0.6638, Validation Accuracy: 0.6942\n","Epoch [5/50], Train Loss: 0.6443, Validation Accuracy: 0.7045\n","Epoch [6/50], Train Loss: 0.6195, Validation Accuracy: 0.7167\n","Epoch [7/50], Train Loss: 0.5907, Validation Accuracy: 0.7298\n","Epoch [8/50], Train Loss: 0.5616, Validation Accuracy: 0.7336\n","Epoch [9/50], Train Loss: 0.5345, Validation Accuracy: 0.7383\n","Epoch [10/50], Train Loss: 0.5138, Validation Accuracy: 0.7392\n","Epoch [11/50], Train Loss: 0.4985, Validation Accuracy: 0.7542\n","Epoch [12/50], Train Loss: 0.4897, Validation Accuracy: 0.7552\n","Epoch [13/50], Train Loss: 0.4837, Validation Accuracy: 0.7589\n","Epoch [14/50], Train Loss: 0.4797, Validation Accuracy: 0.7636\n","Epoch [15/50], Train Loss: 0.4759, Validation Accuracy: 0.7664\n","Epoch [16/50], Train Loss: 0.4740, Validation Accuracy: 0.7683\n","Epoch [17/50], Train Loss: 0.4718, Validation Accuracy: 0.7702\n","Epoch [18/50], Train Loss: 0.4700, Validation Accuracy: 0.7683\n","Epoch [19/50], Train Loss: 0.4676, Validation Accuracy: 0.7683\n","Epoch [20/50], Train Loss: 0.4664, Validation Accuracy: 0.7683\n","Epoch [21/50], Train Loss: 0.4655, Validation Accuracy: 0.7711\n","Epoch [22/50], Train Loss: 0.4638, Validation Accuracy: 0.7692\n","Epoch [23/50], Train Loss: 0.4628, Validation Accuracy: 0.7702\n","Epoch [24/50], Train Loss: 0.4611, Validation Accuracy: 0.7683\n","Epoch [25/50], Train Loss: 0.4602, Validation Accuracy: 0.7683\n","Epoch [26/50], Train Loss: 0.4592, Validation Accuracy: 0.7655\n","Epoch [27/50], Train Loss: 0.4582, Validation Accuracy: 0.7655\n","Epoch [28/50], Train Loss: 0.4565, Validation Accuracy: 0.7683\n","Epoch [29/50], Train Loss: 0.4557, Validation Accuracy: 0.7664\n","Epoch [30/50], Train Loss: 0.4547, Validation Accuracy: 0.7664\n","Epoch [31/50], Train Loss: 0.4534, Validation Accuracy: 0.7645\n","Epoch [32/50], Train Loss: 0.4524, Validation Accuracy: 0.7655\n","Epoch [33/50], Train Loss: 0.4518, Validation Accuracy: 0.7664\n","Epoch [34/50], Train Loss: 0.4508, Validation Accuracy: 0.7664\n","Epoch [35/50], Train Loss: 0.4501, Validation Accuracy: 0.7683\n","Epoch [36/50], Train Loss: 0.4491, Validation Accuracy: 0.7655\n","Epoch [37/50], Train Loss: 0.4482, Validation Accuracy: 0.7664\n","Epoch [38/50], Train Loss: 0.4478, Validation Accuracy: 0.7636\n","Epoch [39/50], Train Loss: 0.4467, Validation Accuracy: 0.7636\n","Epoch [40/50], Train Loss: 0.4462, Validation Accuracy: 0.7636\n","Epoch [41/50], Train Loss: 0.4454, Validation Accuracy: 0.7608\n","Epoch [42/50], Train Loss: 0.4447, Validation Accuracy: 0.7692\n","Epoch [43/50], Train Loss: 0.4439, Validation Accuracy: 0.7711\n","Epoch [44/50], Train Loss: 0.4433, Validation Accuracy: 0.7636\n","Epoch [45/50], Train Loss: 0.4421, Validation Accuracy: 0.7683\n","Epoch [46/50], Train Loss: 0.4418, Validation Accuracy: 0.7683\n","Epoch [47/50], Train Loss: 0.4412, Validation Accuracy: 0.7702\n","Epoch [48/50], Train Loss: 0.4404, Validation Accuracy: 0.7683\n","Epoch [49/50], Train Loss: 0.4399, Validation Accuracy: 0.7683\n","Epoch [50/50], Train Loss: 0.4397, Validation Accuracy: 0.7674\n","Using Best Model, Test Accuracy: 0.7608\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1123: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1410.)\n","  result = _VF.lstm(\n"]}],"source":["# Reset the seed to ensure fairness when comparing performances\n","set_seed()\n","\n","# Load Data\n","train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","\n","# Initialize the model\n","model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation='mhsa_max_pool')\n","model.to(device)\n","# Use BCELoss for binary classification\n","criterion = nn.BCELoss()\n","# Choose optimizer based on trial suggestion\n","optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","# Initialize the early stopper\n","early_stopper = EarlyStopper()\n","\n","# Lists to store loss and accuracy for visualization\n","train_losses = []\n","val_accuracies = []\n","\n","# Training Loop\n","best_val_accuracy = 0\n","best_model = None\n","for epoch in range(num_epochs):\n","    # Train on train set\n","    train_loss = train_loop(train_loader, model, criterion, optimizer)\n","    train_losses.append(train_loss)\n","\n","    # Evaluate on validation set\n","    val_accuracy = test_loop(val_loader, model)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Get the best model\n","    if val_accuracy > best_val_accuracy:\n","      best_val_accuracy = val_accuracy\n","      best_model = copy.deepcopy(model)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    if early_stopper.early_stop(val_accuracy):\n","       print(\"Early stopping triggered!\")\n","       break\n","\n","# Evaluate on test data\n","test_accuracy = test_loop(test_loader, best_model)\n","\n","# Print out the performance\n","print(f\"Using Best Model, Test Accuracy: {test_accuracy:.4f}\") #0.7833"]},{"cell_type":"markdown","metadata":{"id":"k4nApRER8lLe"},"source":["# **BiLSTM with Residual Connections and BatchNorm**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CQvDxlC18ftg"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class SimpleBiLSTM(nn.Module):\n","    def __init__(self, embedding_matrix, hidden_size, num_layers, aggregation='last_hidden_state', freeze_embedding=True):\n","        super(SimpleBiLSTM, self).__init__()\n","        embedding_dim = embedding_matrix.shape[1]\n","\n","        self.aggregation = aggregation\n","        self.hidden_size = hidden_size  # Store hidden size for reference\n","\n","        # Embedding layer using a pretrained embedding matrix\n","        self.embedding = nn.Embedding.from_pretrained(\n","            torch.tensor(embedding_matrix, dtype=torch.float32),\n","            padding_idx=0,  # Using 0 for padding index\n","            freeze=freeze_embedding,  # Freeze embeddings\n","        )\n","\n","        self.residual_projection = nn.Linear(embedding_dim, hidden_size * 2)\n","\n","        # BiLSTM layer with bidirectional=True\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","\n","        # Batch normalization layers\n","        self.batch_norm_lstm = nn.BatchNorm1d(hidden_size * 2)  # For BiLSTM output (bidirectional, hence 2x hidden_size)\n","        self.batch_norm_fc = nn.BatchNorm1d(hidden_size * 4)  # For concatenated max and mean pooling\n","\n","        self.dropout = nn.Dropout(0.1)\n","\n","        # Output layer dimensions adjusted for bidirectional LSTM (2 * hidden_size)\n","        self.fc = nn.Linear(4 * hidden_size, 1)  # Adjusted output size for concatenated max and mean pooling\n","\n","    def forward(self, x):\n","        # Pass through the embedding layer\n","        embedded = self.embedding(x)\n","\n","        # Initialize hidden and cell states for both directions (2*num_layers for bidirectional)\n","        h0 = torch.zeros(2 * self.lstm.num_layers, embedded.size(0), self.hidden_size).to(embedded.device)\n","        c0 = torch.zeros(2 * self.lstm.num_layers, embedded.size(0), self.hidden_size).to(embedded.device)\n","\n","        # Forward pass through BiLSTM\n","        out, (hn, cn) = self.lstm(embedded, (h0, c0))\n","\n","        # Apply residual connection\n","        residual = self.residual_projection(embedded)  # Project embeddings to match 'out' dimensions\n","        out = out + residual  # Add residual connection\n","\n","        # Apply batch normalization on the BiLSTM output\n","        out = self.batch_norm_lstm(out.transpose(1, 2)).transpose(1, 2)  # Transpose for batch norm\n","\n","        # Aggregation method\n","        if self.aggregation == 'last_hidden_state':\n","            # Concatenate the last hidden states from both directions\n","            out = torch.cat((hn[-2], hn[-1]), dim=1)  # -2 and -1 refer to the last forward and backward states\n","        elif self.aggregation == 'max_mean_pool':\n","            # Mask out padding tokens for pooling\n","            mask = (x != self.embedding.padding_idx).unsqueeze(-1)\n","\n","            # Max pooling\n","            max_pooled_out = out.masked_fill(~mask, -100000)  # Mask padding tokens with a large negative value\n","            max_pooled_out, _ = torch.max(max_pooled_out, dim=1)\n","\n","            # Mean pooling\n","            mean_pooled_out = out * mask\n","            summed = mean_pooled_out.sum(dim=1)\n","            valid_counts = mask.sum(dim=1).clamp(min=1)\n","            mean_pooled_out = summed / valid_counts\n","\n","            # Concatenate max and mean pooling outputs\n","            out = torch.cat((max_pooled_out, mean_pooled_out), dim=1)\n","\n","        # Apply batch normalization before the fully connected layer\n","        out = self.batch_norm_fc(out)\n","\n","        # Dropout and final fully connected layer\n","        out = self.dropout(out)\n","        out = self.fc(out)  # Final fully connected layer\n","\n","        # Sigmoid activation for binary classification\n","        out = torch.sigmoid(out)\n","\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2938075,"status":"ok","timestamp":1730479443533,"user":{"displayName":"alvin khoo","userId":"02395113757594492728"},"user_tz":-480},"id":"4J8bmtlEaD8x","outputId":"c4178b14-7e0e-4e07-ab6d-fa32c9acfa64"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-11-01 15:55:05,327] A new study created in memory with name: no-name-57be0778-482e-43c6-a7a8-da529862abe5\n","[I 2024-11-01 15:55:58,238] Trial 0 finished with value: 0.7570356472795498 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.7570356472795498.\n","[I 2024-11-01 15:57:16,516] Trial 1 finished with value: 0.7917448405253283 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7917448405253283.\n","[I 2024-11-01 16:00:40,468] Trial 2 finished with value: 0.7495309568480301 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7917448405253283.\n","[I 2024-11-01 16:02:20,922] Trial 3 finished with value: 0.7467166979362101 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7917448405253283.\n","[I 2024-11-01 16:05:01,414] Trial 4 finished with value: 0.774859287054409 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7917448405253283.\n","[I 2024-11-01 16:05:42,341] Trial 5 finished with value: 0.7776735459662288 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7917448405253283.\n","[I 2024-11-01 16:07:07,100] Trial 6 finished with value: 0.7654784240150094 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7917448405253283.\n","[I 2024-11-01 16:09:05,514] Trial 7 finished with value: 0.7842401500938087 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7917448405253283.\n","[I 2024-11-01 16:11:54,634] Trial 8 finished with value: 0.7711069418386491 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7917448405253283.\n","[I 2024-11-01 16:13:22,077] Trial 9 finished with value: 0.773921200750469 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7917448405253283.\n","[I 2024-11-01 16:14:09,109] Trial 10 finished with value: 0.7917448405253283 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7917448405253283.\n","[I 2024-11-01 16:14:55,368] Trial 11 finished with value: 0.7917448405253283 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7917448405253283.\n","[I 2024-11-01 16:15:41,932] Trial 12 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7917448405253283.\n","[I 2024-11-01 16:16:28,149] Trial 13 finished with value: 0.7917448405253283 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7917448405253283.\n","[I 2024-11-01 16:16:56,025] Trial 14 finished with value: 0.7898686679174484 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7917448405253283.\n","[I 2024-11-01 16:18:00,508] Trial 15 finished with value: 0.7889305816135085 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7917448405253283.\n","[I 2024-11-01 16:18:46,565] Trial 16 finished with value: 0.7926829268292683 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 16 with value: 0.7926829268292683.\n","[I 2024-11-01 16:20:04,505] Trial 17 finished with value: 0.7926829268292683 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 16 with value: 0.7926829268292683.\n","[I 2024-11-01 16:20:37,104] Trial 18 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:21:06,976] Trial 19 finished with value: 0.7795497185741088 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:21:39,735] Trial 20 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:22:12,398] Trial 21 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:22:45,219] Trial 22 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:23:17,874] Trial 23 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:23:50,500] Trial 24 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:24:23,395] Trial 25 finished with value: 0.7908067542213884 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:24:56,164] Trial 26 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:25:39,889] Trial 27 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:26:05,472] Trial 28 finished with value: 0.7373358348968105 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:28:54,463] Trial 29 finished with value: 0.7804878048780488 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:29:24,434] Trial 30 finished with value: 0.7795497185741088 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:29:57,052] Trial 31 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:30:29,793] Trial 32 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:31:02,523] Trial 33 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:31:35,236] Trial 34 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:32:03,593] Trial 35 finished with value: 0.7617260787992496 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:33:09,110] Trial 36 finished with value: 0.7673545966228893 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:34:08,973] Trial 37 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:35:43,051] Trial 38 finished with value: 0.5 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:36:24,095] Trial 39 finished with value: 0.7879924953095685 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:37:25,862] Trial 40 finished with value: 0.7842401500938087 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:37:58,508] Trial 41 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:38:31,401] Trial 42 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:39:04,024] Trial 43 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:39:35,985] Trial 44 finished with value: 0.5 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:41:25,600] Trial 45 finished with value: 0.7786116322701688 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:41:52,648] Trial 46 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:42:25,590] Trial 47 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:43:25,381] Trial 48 finished with value: 0.7504690431519699 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n","[I 2024-11-01 16:44:03,076] Trial 49 finished with value: 0.773921200750469 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482.\n"]}],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Reset the seed for fairness\n","    torch.manual_seed(42)\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers,aggregation='max_mean_pool',freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1rVhCTA8rZuv"},"outputs":[],"source":["# The Best Hyperparameters\n","hidden_size = 128  # Hidden size choices\n","num_layers = 1 # Number of RNN layers choices\n","learning_rate = 0.0001  # Learning rate choices\n","batch_size = 64  # Batch size choices\n","num_epochs = 30  # num epoch choices\n","optimizer_name = 'RMSprop'  # Optimizer choices\n","\n","\n","#[I 2024-11-01 16:20:37,104] Trial 18 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 18 with value: 0.7945590994371482."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26439,"status":"ok","timestamp":1730615145805,"user":{"displayName":"alvin khoo","userId":"02395113757594492728"},"user_tz":-480},"id":"DIXZhGZPrqQn","outputId":"663ae40b-124f-4848-ed4c-4c6d185eb7e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/30], Train Loss: 0.5124, Validation Accuracy: 0.7636\n","Epoch [2/30], Train Loss: 0.4421, Validation Accuracy: 0.7589\n","Epoch [3/30], Train Loss: 0.4178, Validation Accuracy: 0.7730\n","Epoch [4/30], Train Loss: 0.3931, Validation Accuracy: 0.7730\n","Epoch [5/30], Train Loss: 0.3761, Validation Accuracy: 0.7795\n","Epoch [6/30], Train Loss: 0.3519, Validation Accuracy: 0.7786\n","Epoch [7/30], Train Loss: 0.3333, Validation Accuracy: 0.7777\n","Epoch [8/30], Train Loss: 0.3108, Validation Accuracy: 0.7852\n","Epoch [9/30], Train Loss: 0.2902, Validation Accuracy: 0.7842\n","Epoch [10/30], Train Loss: 0.2666, Validation Accuracy: 0.7871\n","Epoch [11/30], Train Loss: 0.2453, Validation Accuracy: 0.7917\n","Epoch [12/30], Train Loss: 0.2220, Validation Accuracy: 0.7917\n","Epoch [13/30], Train Loss: 0.2090, Validation Accuracy: 0.7795\n","Epoch [14/30], Train Loss: 0.1826, Validation Accuracy: 0.7946\n","Epoch [15/30], Train Loss: 0.1694, Validation Accuracy: 0.7871\n","Epoch [16/30], Train Loss: 0.1492, Validation Accuracy: 0.7842\n","Epoch [17/30], Train Loss: 0.1321, Validation Accuracy: 0.7749\n","Epoch [18/30], Train Loss: 0.1187, Validation Accuracy: 0.7964\n","Epoch [19/30], Train Loss: 0.1065, Validation Accuracy: 0.7927\n","Epoch [20/30], Train Loss: 0.0934, Validation Accuracy: 0.8002\n","Epoch [21/30], Train Loss: 0.0818, Validation Accuracy: 0.7946\n","Epoch [22/30], Train Loss: 0.0694, Validation Accuracy: 0.7992\n","Epoch [23/30], Train Loss: 0.0638, Validation Accuracy: 0.7899\n","Epoch [24/30], Train Loss: 0.0577, Validation Accuracy: 0.7805\n","Epoch [25/30], Train Loss: 0.0510, Validation Accuracy: 0.7880\n","Epoch [26/30], Train Loss: 0.0426, Validation Accuracy: 0.7795\n","Epoch [27/30], Train Loss: 0.0390, Validation Accuracy: 0.7899\n","Epoch [28/30], Train Loss: 0.0332, Validation Accuracy: 0.7795\n","Epoch [29/30], Train Loss: 0.0271, Validation Accuracy: 0.7824\n","Epoch [30/30], Train Loss: 0.0280, Validation Accuracy: 0.7627\n","Using Best Model, Test Accuracy: 0.7983\n"]}],"source":["# Reset the seed to ensure fairness when comparing performances\n","set_seed()\n","\n","# Load Data\n","train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","\n","# Initialize the model\n","model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation='max_mean_pool')\n","model.to(device)\n","# Use BCELoss for binary classification\n","criterion = nn.BCELoss()\n","# Choose optimizer based on trial suggestion\n","optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","# Initialize the early stopper\n","early_stopper = EarlyStopper()\n","\n","# Lists to store loss and accuracy for visualization\n","train_losses = []\n","val_accuracies = []\n","\n","# Training Loop\n","best_val_accuracy = 0\n","best_model = None\n","for epoch in range(num_epochs):\n","    # Train on train set\n","    train_loss = train_loop(train_loader, model, criterion, optimizer)\n","    train_losses.append(train_loss)\n","\n","    # Evaluate on validation set\n","    val_accuracy = test_loop(val_loader, model)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Get the best model\n","    if val_accuracy > best_val_accuracy:\n","      best_val_accuracy = val_accuracy\n","      best_model = copy.deepcopy(model)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    if early_stopper.early_stop(val_accuracy):\n","       print(\"Early stopping triggered!\")\n","       break\n","\n","# Evaluate on test data\n","test_accuracy = test_loop(test_loader, best_model)\n","\n","# Print out the performance\n","print(f\"Using Best Model, Test Accuracy: {test_accuracy:.4f}\") #0.8049"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":465},"executionInfo":{"elapsed":1184,"status":"ok","timestamp":1730481256004,"user":{"displayName":"alvin khoo","userId":"02395113757594492728"},"user_tz":-480},"id":"FG0NyN1HsGyd","outputId":"649e7727-2688-43bb-d5ee-c10a83626f2b"},"outputs":[{"data":{"image/png":"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\n","text/plain":["<Figure size 1200x500 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_performance(train_losses, val_accuracies, len(train_losses))"]},{"cell_type":"markdown","source":["# References"],"metadata":{"id":"dMJHOaVxPnGe"}},{"cell_type":"markdown","metadata":{"id":"m-GjCdEFg6Cb"},"source":["# **Handling OOV**"]},{"cell_type":"markdown","metadata":{"id":"JizMIPPhiaju"},"source":["## **Load FastText Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p8RAPJpEiaIm"},"outputs":[],"source":["# Load the pre-trained Word2Vec model\n","if 'google.colab' in sys.modules:\n","    print(\"Running on Google Colab's hosted runtime\")\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    fasttext_model = load_facebook_model(PATH_TO_FASTTEXT_MODEL_DRIVE)\n","else:\n","    print(\"Running on a local runtime\")\n","    fasttext_model = load_facebook_model(PATH_TO_FASTTEXT_MODEL_LOCAL)\n","\n","# Get the dimension of the embeddings\n","fasttext_vector_dim = fasttext_model.vector_size\n","print(f\"FastText Dimension: {fasttext_vector_dim}\")"]},{"cell_type":"markdown","metadata":{"id":"t7SB0sh1igTS"},"source":["## **Preprocessing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6Z85NT3g9uM"},"outputs":[],"source":["X_train_OOV = preprocessed_train_dataset\n","Y_train_OOV = [entry['label'] for entry in train_dataset]\n","\n","#X_val_OOV = drop_non_train_vocab(preprocessed_validation_dataset, lemmatized_vocabulary)\n","X_val_OOV = preprocessed_validation_dataset\n","Y_val_OOV = [entry['label'] for entry in validation_dataset]\n","\n","#X_test_OOV = drop_non_train_vocab(preprocessed_test_dataset, lemmatized_vocabulary)\n","X_test_OOV = preprocessed_test_dataset\n","Y_test_OOV = [entry['label'] for entry in test_dataset]\n","\n","def create_fasttext_embedding_matrix(fasttext_model, word2index, embedding_dim=300):\n","    vocab_size = len(word2index)  # Number of words in the training vocabulary\n","\n","    # Initialize the embedding matrix with zeros (or any other value)\n","    embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n","\n","    for word, idx in word2index.items():\n","        embedding_matrix[idx] = fasttext_model.wv[word]\n","\n","    return embedding_matrix\n","\n","fasttext_embedding_dim = fasttext_model.vector_size  # Get the dimension of the word vectors in Word2Vec\n","fasttext_embedding_matrix = create_fasttext_embedding_matrix(fasttext_model, word2index, embedding_dim)\n","print(f\"fasttext_embedding_matrix shape: {fasttext_embedding_matrix.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"Z001Q1W4kUvF"},"source":["## **Seq2Seq + Padding**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xq1_yvQGkX3Z"},"outputs":[],"source":["max_length_with_oov = get_max_sentence_length(X_train_OOV, X_val_OOV, X_test_OOV)\n","print(f\"Maximum sentence length of three datasets: {max_length_with_oov}\")\n","\n","X_train_OOV_sequence = convert_to_sequence_data(X_train_OOV, word2index, max_length_with_oov)\n","X_val_OOV_sequence = convert_to_sequence_data(X_val_OOV , word2index, max_length_with_oov)\n","X_test_OOV_sequence = convert_to_sequence_data(X_test_OOV, word2index, max_length_with_oov)\n","\n","print(f\"Shape of X_train_OOV_sequence: {X_train_OOV_sequence.shape}\")\n","print(f\"Shape of X_val_OOV_sequence: {X_val_OOV_sequence.shape}\")\n","print(f\"Shape of X_test_OOV_sequence: {X_test_OOV_sequence.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"LZ7WK4vZZLMv"},"source":["#**BiLSTM - Word2Vec (MinPool - Unfreeze Embedding) - Normal**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tg2SQDtMXgkx"},"outputs":[],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Reset the seed for fairness\n","    torch.manual_seed(42)\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [10, 20, 30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation = \"mean_pool\",freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","# Start the Optuna study\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)\n","\n","#[I 2024-10-26 05:27:51,546] Trial 27 finished with value: 0.7720450281425891 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 27 with value: 0.7720450281425891."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hR8YFHrQZtiT"},"outputs":[],"source":["# The Best Hyperparameters\n","hidden_size = 128  # Hidden size choices\n","num_layers = 1 # Number of RNN layers choices\n","learning_rate = 0.1  # Learning rate choices\n","batch_size = 32  # Batch size choices\n","num_epochs = 50  # num epoch choices\n","optimizer_name = 'SGD'  # Optimizer choices\n","#[I 2024-10-26 05:27:51,546] Trial 27 finished with value: 0.7720450281425891 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 27 with value: 0.7720450281425891."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fRYPsdwQZ6B0"},"outputs":[],"source":["# Reset the seed to ensure fairness when comparing performances\n","set_seed()\n","\n","# Load Data\n","train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","\n","# Initialize the model\n","model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation='mean_pool')\n","model.to(device)\n","# Use BCELoss for binary classification\n","criterion = nn.BCELoss()\n","# Choose optimizer based on trial suggestion\n","optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","# Initialize the early stopper\n","early_stopper = EarlyStopper()\n","\n","# Lists to store loss and accuracy for visualization\n","train_losses = []\n","val_accuracies = []\n","\n","# Training Loop\n","best_val_accuracy = 0\n","best_model = None\n","for epoch in range(num_epochs):\n","    # Train on train set\n","    train_loss = train_loop(train_loader, model, criterion, optimizer)\n","    train_losses.append(train_loss)\n","\n","    # Evaluate on validation set\n","    val_accuracy = test_loop(val_loader, model)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Get the best model\n","    if val_accuracy > best_val_accuracy:\n","      best_val_accuracy = val_accuracy\n","      best_model = copy.deepcopy(model)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    # Early stopping\n","    if early_stopper.early_stop(val_accuracy):\n","        print(\"Early stopping triggered!\")\n","        break\n","\n","# Evaluate on test data\n","test_accuracy = test_loop(test_loader, best_model)\n","\n","# Print out the performance\n","print(f\"Using Best Model, Test Accuracy: {test_accuracy:.4f}\") #0.7702"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvnAXdYB5_iI"},"outputs":[],"source":["BILSTM_word2vec_MeanPool = 0.7702"]},{"cell_type":"markdown","metadata":{"id":"iwOA2AZ1Xkn2"},"source":["#**BiLSTM - Word2Vec (MaxPool - Unfreeze Embedding) - Normal Embedding**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dICST8TcXgih"},"outputs":[],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Reset the seed for fairness\n","    torch.manual_seed(42)\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [10, 20, 30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation = \"max_pool\",freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","# Start the Optuna study\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)\n","\n","#[I 2024-10-26 04:55:48,552] Trial 25 finished with value: 0.7908067542213884 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 25 with value: 0.7908067542213884."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-spwZlmFY3Dr"},"outputs":[],"source":["# The Best Hyperparameters\n","hidden_size = 64  # Hidden size choices\n","num_layers = 1 # Number of RNN layers choices\n","learning_rate = 0.1  # Learning rate choices\n","batch_size = 32  # Batch size choices\n","num_epochs = 50  # num epoch choices\n","optimizer_name = 'SGD'  # Optimizer choices\n","#[I 2024-10-26 04:55:48,552] Trial 25 finished with value: 0.7908067542213884 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 25 with value: 0.7908067542213884."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xxUEMq_9ZFtE"},"outputs":[],"source":["# Reset the seed to ensure fairness when comparing performances\n","set_seed()\n","\n","# Load Data\n","train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","\n","# Initialize the model\n","model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation='max_pool')\n","model.to(device)\n","# Use BCELoss for binary classification\n","criterion = nn.BCELoss()\n","# Choose optimizer based on trial suggestion\n","optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","# Initialize the early stopper\n","early_stopper = EarlyStopper()\n","\n","# Lists to store loss and accuracy for visualization\n","train_losses = []\n","val_accuracies = []\n","\n","# Training Loop\n","best_val_accuracy = 0\n","best_model = None\n","for epoch in range(num_epochs):\n","    # Train on train set\n","    train_loss = train_loop(train_loader, model, criterion, optimizer)\n","    train_losses.append(train_loss)\n","\n","    # Evaluate on validation set\n","    val_accuracy = test_loop(val_loader, model)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Get the best model\n","    if val_accuracy > best_val_accuracy:\n","      best_val_accuracy = val_accuracy\n","      best_model = copy.deepcopy(model)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    # Early stopping\n","    if early_stopper.early_stop(val_accuracy):\n","        print(\"Early stopping triggered!\")\n","        break\n","\n","# Evaluate on test data\n","test_accuracy = test_loop(test_loader, best_model)\n","\n","# Print out the performance\n","print(f\"Using Best Model, Test Accuracy: {test_accuracy:.4f}\") #0.7720"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p8F_ljiL60UR"},"outputs":[],"source":["BILSTM_word2vec_MaxPool = 0.7720"]},{"cell_type":"markdown","metadata":{"id":"MEy9arA5-jc8"},"source":["#**BiLSTM - Word2Vec (Last Hidden State - Unfreeze Embedding)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FTjrhx7pDvgY"},"outputs":[],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Reset the seed for fairness\n","    torch.manual_seed(42)\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [10, 20, 30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","# Start the Optuna study\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)\n","\n","#[I 2024-10-26 03:52:57,093] Trial 33 finished with value: 0.7842401500938087 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 10, 'optimizer': 'RMSprop'}. Best is trial 33 with value: 0.7842401500938087."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"65K76GM2XKtV"},"outputs":[],"source":["# The Best Hyperparameters\n","hidden_size = 256  # Hidden size choices\n","num_layers = 2 # Number of RNN layers choices\n","learning_rate = 0.001  # Learning rate choices\n","batch_size = 32  # Batch size choices\n","num_epochs = 10  # num epoch choices\n","optimizer_name = 'RMSprop'  # Optimizer choices\n","#[I 2024-10-25 15:00:47,380] Trial 16 finished with value: 0.7898686679174484 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 16 with value: 0.7898686679174484."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35588,"status":"ok","timestamp":1730561886461,"user":{"displayName":"Alvin Khoo","userId":"12471070573568746260"},"user_tz":-480},"id":"YYMYBczfVjrD","outputId":"a6d0b385-192c-4842-af81-7d21badcf9f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/10], Train Loss: 0.6431, Validation Accuracy: 0.6144\n","Epoch [2/10], Train Loss: 0.5481, Validation Accuracy: 0.7523\n","Epoch [3/10], Train Loss: 0.4751, Validation Accuracy: 0.7598\n","Epoch [4/10], Train Loss: 0.4433, Validation Accuracy: 0.7720\n","Epoch [5/10], Train Loss: 0.4124, Validation Accuracy: 0.7777\n","Epoch [6/10], Train Loss: 0.3806, Validation Accuracy: 0.7711\n","Epoch [7/10], Train Loss: 0.3353, Validation Accuracy: 0.7899\n","Epoch [8/10], Train Loss: 0.2930, Validation Accuracy: 0.7805\n","Epoch [9/10], Train Loss: 0.2407, Validation Accuracy: 0.7702\n","Epoch [10/10], Train Loss: 0.1939, Validation Accuracy: 0.7824\n","Using Best Model, Test Accuracy: 0.7889\n"]}],"source":["# Reset the seed to ensure fairness when comparing performances\n","set_seed()\n","\n","# Load Data\n","train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","\n","# Initialize the model\n","model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers)\n","model.to(device)\n","# Use BCELoss for binary classification\n","criterion = nn.BCELoss()\n","# Choose optimizer based on trial suggestion\n","optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","# Initialize the early stopper\n","early_stopper = EarlyStopper()\n","\n","# Lists to store loss and accuracy for visualization\n","train_losses = []\n","val_accuracies = []\n","\n","# Training Loop\n","best_val_accuracy = 0\n","best_model = None\n","for epoch in range(num_epochs):\n","    # Train on train set\n","    train_loss = train_loop(train_loader, model, criterion, optimizer)\n","    train_losses.append(train_loss)\n","\n","    # Evaluate on validation set\n","    val_accuracy = test_loop(val_loader, model)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Get the best model\n","    if val_accuracy > best_val_accuracy:\n","      best_val_accuracy = val_accuracy\n","      best_model = copy.deepcopy(model)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    # Early stopping\n","    if early_stopper.early_stop(val_accuracy):\n","        print(\"Early stopping triggered!\")\n","        break\n","\n","# Evaluate on test data\n","test_accuracy = test_loop(test_loader, best_model)\n","\n","# Print out the performance\n","print(f\"Using Best Model, Test Accuracy: {test_accuracy:.4f}\") #0.7889"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFLhUhBu63WX"},"outputs":[],"source":["BILSTM_word2vec_LHS = 0.7720"]},{"cell_type":"markdown","metadata":{"id":"jzvMkDroZek5"},"source":["#**Load FastText Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gfQygCm5Xgpa"},"outputs":[],"source":["!pip install fasttext\n","import fasttext\n","from google.colab import drive\n","drive.mount('/content/drive')\n","fasttext_model = fasttext.load_model(PATH_TO_FASTTEXT_MODEL_DRIVE)\n","# # Load the pre-trained Word2Vec model\n","# if 'google.colab' in sys.modules:\n","#     print(\"Running on Google Colab's hosted runtime\")\n","#     from google.colab import drive\n","#     drive.mount('/content/drive')\n","#     fasttext_model = load_facebook_model(PATH_TO_FASTTEXT_MODEL_DRIVE)\n","# else:\n","#     print(\"Running on a local runtime\")\n","#     fasttext_model = load_facebook_model(PATH_TO_FASTTEXT_MODEL_LOCAL)\n","\n","# # Get the dimension of the embeddings\n","# fasttext_vector_dim = fasttext_model.vector_size\n","# print(f\"FastText Dimension: {fasttext_vector_dim}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87QvS8c3yxnk"},"outputs":[],"source":["# Initialize the lemmatizer and stopwords\n","lemmatizer = WordNetLemmatizer()\n","punctuation = set(string.punctuation)\n","stop_words = set(stopwords.words('english'))\n","\n","# Dataset Preprocessing to tokenize and/or lemmatize\n","def preprocess_dataset(data, lemmatization = True):\n","    processed_sentences = []\n","\n","    for entry in data:\n","        text = entry['text']\n","        # Tokenize the sentence\n","        tokens = word_tokenize(text)\n","        if lemmatization:\n","          # With case folding, punctuation and stop words removal\n","          tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if word not in punctuation and word.lower() not in stop_words]\n","\n","        processed_sentences.append(tokens)\n","\n","    return processed_sentences\n","\n","# Get vocab and oov words\n","def get_vocab_OOV(sentences, w2v_model):\n","    vocabulary = set()\n","    oov_words = set()\n","\n","    for sentence in sentences:\n","        for word in sentence:\n","            vocabulary.add(word)\n","            if word not in w2v_model:\n","                oov_words.add(word)\n","\n","    return vocabulary, oov_words\n","\n","vocabulary, oov_words = get_vocab_OOV(preprocess_dataset(train_dataset, False), fasttext_model)\n","lemmatized_vocabulary, lemmatized_oov_words = get_vocab_OOV(preprocess_dataset(train_dataset), fasttext_model)\n","\n","print(f\"Before lemmatization: Vocabulary size: {len(vocabulary)}, OOV words: {len(oov_words)}\")\n","print(f\"After lemmatization: Vocabulary size: {len(lemmatized_vocabulary)}, OOV words: {len(lemmatized_oov_words)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pr-zIhtoyyeN"},"outputs":[],"source":["preprocessed_train_dataset = preprocess_dataset(train_dataset)\n","preprocessed_validation_dataset = preprocess_dataset(validation_dataset)\n","preprocessed_test_dataset = preprocess_dataset(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2kJnfH5Q83aJ"},"outputs":[],"source":["def drop_oov(preprocessed_dataset, w2v_model):\n","    return [[word for word in sentence if word in w2v_model] for sentence in preprocessed_dataset]\n","\n","def drop_non_train_vocab(dataset, train_vocab):\n","    return [[word for word in sentence if word in train_vocab] for sentence in dataset]\n","\n","X_train = drop_oov(preprocessed_train_dataset, fasttext_model)\n","Y_train = [entry['label'] for entry in train_dataset]\n","\n","X_val = drop_non_train_vocab(drop_oov(preprocessed_validation_dataset, fasttext_model), lemmatized_vocabulary)\n","Y_val = [entry['label'] for entry in validation_dataset]\n","\n","X_test = drop_non_train_vocab(drop_oov(preprocessed_test_dataset, fasttext_model), lemmatized_vocabulary)\n","Y_test = [entry['label'] for entry in test_dataset]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sa64J1xe6fPM"},"outputs":[],"source":["def get_word2index(train_vocab):\n","    word2index = {}\n","    for idx, word in enumerate(train_vocab):\n","        word2index[word] = idx + 1  # +1 to offset 0 for padding\n","    return word2index\n","\n","word2index = get_word2index(lemmatized_vocabulary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mruicZQF9O-X"},"outputs":[],"source":["def create_embedding_matrix(w2v_model, word2index, embedding_dim=300):\n","    \"\"\"\n","    Creates an embedding matrix based on the word2index mapping and Word2Vec model.\n","\n","    Args:\n","    - w2v_model: Pre-trained Word2Vec model.\n","    - word2index: Dictionary mapping words to their indices.\n","    - embedding_dim: Dimension of the Word2Vec word vectors (default: 300).\n","\n","    Returns:\n","    - embedding_matrix: Embedding matrix where each row corresponds to the vector of a word in the vocabulary.\n","    \"\"\"\n","    vocab_size = len(word2index)  # Number of words in the training vocabulary\n","\n","    # Initialize the embedding matrix with zeros (or any other value)\n","    embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n","\n","    for word, idx in word2index.items():\n","        if word in w2v_model:\n","            # If the word exists in the Word2Vec model, use its pre-trained embedding\n","            embedding_matrix[idx] = w2v_model[word]\n","        else:\n","            # For out-of-vocabulary (OOV) words, initialize a random vector\n","            embedding_matrix[idx] = np.random.normal(size=(embedding_dim,))\n","\n","    return embedding_matrix\n","\n","embedding_dim = fasttext_model.get_dimension()  # Get the dimension of the word vectors in Word2Vec\n","embedding_matrix = create_embedding_matrix(fasttext_model, word2index, embedding_dim)\n","print(f\"embedding_matrix shape: {embedding_matrix.shape}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qg2x_Gp39S8J"},"outputs":[],"source":["# Get maximum sentence length\n","def get_max_sentence_length(*datasets):\n","    \"\"\"\n","    Get the maximum sentence length from the provided datasets.\n","\n","    Args:\n","    - datasets: A variable number of lists containing tokenized sentences.\n","\n","    Returns:\n","    - max_length: The maximum sentence length across all datasets.\n","    \"\"\"\n","    max_length = 0\n","    for index, dataset in enumerate(datasets):\n","        # Update max_length if a longer sentence is found\n","        max_length = max(max_length, max(len(sentence) for sentence in dataset))\n","        min_length = min(len(sentence) for sentence in dataset)\n","        print(f\"Min sentence length in dataset {index}: {min_length}\")\n","    return max_length\n","\n","# Example usage\n","max_length = get_max_sentence_length(X_train, X_val, X_test)\n","print(f\"Maximum sentence length of three datasets: {max_length}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RaEpjSwc9Xg7"},"outputs":[],"source":["def convert_to_sequence_data(tokenized_data, word2index, max_length=max_length):\n","    \"\"\"\n","    Preprocess the tokenized data for RNN input.\n","\n","    Args:\n","    - tokenized_data: List of tokenized sentences.\n","    - word2index: Dictionary mapping words to indices.\n","    - max_length: Maximum length for padding (default is 100).\n","\n","    Returns:\n","    - padded_sequences: Padded tensor of sequences.\n","    \"\"\"\n","    # Convert tokens to indices\n","    indexed_sequences = []\n","    for sentence in tokenized_data:\n","        indexed_sentence = [word2index.get(word, 0) for word in sentence]  # Use 0 for OOV words\n","\n","        # Truncate or pad the indexed sentence to the specified max_length\n","        if len(indexed_sentence) > max_length:\n","            indexed_sentence = indexed_sentence[:max_length]  # Truncate\n","        else:\n","            indexed_sentence += [0] * (max_length - len(indexed_sentence))  # Pad with zeros\n","\n","        indexed_sequences.append(torch.tensor(indexed_sentence, dtype=torch.long))\n","\n","    # Convert list of tensors to a padded tensor\n","    padded_sequences = torch.stack(indexed_sequences)\n","\n","    return padded_sequences\n","\n","# Convert to sequence\n","X_train_sequence = convert_to_sequence_data(X_train, word2index)\n","X_val_sequence = convert_to_sequence_data(X_val , word2index)\n","X_test_sequence = convert_to_sequence_data(X_test, word2index)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"csd6nBepXgtd"},"outputs":[],"source":["import gc\n","del fasttext_model\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"kuv_VbgNnI6f"},"source":["\n","#**BiLSTM - FastText (MeanPool - Unfreeze Embedding)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zm-Q6FhbXgvl"},"outputs":[],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Reset the seed for fairness\n","    torch.manual_seed(42)\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [10, 20, 30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation = \"mean_pool\",freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","# Start the Optuna study\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)\n","\n","#[I 2024-10-26 11:05:02,604] Trial 49 finished with value: 0.775797373358349 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 7 with value: 0.7823639774859287."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o0QgpnIHclYy"},"outputs":[],"source":["# The Best Hyperparameters\n","hidden_size = 128  # Hidden size choices\n","num_layers = 3 # Number of RNN layers choices\n","learning_rate = 1e-05  # Learning rate choices\n","batch_size = 128  # Batch size choices\n","num_epochs = 50  # num epoch choices\n","optimizer_name = 'RMSprop'  # Optimizer choices\n","\n","#[I 2024-10-26 11:05:02,604] Trial 49 finished with value: 0.775797373358349 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 7 with value: 0.7823639774859287"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":89063,"status":"ok","timestamp":1730561812015,"user":{"displayName":"Alvin Khoo","userId":"12471070573568746260"},"user_tz":-480},"id":"E0kHIVmzdj6O","outputId":"eb3c7348-9822-4221-aebb-9bd50e47309f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/50], Train Loss: 0.6932, Validation Accuracy: 0.5000\n","Epoch [2/50], Train Loss: 0.6925, Validation Accuracy: 0.5000\n","Epoch [3/50], Train Loss: 0.6919, Validation Accuracy: 0.5253\n","Epoch [4/50], Train Loss: 0.6911, Validation Accuracy: 0.6370\n","Epoch [5/50], Train Loss: 0.6899, Validation Accuracy: 0.6407\n","Epoch [6/50], Train Loss: 0.6882, Validation Accuracy: 0.6482\n","Epoch [7/50], Train Loss: 0.6857, Validation Accuracy: 0.6454\n","Epoch [8/50], Train Loss: 0.6814, Validation Accuracy: 0.6463\n","Epoch [9/50], Train Loss: 0.6743, Validation Accuracy: 0.6520\n","Epoch [10/50], Train Loss: 0.6628, Validation Accuracy: 0.6689\n","Epoch [11/50], Train Loss: 0.6447, Validation Accuracy: 0.6604\n","Epoch [12/50], Train Loss: 0.6209, Validation Accuracy: 0.6651\n","Epoch [13/50], Train Loss: 0.5967, Validation Accuracy: 0.6745\n","Epoch [14/50], Train Loss: 0.5757, Validation Accuracy: 0.6839\n","Epoch [15/50], Train Loss: 0.5596, Validation Accuracy: 0.6998\n","Epoch [16/50], Train Loss: 0.5463, Validation Accuracy: 0.7036\n","Epoch [17/50], Train Loss: 0.5361, Validation Accuracy: 0.7083\n","Epoch [18/50], Train Loss: 0.5274, Validation Accuracy: 0.7092\n","Epoch [19/50], Train Loss: 0.5215, Validation Accuracy: 0.7195\n","Epoch [20/50], Train Loss: 0.5154, Validation Accuracy: 0.7223\n","Epoch [21/50], Train Loss: 0.5107, Validation Accuracy: 0.7261\n","Epoch [22/50], Train Loss: 0.5071, Validation Accuracy: 0.7280\n","Epoch [23/50], Train Loss: 0.5032, Validation Accuracy: 0.7308\n","Epoch [24/50], Train Loss: 0.5004, Validation Accuracy: 0.7251\n","Epoch [25/50], Train Loss: 0.4988, Validation Accuracy: 0.7364\n","Epoch [26/50], Train Loss: 0.4960, Validation Accuracy: 0.7345\n","Epoch [27/50], Train Loss: 0.4939, Validation Accuracy: 0.7345\n","Epoch [28/50], Train Loss: 0.4918, Validation Accuracy: 0.7345\n","Epoch [29/50], Train Loss: 0.4900, Validation Accuracy: 0.7373\n","Epoch [30/50], Train Loss: 0.4882, Validation Accuracy: 0.7383\n","Epoch [31/50], Train Loss: 0.4868, Validation Accuracy: 0.7336\n","Epoch [32/50], Train Loss: 0.4855, Validation Accuracy: 0.7364\n","Epoch [33/50], Train Loss: 0.4838, Validation Accuracy: 0.7420\n","Epoch [34/50], Train Loss: 0.4831, Validation Accuracy: 0.7364\n","Epoch [35/50], Train Loss: 0.4810, Validation Accuracy: 0.7345\n","Epoch [36/50], Train Loss: 0.4804, Validation Accuracy: 0.7373\n","Epoch [37/50], Train Loss: 0.4793, Validation Accuracy: 0.7402\n","Epoch [38/50], Train Loss: 0.4777, Validation Accuracy: 0.7345\n","Epoch [39/50], Train Loss: 0.4766, Validation Accuracy: 0.7364\n","Epoch [40/50], Train Loss: 0.4760, Validation Accuracy: 0.7383\n","Epoch [41/50], Train Loss: 0.4738, Validation Accuracy: 0.7383\n","Epoch [42/50], Train Loss: 0.4736, Validation Accuracy: 0.7448\n","Epoch [43/50], Train Loss: 0.4726, Validation Accuracy: 0.7383\n","Epoch [44/50], Train Loss: 0.4707, Validation Accuracy: 0.7392\n","Epoch [45/50], Train Loss: 0.4710, Validation Accuracy: 0.7411\n","Epoch [46/50], Train Loss: 0.4706, Validation Accuracy: 0.7392\n","Epoch [47/50], Train Loss: 0.4693, Validation Accuracy: 0.7411\n","Epoch [48/50], Train Loss: 0.4676, Validation Accuracy: 0.7430\n","Epoch [49/50], Train Loss: 0.4670, Validation Accuracy: 0.7448\n","Epoch [50/50], Train Loss: 0.4670, Validation Accuracy: 0.7439\n","Using Best Model, Test Accuracy: 0.7636\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1123: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1410.)\n","  result = _VF.lstm(\n"]}],"source":["# Reset the seed to ensure fairness when comparing performances\n","set_seed()\n","\n","# Load Data\n","train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","\n","# Initialize the model\n","model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation='mean_pool')\n","model.to(device)\n","# Use BCELoss for binary classification\n","criterion = nn.BCELoss()\n","# Choose optimizer based on trial suggestion\n","optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","# Initialize the early stopper\n","early_stopper = EarlyStopper()\n","\n","# Lists to store loss and accuracy for visualization\n","train_losses = []\n","val_accuracies = []\n","\n","# Training Loop\n","best_val_accuracy = 0\n","best_model = None\n","for epoch in range(num_epochs):\n","    # Train on train set\n","    train_loss = train_loop(train_loader, model, criterion, optimizer)\n","    train_losses.append(train_loss)\n","\n","    # Evaluate on validation set\n","    val_accuracy = test_loop(val_loader, model)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Get the best model\n","    if val_accuracy > best_val_accuracy:\n","      best_val_accuracy = val_accuracy\n","      best_model = copy.deepcopy(model)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    # Early stopping\n","    if early_stopper.early_stop(val_accuracy):\n","        print(\"Early stopping triggered!\")\n","        break\n","\n","# Evaluate on test data\n","test_accuracy = test_loop(test_loader, best_model)\n","\n","# Print out the performance\n","print(f\"Using Best Model, Test Accuracy: {test_accuracy:.4f}\") #0.7486"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FbXh6is_57IR"},"outputs":[],"source":["BILSTM_FT_MeanPool = 0.7486"]},{"cell_type":"markdown","metadata":{"id":"e9wvOWNynBor"},"source":["#**BiLSTM - FastText (MaxPool - Unfreeze Embedding)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0DE8O8ieXgyD"},"outputs":[],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Reset the seed for fairness\n","    torch.manual_seed(42)\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [10, 20, 30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation = \"max_pool\",freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","# Start the Optuna study\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)\n","\n","#[I 2024-10-26 11:15:41,274] Trial 11 finished with value: 0.7842401500938087 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.7842401500938087."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2kOFntRk4PVU"},"outputs":[],"source":["# The Best Hyperparameters\n","hidden_size = 128  # Hidden size choices\n","num_layers = 2 # Number of RNN layers choices\n","learning_rate = 1e-05  # Learning rate choices\n","batch_size = 128  # Batch size choices\n","num_epochs = 40  # num epoch choices\n","optimizer_name = 'Adam'  # Optimizer choices\n","#[I 2024-10-26 11:15:41,274] Trial 11 finished with value: 0.7842401500938087 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.7842401500938087."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"06qGJIwh4XK8"},"outputs":[],"source":["# Reset the seed to ensure fairness when comparing performances\n","set_seed()\n","\n","# Load Data\n","train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","\n","# Initialize the model\n","model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation='mean_pool')\n","model.to(device)\n","# Use BCELoss for binary classification\n","criterion = nn.BCELoss()\n","# Choose optimizer based on trial suggestion\n","optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","# Initialize the early stopper\n","early_stopper = EarlyStopper()\n","\n","# Lists to store loss and accuracy for visualization\n","train_losses = []\n","val_accuracies = []\n","\n","# Training Loop\n","best_val_accuracy = 0\n","best_model = None\n","for epoch in range(num_epochs):\n","    # Train on train set\n","    train_loss = train_loop(train_loader, model, criterion, optimizer)\n","    train_losses.append(train_loss)\n","\n","    # Evaluate on validation set\n","    val_accuracy = test_loop(val_loader, model)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Get the best model\n","    if val_accuracy > best_val_accuracy:\n","      best_val_accuracy = val_accuracy\n","      best_model = copy.deepcopy(model)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    # Early stopping\n","    if early_stopper.early_stop(val_accuracy):\n","        print(\"Early stopping triggered!\")\n","        break\n","\n","# Evaluate on test data\n","test_accuracy = test_loop(test_loader, best_model)\n","\n","# Print out the performance\n","print(f\"Using Best Model, Test Accuracy: {test_accuracy:.4f}\") #0.7336"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkRzXB_2502o"},"outputs":[],"source":["BILSTM_FT_MaxPool = 0.7336"]},{"cell_type":"markdown","metadata":{"id":"5KeLj87vnO4B"},"source":["\n","#**BiLSTM - FastText (Last Hidden Sequence - Unfreeze Embedding)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8rHsZPbXkCc"},"outputs":[],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Reset the seed for fairness\n","    torch.manual_seed(42)\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [10, 20, 30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","# Start the Optuna study\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)\n","#[I 2024-10-26 12:21:30,570] Trial 3 finished with value: 0.7804878048780488 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7804878048780488."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XejMbQzT4f7T"},"outputs":[],"source":["# The Best Hyperparameters\n","hidden_size = 128  # Hidden size choices\n","num_layers = 3 # Number of RNN layers choices\n","learning_rate = 1e-05  # Learning rate choices\n","batch_size = 64  # Batch size choices\n","num_epochs = 40  # num epoch choices\n","optimizer_name = 'Adam'  # Optimizer choices\n","#[I 2024-10-26 12:21:30,570] Trial 3 finished with value: 0.7804878048780488 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7804878048780488."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A6jxAKNX4s38"},"outputs":[],"source":["# Reset the seed to ensure fairness when comparing performances\n","set_seed()\n","\n","# Load Data\n","train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","\n","# Initialize the model\n","model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation='mean_pool')\n","model.to(device)\n","# Use BCELoss for binary classification\n","criterion = nn.BCELoss()\n","# Choose optimizer based on trial suggestion\n","optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","# Initialize the early stopper\n","early_stopper = EarlyStopper()\n","\n","# Lists to store loss and accuracy for visualization\n","train_losses = []\n","val_accuracies = []\n","\n","# Training Loop\n","best_val_accuracy = 0\n","best_model = None\n","for epoch in range(num_epochs):\n","    # Train on train set\n","    train_loss = train_loop(train_loader, model, criterion, optimizer)\n","    train_losses.append(train_loss)\n","\n","    # Evaluate on validation set\n","    val_accuracy = test_loop(val_loader, model)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Get the best model\n","    if val_accuracy > best_val_accuracy:\n","      best_val_accuracy = val_accuracy\n","      best_model = copy.deepcopy(model)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    # Early stopping\n","    if early_stopper.early_stop(val_accuracy):\n","        print(\"Early stopping triggered!\")\n","        break\n","\n","# Evaluate on test data\n","test_accuracy = test_loop(test_loader, best_model)\n","\n","# Print out the performance\n","print(f\"Using Best Model, Test Accuracy: {test_accuracy:.4f}\") #0.7486"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vs_-pFSm5mzz"},"outputs":[],"source":["BILSTM_FT_LHS = 0.7486"]},{"cell_type":"markdown","metadata":{"id":"DfhnSzJY5ny7"},"source":["#**Results (without OOV and stop words)**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hqzItOb35pzH"},"outputs":[],"source":["\n","import pandas as pd\n","\n","# Define data for each model type in a list of dictionaries\n","data = [\n","    {\"Type\": \"BILSTM_FT_LHS\", \"Test Accuracy\": BILSTM_FT_LHS},\n","    {\"Type\": \"BILSTM_FT_MeanPool\", \"Test Accuracy\": BILSTM_FT_MeanPool},\n","    {\"Type\": \"BILSTM_FT_MaxPool\", \"Test Accuracy\": BILSTM_FT_MaxPool},\n","    {\"Type\": \"BILSTM_word2vec_LHS\", \"Test Accuracy\": BILSTM_word2vec_LHS},\n","    {\"Type\": \"BILSTM_word2vec_MeanPool\", \"Test Accuracy\": BILSTM_word2vec_MeanPool},\n","    {\"Type\": \"BILSTM_word2vec_MaxPool\", \"Test Accuracy\": BILSTM_word2vec_MaxPool}\n","]\n","\n","# Create the DataFrame directly from the list of dictionaries\n","df = pd.DataFrame(data)\n","\n","# Display the DataFrame\n","print(df)"]},{"cell_type":"markdown","metadata":{"id":"jpu4M4jB96DL"},"source":["#**Experiment 1 (OOV in train and with stop words)**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":71916,"status":"ok","timestamp":1730037174933,"user":{"displayName":"Alvin Khoo","userId":"12471070573568746260"},"user_tz":-480},"id":"-4Trz54f-PB7","outputId":"a46c7b1a-8ab7-42eb-b116-1c37526e656b"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-10-27 12:40:09,553] A new study created in memory with name: no-name-448ef0cf-c7d7-448c-9bae-d99df049cd88\n","[I 2024-10-27 12:40:17,786] Trial 0 finished with value: 0.7711069418386491 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 20, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.7711069418386491.\n","[I 2024-10-27 12:42:28,171] Trial 1 finished with value: 0.7776735459662288 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7776735459662288.\n","[I 2024-10-27 12:42:38,642] Trial 2 finished with value: 0.7401500938086304 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 10, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7776735459662288.\n","[I 2024-10-27 12:44:01,078] Trial 3 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 12:44:28,442] Trial 4 finished with value: 0.5 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 12:44:54,112] Trial 5 finished with value: 0.5600375234521576 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 20, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 12:47:32,831] Trial 6 finished with value: 0.775797373358349 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 12:48:29,050] Trial 7 finished with value: 0.5290806754221389 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 20, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 12:48:43,922] Trial 8 finished with value: 0.574108818011257 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 10, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 12:49:51,481] Trial 9 finished with value: 0.5403377110694184 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 20, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 12:54:30,246] Trial 10 finished with value: 0.5337711069418386 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 12:56:38,132] Trial 11 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 12:59:28,330] Trial 12 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:03:13,793] Trial 13 finished with value: 0.7804878048780488 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:03:55,821] Trial 14 finished with value: 0.774859287054409 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:05:28,975] Trial 15 finished with value: 0.7842401500938087 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:07:18,665] Trial 16 finished with value: 0.776735459662289 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:07:57,144] Trial 17 finished with value: 0.7223264540337712 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:09:30,223] Trial 18 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:10:17,127] Trial 19 finished with value: 0.7776735459662288 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:11:29,667] Trial 20 finished with value: 0.50187617260788 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:13:37,788] Trial 21 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:15:45,857] Trial 22 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:16:19,299] Trial 23 finished with value: 0.776735459662289 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:17:01,779] Trial 24 finished with value: 0.6360225140712945 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 10, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:18:04,637] Trial 25 finished with value: 0.7786116322701688 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:20:54,513] Trial 26 finished with value: 0.7504690431519699 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:23:02,259] Trial 27 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:24:05,443] Trial 28 finished with value: 0.7786116322701688 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:24:33,159] Trial 29 finished with value: 0.5590994371482176 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:24:45,620] Trial 30 finished with value: 0.775797373358349 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:27:35,920] Trial 31 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:30:26,216] Trial 32 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:33:16,376] Trial 33 finished with value: 0.776735459662289 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:33:58,790] Trial 34 finished with value: 0.5 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 10, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:36:06,642] Trial 35 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:36:58,954] Trial 36 finished with value: 0.773921200750469 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:38:17,983] Trial 37 finished with value: 0.5 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 20, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:38:57,973] Trial 38 finished with value: 0.7373358348968105 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:39:34,160] Trial 39 finished with value: 0.5562851782363978 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 10, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:40:59,141] Trial 40 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 20, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:42:32,058] Trial 41 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:44:04,959] Trial 42 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:45:38,104] Trial 43 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:47:15,445] Trial 44 finished with value: 0.50187617260788 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:48:08,790] Trial 45 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:50:05,189] Trial 46 finished with value: 0.7485928705440901 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:51:13,391] Trial 47 finished with value: 0.7795497185741088 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:51:44,471] Trial 48 finished with value: 0.5 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 20, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.7861163227016885.\n","[I 2024-10-27 13:52:54,437] Trial 49 finished with value: 0.7504690431519699 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885.\n"]}],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Reset the seed for fairness\n","    torch.manual_seed(42)\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [10, 20, 30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","# Start the Optuna study\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)\n","\n","#[I 2024-10-27 12:44:01,078] Trial 3 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7861163227016885."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2878788,"status":"ok","timestamp":1730041791096,"user":{"displayName":"Alex Khoo Shien How","userId":"06086813408012082662"},"user_tz":-480},"id":"FChw2BYN-PH7","outputId":"4c2622d5-7fb4-43fb-fae8-149c93bb18d9"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-10-27 14:21:52,042] A new study created in memory with name: no-name-7e8ac758-5df4-48c9-8e10-a8001f4acf61\n","[I 2024-10-27 14:22:22,133] Trial 0 finished with value: 0.5028142589118199 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 20, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.5028142589118199.\n","[I 2024-10-27 14:23:37,992] Trial 1 finished with value: 0.7682926829268293 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7682926829268293.\n","[I 2024-10-27 14:24:04,407] Trial 2 finished with value: 0.774859287054409 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.774859287054409.\n","[I 2024-10-27 14:24:18,007] Trial 3 finished with value: 0.49906191369606 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 10, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.774859287054409.\n","[I 2024-10-27 14:25:14,759] Trial 4 finished with value: 0.7954971857410882 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:26:56,114] Trial 5 finished with value: 0.773921200750469 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 20, 'optimizer': 'SGD'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:27:30,067] Trial 6 finished with value: 0.7363977485928705 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 20, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:29:22,730] Trial 7 finished with value: 0.7842401500938087 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:29:44,140] Trial 8 finished with value: 0.7786116322701688 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:30:07,399] Trial 9 finished with value: 0.6463414634146342 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:31:50,513] Trial 10 finished with value: 0.7926829268292683 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:33:33,546] Trial 11 finished with value: 0.7926829268292683 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:35:16,609] Trial 12 finished with value: 0.7926829268292683 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:37:04,928] Trial 13 finished with value: 0.7926829268292683 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:37:34,451] Trial 14 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 10, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:37:53,612] Trial 15 finished with value: 0.7786116322701688 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 10, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:38:18,778] Trial 16 finished with value: 0.7570356472795498 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 10, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:39:29,753] Trial 17 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:40:07,940] Trial 18 finished with value: 0.7851782363977486 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 10, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:40:21,723] Trial 19 finished with value: 0.776735459662289 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:40:40,939] Trial 20 finished with value: 0.7664165103189493 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 10, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:41:51,574] Trial 21 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:43:05,189] Trial 22 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:44:21,315] Trial 23 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:45:32,131] Trial 24 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:47:06,344] Trial 25 finished with value: 0.7626641651031895 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:47:30,140] Trial 26 finished with value: 0.7495309568480301 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 10, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:47:50,710] Trial 27 finished with value: 0.7804878048780488 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:49:50,056] Trial 28 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:50:05,183] Trial 29 finished with value: 0.525328330206379 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 20, 'optimizer': 'SGD'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:50:21,756] Trial 30 finished with value: 0.5544090056285178 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 10, 'optimizer': 'SGD'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:51:32,378] Trial 31 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:52:42,993] Trial 32 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:53:53,559] Trial 33 finished with value: 0.7560975609756098 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:55:27,850] Trial 34 finished with value: 0.7495309568480301 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:56:38,386] Trial 35 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:56:53,426] Trial 36 finished with value: 0.5469043151969981 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 20, 'optimizer': 'SGD'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:58:42,538] Trial 37 finished with value: 0.49906191369606 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 14:59:58,658] Trial 38 finished with value: 0.7626641651031895 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 15:00:49,002] Trial 39 finished with value: 0.7720450281425891 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 15:01:17,019] Trial 40 finished with value: 0.5553470919324578 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 20, 'optimizer': 'SGD'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 15:02:27,465] Trial 41 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 15:03:37,940] Trial 42 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 15:04:48,396] Trial 43 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 15:05:47,317] Trial 44 finished with value: 0.50093808630394 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 15:06:10,946] Trial 45 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 10, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 15:07:21,448] Trial 46 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 15:08:23,992] Trial 47 finished with value: 0.7504690431519699 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 15:09:34,177] Trial 48 finished with value: 0.7776735459662288 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.7954971857410882.\n","[I 2024-10-27 15:09:50,607] Trial 49 finished with value: 0.7786116322701688 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 10, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882.\n"]}],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Reset the seed for fairness\n","    torch.manual_seed(42)\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [10, 20, 30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation=\"max_pool\",freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","# Start the Optuna study\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)\n","\n","#[I 2024-10-27 14:25:14,759] Trial 4 finished with value: 0.7954971857410882 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7954971857410882."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":769748,"status":"ok","timestamp":1730045766863,"user":{"displayName":"Alex Khoo Shien How","userId":"06086813408012082662"},"user_tz":-480},"id":"iM9ky2Gs-PKS","outputId":"6a76a641-3593-4f97-ad20-a73a00978561"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-10-27 15:13:54,631] A new study created in memory with name: no-name-98285b3e-504f-40d5-9555-a177a97e67d7\n","[I 2024-10-27 15:14:16,145] Trial 0 finished with value: 0.7814258911819888 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.7814258911819888.\n","[I 2024-10-27 15:15:17,805] Trial 1 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:15:45,190] Trial 2 finished with value: 0.5 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:16:00,758] Trial 3 finished with value: 0.6097560975609756 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 10, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:16:31,983] Trial 4 finished with value: 0.7804878048780488 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 20, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:16:39,614] Trial 5 finished with value: 0.5065666041275797 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 10, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:16:45,866] Trial 6 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 10, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:18:12,396] Trial 7 finished with value: 0.7776735459662288 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 20, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:18:37,620] Trial 8 finished with value: 0.7589118198874296 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 20, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:19:06,273] Trial 9 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 20, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:20:49,058] Trial 10 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:22:31,305] Trial 11 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:24:14,327] Trial 12 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:25:36,388] Trial 13 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:27:18,750] Trial 14 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:28:20,586] Trial 15 finished with value: 0.7617260787992496 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:29:42,855] Trial 16 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:32:31,791] Trial 17 finished with value: 0.773921200750469 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:33:42,163] Trial 18 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:34:48,078] Trial 19 finished with value: 0.7729831144465291 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:39:13,402] Trial 20 finished with value: 0.5 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:40:55,813] Trial 21 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:42:38,848] Trial 22 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:44:21,079] Trial 23 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:46:03,768] Trial 24 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:46:45,973] Trial 25 finished with value: 0.7354596622889306 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:48:08,090] Trial 26 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:49:50,935] Trial 27 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:50:57,461] Trial 28 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:51:52,735] Trial 29 finished with value: 0.7692307692307693 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:52:31,333] Trial 30 finished with value: 0.7701688555347092 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:54:13,828] Trial 31 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:55:56,290] Trial 32 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:57:36,667] Trial 33 finished with value: 0.5 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:57:53,715] Trial 34 finished with value: 0.5975609756097561 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 10, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:58:58,266] Trial 35 finished with value: 0.7851782363977486 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 15:59:06,336] Trial 36 finished with value: 0.7692307692307693 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 10, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 16:01:55,258] Trial 37 finished with value: 0.773921200750469 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 16:02:34,407] Trial 38 finished with value: 0.7429643527204502 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 16:03:40,930] Trial 39 finished with value: 0.50187617260788 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 20, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 16:04:34,242] Trial 40 finished with value: 0.7720450281425891 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 16:05:56,169] Trial 41 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 16:07:18,033] Trial 42 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 16:08:39,851] Trial 43 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 16:09:59,917] Trial 44 finished with value: 0.5 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 16:10:40,952] Trial 45 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 20, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 16:11:01,156] Trial 46 finished with value: 0.7889305816135085 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 10, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 16:12:00,984] Trial 47 finished with value: 0.7091932457786116 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 16:15:10,069] Trial 48 finished with value: 0.776735459662289 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n","[I 2024-10-27 16:16:06,445] Trial 49 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482.\n"]}],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Reset the seed for fairness\n","    torch.manual_seed(42)\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [10, 20, 30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation=\"mean_pool\",freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","# Start the Optuna study\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)\n","#[I 2024-10-27 15:15:17,805] Trial 1 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7945590994371482."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IWyfMcKgaHqb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7ryqaQ7xJPf"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"eJ_4jRpVaJZJ"},"source":["#Statistical Average"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rgiRSLhYyjZ-"},"outputs":[],"source":["\n","#X_train = drop_oov(preprocessed_train_dataset, w2v_model)\n","X_train = preprocessed_train_dataset\n","Y_train = [entry['label'] for entry in train_dataset]\n","\n","#X_val = drop_non_train_vocab(drop_oov(preprocessed_validation_dataset, w2v_model), lemmatized_vocabulary)\n","X_val = drop_non_train_vocab(preprocessed_validation_dataset, lemmatized_vocabulary)\n","Y_val = [entry['label'] for entry in validation_dataset]\n","\n","#X_test = drop_non_train_vocab(drop_oov(preprocessed_test_dataset, w2v_model), lemmatized_vocabulary)\n","X_test = drop_non_train_vocab(preprocessed_test_dataset, lemmatized_vocabulary)\n","Y_test = [entry['label'] for entry in test_dataset]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":368,"status":"ok","timestamp":1730165605516,"user":{"displayName":"Alvin Khoo","userId":"12471070573568746260"},"user_tz":-480},"id":"-PaeMq0Kyvsi","outputId":"5b6ca54d-d68d-4ca3-829c-5f88e1ee7833"},"outputs":[{"name":"stdout","output_type":"stream","text":["embedding_matrix shape: (16566, 300)\n"]}],"source":["def create_embedding_matrix(w2v_model, word2index, embedding_dim=300):\n","    \"\"\"\n","    Creates an embedding matrix based on the word2index mapping and Word2Vec model.\n","\n","    Args:\n","    - w2v_model: Pre-trained Word2Vec model.\n","    - word2index: Dictionary mapping words to their indices.\n","    - embedding_dim: Dimension of the Word2Vec word vectors (default: 300).\n","\n","    Returns:\n","    - embedding_matrix: Embedding matrix where each row corresponds to the vector of a word in the vocabulary.\n","    \"\"\"\n","    vocab_size = len(word2index)  # Number of words in the training vocabulary\n","\n","    # Initialize the embedding matrix with zeros (or any other value)\n","    embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n","    num_oov = 0\n","\n","    for word, idx in word2index.items():\n","        if word in w2v_model:\n","            # If the word exists in the Word2Vec model, use its pre-trained embedding\n","            embedding_matrix[idx] = w2v_model[word]\n","        else:\n","            num_oov += 1\n","\n","    avg_embedding = np.mean(embedding_matrix[np.nonzero(embedding_matrix)], axis=0)\n","\n","    # Assign average embedding to OOV words\n","    for word, idx in word2index.items():\n","        if embedding_matrix[idx].sum() == 0:  # Check if embedding is all zeros\n","            embedding_matrix[idx] = avg_embedding\n","\n","\n","    return embedding_matrix\n","\n","embedding_dim = w2v_model.vector_size  # Get the dimension of the word vectors in Word2Vec\n","embedding_matrix = create_embedding_matrix(w2v_model, word2index, embedding_dim)\n","print(f\"embedding_matrix shape: {embedding_matrix.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":999},"executionInfo":{"elapsed":3277,"status":"error","timestamp":1730165642576,"user":{"displayName":"Alvin Khoo","userId":"12471070573568746260"},"user_tz":-480},"id":"P2KNC9ed-PO0","outputId":"502fbd8f-b1a2-464d-dc9c-cdb9c6058b7e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-10-29 01:33:59,187] A new study created in memory with name: no-name-00ea7faf-212d-42cf-802c-53d965c3a944\n","[W 2024-10-29 01:34:02,063] Trial 0 failed with parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'} because of the following error: KeyboardInterrupt().\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n","    value_or_values = func(trial)\n","  File \"<ipython-input-28-a7714a43bc85>\", line 29, in objective\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","  File \"<ipython-input-4-3e3d26fd2c5d>\", line 48, in optimizer_factory\n","    return torch.optim.RMSprop(model.parameters(), lr = learning_rate)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/rmsprop.py\", line 68, in __init__\n","    super().__init__(params, defaults)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\", line 371, in __init__\n","    self.add_param_group(cast(dict, param_group))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_compile.py\", line 27, in inner\n","    import torch._dynamo\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/__init__.py\", line 3, in <module>\n","    from . import convert_frame, eval_frame, resume_execution\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 31, in <module>\n","    from torch._dynamo.utils import CompileTimeInstructionCounter\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 62, in <module>\n","    import torch.fx.experimental.symbolic_shapes\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 65, in <module>\n","    from torch.utils._sympy.functions import (\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_sympy/functions.py\", line 7, in <module>\n","    import sympy\n","  File \"/usr/local/lib/python3.10/dist-packages/sympy/__init__.py\", line 173, in <module>\n","    from .solvers import (solve, solve_linear_system, solve_linear_system_LU,\n","  File \"/usr/local/lib/python3.10/dist-packages/sympy/solvers/__init__.py\", line 21, in <module>\n","    from .ode import checkodesol, classify_ode, dsolve, \\\n","  File \"/usr/local/lib/python3.10/dist-packages/sympy/solvers/ode/__init__.py\", line 1, in <module>\n","    from .ode import (allhints, checkinfsol, classify_ode,\n","  File \"/usr/local/lib/python3.10/dist-packages/sympy/solvers/ode/ode.py\", line 3575, in <module>\n","    from .single import SingleODEProblem, SingleODESolver, solver_map\n","  File \"/usr/local/lib/python3.10/dist-packages/sympy/solvers/ode/single.py\", line 34, in <module>\n","    from .lie_group import _ode_lie_group\n","  File \"/usr/local/lib/python3.10/dist-packages/sympy/solvers/ode/lie_group.py\", line 34, in <module>\n","    from sympy.solvers.pde import pdsolve\n","KeyboardInterrupt\n","[W 2024-10-29 01:34:02,074] Trial 0 failed with value None.\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-a7714a43bc85>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Start the Optuna study\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-a7714a43bc85>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Choose optimizer based on trial suggestion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Evaluation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-3e3d26fd2c5d>\u001b[0m in \u001b[0;36moptimizer_factory\u001b[0;34m(optimizer_name, model, learning_rate, momentum)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptimizer_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'RMSprop'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Method to train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/rmsprop.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, alpha, eps, weight_decay, momentum, centered, capturable, foreach, maximize, differentiable)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mdifferentiable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdifferentiable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         )\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: D105\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;31m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdisable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__dynamo_disable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdisable_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mdisable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallback_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalStateGuard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_compile_pg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompileTimeInstructionCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inductor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minductor_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mShapeGuard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTracingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_dispatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_traceable_wrapper_subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m from torch.utils._sympy.functions import (\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mApplication\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFloorDiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPythonMod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIsNonOverlappingAndDenseIndicator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCleanDiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFloorToInt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCeilToInt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_sympy/functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msympify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sympy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    171\u001b[0m         Integers, Rationals)\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m from .solvers import (solve, solve_linear_system, solve_linear_system_LU,\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0msolve_undetermined_coeffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsolve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolve_linear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchecksol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdet_quick\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0minv_quick\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_assumptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailing_assumptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiophantine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sympy/solvers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrecurr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrsolve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsolve_poly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsolve_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsolve_hyper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheckodesol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassify_ode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsolve\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mhomogeneous_order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sympy/solvers/ode/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from .ode import (allhints, checkinfsol, classify_ode,\n\u001b[0m\u001b[1;32m      2\u001b[0m         constantsimp, dsolve, homogeneous_order)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlie_group\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minfinitesimals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sympy/solvers/ode/ode.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   3573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3574\u001b[0m \u001b[0;31m#This import is written at the bottom to avoid circular imports.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3575\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msingle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSingleODEProblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSingleODESolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sympy/solvers/ode/single.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0m_solve_undetermined_coefficients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_solve_variation_of_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_test_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_undetermined_coefficients_match\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0m_get_simplified_sol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlie_group\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_ode_lie_group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sympy/solvers/ode/lie_group.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     separatevars, simplify)\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolvers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msolve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolvers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpde\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpdsolve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumbered_symbols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Reset the seed for fairness\n","    torch.manual_seed(42)\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [10, 20, 30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation=\"max_pool\",freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","# Start the Optuna study\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)\n"]},{"cell_type":"markdown","metadata":{"id":"gOIzrYMDz82W"},"source":["#BiLSTM with Residual Connections"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xCBgJ5Xq0Biy"},"outputs":[],"source":["class SimpleBiLSTM(nn.Module):\n","    def __init__(self, embedding_matrix, hidden_size, num_layers, aggregation='last_hidden_state', freeze_embedding=True):\n","        super(SimpleBiLSTM, self).__init__()\n","        embedding_dim = embedding_matrix.shape[1]\n","\n","        self.aggregation = aggregation\n","        self.hidden_size = hidden_size  # Store hidden size for reference\n","\n","        # Embedding layer using a pretrained embedding matrix\n","        self.embedding = nn.Embedding.from_pretrained(\n","            torch.tensor(embedding_matrix, dtype=torch.float32),\n","            padding_idx=0,  # Using 0 for padding index\n","            freeze=freeze_embedding,  # Freeze embeddings\n","        )\n","\n","        self.residual_projection = nn.Linear(embedding_dim, hidden_size * 2)\n","\n","        # BiLSTM layer with bidirectional=True\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","        self.dropout = nn.Dropout(0.1)\n","\n","        # Output layer dimensions adjusted for bidirectional LSTM (2 * hidden_size)\n","        self.fc = nn.Linear(4 * hidden_size, 1)  # Adjusted output size for concatenated max and mean pooling\n","\n","    def forward(self, x):\n","        # Pass through the embedding layer\n","        embedded = self.embedding(x)\n","\n","        # Initialize hidden and cell states for both directions (2*num_layers for bidirectional)\n","        h0 = torch.zeros(2 * self.lstm.num_layers, embedded.size(0), self.hidden_size).to(embedded.device)\n","        c0 = torch.zeros(2 * self.lstm.num_layers, embedded.size(0), self.hidden_size).to(embedded.device)\n","\n","        # Forward pass through BiLSTM\n","        out, (hn, cn) = self.lstm(embedded, (h0, c0))\n","\n","        # Apply residual connection\n","        residual = self.residual_projection(embedded)  # Project embeddings to match 'out' dimensions\n","        out = out + residual  # Add residual connection\n","\n","        # Aggregation method\n","        if self.aggregation == 'last_hidden_state':\n","            # Concatenate the last hidden states from both directions\n","            out = torch.cat((hn[-2], hn[-1]), dim=1)  # -2 and -1 refer to the last forward and backward states\n","        elif self.aggregation == 'max_mean_pool':\n","            # Mask out padding tokens for pooling\n","            mask = (x != self.embedding.padding_idx).unsqueeze(-1)\n","\n","            # Max pooling\n","            max_pooled_out = out.masked_fill(~mask, -100000)  # Mask padding tokens with a large negative value\n","            max_pooled_out, _ = torch.max(max_pooled_out, dim=1)\n","\n","            # Mean pooling\n","            mean_pooled_out = out * mask\n","            summed = mean_pooled_out.sum(dim=1)\n","            valid_counts = mask.sum(dim=1).clamp(min=1)\n","            mean_pooled_out = summed / valid_counts\n","\n","            # Concatenate max and mean pooling outputs\n","            out = torch.cat((max_pooled_out, mean_pooled_out), dim=1)\n","\n","        out = self.dropout(out)  # Apply dropout\n","        out = self.fc(out)  # Final fully connected layer\n","\n","        # Sigmoid activation for binary classification\n","        out = torch.sigmoid(out)\n","\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"YsxETN5f3onC","outputId":"52fd7336-d56d-4ad2-ed12-e533090f4f9e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-11-03 06:46:06,350] A new study created in memory with name: no-name-6b63c27b-bc5e-41be-8c66-62bf4779e918\n","[I 2024-11-03 06:47:03,319] Trial 0 finished with value: 0.7317073170731707 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.7317073170731707.\n","[I 2024-11-03 06:50:43,869] Trial 1 finished with value: 0.7908067542213884 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7908067542213884.\n","[I 2024-11-03 06:51:22,876] Trial 2 finished with value: 0.7879924953095685 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7908067542213884.\n","[I 2024-11-03 06:52:25,462] Trial 3 finished with value: 0.7664165103189493 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7908067542213884.\n","[I 2024-11-03 06:55:45,710] Trial 4 finished with value: 0.50093808630394 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7908067542213884.\n","[I 2024-11-03 06:58:54,221] Trial 5 finished with value: 0.5675422138836773 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7908067542213884.\n","[I 2024-11-03 06:59:37,813] Trial 6 finished with value: 0.7523452157598499 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7908067542213884.\n","[I 2024-11-03 07:01:15,347] Trial 7 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7908067542213884.\n","[I 2024-11-03 07:02:15,603] Trial 8 finished with value: 0.50093808630394 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7908067542213884.\n","[I 2024-11-03 07:03:17,654] Trial 9 finished with value: 0.6866791744840526 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7908067542213884.\n","[I 2024-11-03 07:03:52,765] Trial 10 finished with value: 0.5225140712945591 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7908067542213884.\n","[I 2024-11-03 07:05:08,731] Trial 11 finished with value: 0.7776735459662288 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7908067542213884.\n","[I 2024-11-03 07:06:43,785] Trial 12 finished with value: 0.7270168855534709 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7908067542213884.\n","[I 2024-11-03 07:07:17,222] Trial 13 finished with value: 0.50093808630394 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7908067542213884.\n","[I 2024-11-03 07:07:52,005] Trial 14 finished with value: 0.7795497185741088 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7908067542213884.\n"]}],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Reset the seed for fairness\n","    torch.manual_seed(42)\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers,aggregation='max_mean_pool',freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","# Start the Optuna study\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)\n","\n","#[I 2024-11-01 13:40:54,131] Trial 10 finished with value: 0.7964352720450282 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 10 with value: 0.7964352720450282.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hnpSOiejOf_N"},"outputs":[],"source":["# The Best Hyperparameters\n","hidden_size = 64  # Hidden size choices\n","num_layers = 3 # Number of RNN layers choices\n","learning_rate = 1e-05  # Learning rate choices\n","batch_size = 32  # Batch size choices\n","num_epochs = 60  # num epoch choices\n","optimizer_name = 'RMSprop'  # Optimizer choices\n","\n","\n","# [I 2024-11-01 13:40:54,131] Trial 10 finished with value: 0.7964352720450282 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 10 with value: 0.7964352720450282."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":92209,"status":"ok","timestamp":1730615068922,"user":{"displayName":"alvin khoo","userId":"02395113757594492728"},"user_tz":-480},"id":"ZA0kaolARP0P","outputId":"70b8dbea-ded9-4234-dc78-6963c2461932"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/60], Train Loss: 0.6926, Validation Accuracy: 0.5291\n","Epoch [2/60], Train Loss: 0.6886, Validation Accuracy: 0.5976\n","Epoch [3/60], Train Loss: 0.6856, Validation Accuracy: 0.6576\n","Epoch [4/60], Train Loss: 0.6823, Validation Accuracy: 0.6782\n","Epoch [5/60], Train Loss: 0.6781, Validation Accuracy: 0.6820\n","Epoch [6/60], Train Loss: 0.6737, Validation Accuracy: 0.7036\n","Epoch [7/60], Train Loss: 0.6671, Validation Accuracy: 0.7158\n","Epoch [8/60], Train Loss: 0.6577, Validation Accuracy: 0.6932\n","Epoch [9/60], Train Loss: 0.6417, Validation Accuracy: 0.7101\n","Epoch [10/60], Train Loss: 0.6137, Validation Accuracy: 0.7054\n","Epoch [11/60], Train Loss: 0.5767, Validation Accuracy: 0.7158\n","Epoch [12/60], Train Loss: 0.5484, Validation Accuracy: 0.7214\n","Epoch [13/60], Train Loss: 0.5340, Validation Accuracy: 0.7139\n","Epoch [14/60], Train Loss: 0.5238, Validation Accuracy: 0.7317\n","Epoch [15/60], Train Loss: 0.5157, Validation Accuracy: 0.7326\n","Epoch [16/60], Train Loss: 0.5099, Validation Accuracy: 0.7420\n","Epoch [17/60], Train Loss: 0.5045, Validation Accuracy: 0.7392\n","Epoch [18/60], Train Loss: 0.4992, Validation Accuracy: 0.7439\n","Epoch [19/60], Train Loss: 0.4955, Validation Accuracy: 0.7439\n","Epoch [20/60], Train Loss: 0.4924, Validation Accuracy: 0.7495\n","Epoch [21/60], Train Loss: 0.4894, Validation Accuracy: 0.7495\n","Epoch [22/60], Train Loss: 0.4861, Validation Accuracy: 0.7505\n","Epoch [23/60], Train Loss: 0.4835, Validation Accuracy: 0.7458\n","Epoch [24/60], Train Loss: 0.4809, Validation Accuracy: 0.7458\n","Epoch [25/60], Train Loss: 0.4787, Validation Accuracy: 0.7420\n","Epoch [26/60], Train Loss: 0.4767, Validation Accuracy: 0.7486\n","Epoch [27/60], Train Loss: 0.4744, Validation Accuracy: 0.7486\n","Epoch [28/60], Train Loss: 0.4726, Validation Accuracy: 0.7458\n","Epoch [29/60], Train Loss: 0.4711, Validation Accuracy: 0.7458\n","Epoch [30/60], Train Loss: 0.4694, Validation Accuracy: 0.7439\n","Epoch [31/60], Train Loss: 0.4669, Validation Accuracy: 0.7458\n","Epoch [32/60], Train Loss: 0.4657, Validation Accuracy: 0.7430\n","Epoch [33/60], Train Loss: 0.4640, Validation Accuracy: 0.7430\n","Epoch [34/60], Train Loss: 0.4622, Validation Accuracy: 0.7448\n","Epoch [35/60], Train Loss: 0.4608, Validation Accuracy: 0.7430\n","Epoch [36/60], Train Loss: 0.4592, Validation Accuracy: 0.7486\n","Epoch [37/60], Train Loss: 0.4583, Validation Accuracy: 0.7495\n","Epoch [38/60], Train Loss: 0.4563, Validation Accuracy: 0.7486\n","Epoch [39/60], Train Loss: 0.4558, Validation Accuracy: 0.7430\n","Epoch [40/60], Train Loss: 0.4540, Validation Accuracy: 0.7486\n","Epoch [41/60], Train Loss: 0.4534, Validation Accuracy: 0.7477\n","Epoch [42/60], Train Loss: 0.4526, Validation Accuracy: 0.7495\n","Epoch [43/60], Train Loss: 0.4507, Validation Accuracy: 0.7561\n","Epoch [44/60], Train Loss: 0.4491, Validation Accuracy: 0.7505\n","Epoch [45/60], Train Loss: 0.4494, Validation Accuracy: 0.7542\n","Epoch [46/60], Train Loss: 0.4475, Validation Accuracy: 0.7533\n","Epoch [47/60], Train Loss: 0.4466, Validation Accuracy: 0.7552\n","Epoch [48/60], Train Loss: 0.4452, Validation Accuracy: 0.7561\n","Epoch [49/60], Train Loss: 0.4441, Validation Accuracy: 0.7570\n","Epoch [50/60], Train Loss: 0.4433, Validation Accuracy: 0.7552\n","Epoch [51/60], Train Loss: 0.4426, Validation Accuracy: 0.7608\n","Epoch [52/60], Train Loss: 0.4410, Validation Accuracy: 0.7542\n","Epoch [53/60], Train Loss: 0.4400, Validation Accuracy: 0.7598\n","Epoch [54/60], Train Loss: 0.4395, Validation Accuracy: 0.7570\n","Epoch [55/60], Train Loss: 0.4389, Validation Accuracy: 0.7627\n","Epoch [56/60], Train Loss: 0.4381, Validation Accuracy: 0.7608\n","Epoch [57/60], Train Loss: 0.4368, Validation Accuracy: 0.7627\n","Epoch [58/60], Train Loss: 0.4362, Validation Accuracy: 0.7617\n","Epoch [59/60], Train Loss: 0.4351, Validation Accuracy: 0.7561\n","Epoch [60/60], Train Loss: 0.4343, Validation Accuracy: 0.7598\n","Using Best Model, Test Accuracy: 0.7664\n"]}],"source":["# Reset the seed to ensure fairness when comparing performances\n","set_seed()\n","\n","# Load Data\n","train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","\n","# Initialize the model\n","model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation='max_mean_pool')\n","model.to(device)\n","# Use BCELoss for binary classification\n","criterion = nn.BCELoss()\n","# Choose optimizer based on trial suggestion\n","optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","# Initialize the early stopper\n","early_stopper = EarlyStopper()\n","\n","# Lists to store loss and accuracy for visualization\n","train_losses = []\n","val_accuracies = []\n","\n","# Training Loop\n","best_val_accuracy = 0\n","best_model = None\n","for epoch in range(num_epochs):\n","    # Train on train set\n","    train_loss = train_loop(train_loader, model, criterion, optimizer)\n","    train_losses.append(train_loss)\n","\n","    # Evaluate on validation set\n","    val_accuracy = test_loop(val_loader, model)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Get the best model\n","    if val_accuracy > best_val_accuracy:\n","      best_val_accuracy = val_accuracy\n","      best_model = copy.deepcopy(model)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    if early_stopper.early_stop(val_accuracy):\n","       print(\"Early stopping triggered!\")\n","       break\n","\n","# Evaluate on test data\n","test_accuracy = test_loop(test_loader, best_model)\n","\n","# Print out the performance\n","print(f\"Using Best Model, Test Accuracy: {test_accuracy:.4f}\") #0.7833"]},{"cell_type":"markdown","metadata":{"id":"CtOTsiuC6Ari"},"source":["# **MHSA (Mean Pooling)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I3fHODY36A7T"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class SimpleBiLSTM(nn.Module):\n","    def __init__(self, embedding_matrix, hidden_size, num_layers, num_attention_heads=8, aggregation='last_hidden_state', freeze_embedding=True):\n","        super(SimpleBiLSTM, self).__init__()\n","        embedding_dim = embedding_matrix.shape[1]\n","\n","        self.aggregation = aggregation\n","        self.hidden_size = hidden_size  # Store hidden size for reference\n","\n","        # Ensure num_attention_heads is compatible with embedding_dim\n","        if embedding_dim % num_attention_heads != 0:\n","            num_attention_heads = max(1, embedding_dim // 64)  # Adjust to nearest factor if needed\n","            #print(f\"Adjusted num_attention_heads to {num_attention_heads} for compatibility with embedding_dim {embedding_dim}.\")\n","\n","\n","        # Embedding layer using a pretrained embedding matrix\n","        self.embedding = nn.Embedding.from_pretrained(\n","            torch.tensor(embedding_matrix, dtype=torch.float32),\n","            padding_idx=0,  # Using 0 for padding index\n","            freeze=freeze_embedding,  # Freeze embeddings\n","        )\n","\n","        # Multi-head self-attention layer\n","        self.mhsa = nn.MultiheadAttention(embedding_dim, num_attention_heads, batch_first=True)\n","\n","        # BiLSTM layer with bidirectional=True\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","        self.dropout = nn.Dropout(0.3)\n","\n","        # Output layer dimensions adjusted for bidirectional LSTM (2 * hidden_size)\n","        self.fc = nn.Linear(300, 1)\n","\n","    def forward(self, x):\n","        # Pass through the embedding layer\n","        embedded = self.embedding(x)\n","\n","        # Create padding mask for the attention layer\n","        device = embedded.device\n","        padding_mask_final = torch.zeros(x.size(0), x.size(1), dtype=torch.bool, device=device)\n","        padding_mask = (x[:, 1:] == self.embedding.padding_idx)\n","        padding_mask_final[:, 1:] = padding_mask\n","\n","        # Apply multi-head self-attention\n","        attention_out, _ = self.mhsa(embedded, embedded, embedded, key_padding_mask=padding_mask_final)\n","\n","        # Forward pass through BiLSTM using attention output as input\n","        h0 = torch.zeros(2 * self.lstm.num_layers, embedded.size(0), self.hidden_size).to(embedded.device)\n","        c0 = torch.zeros(2 * self.lstm.num_layers, embedded.size(0), self.hidden_size).to(embedded.device)\n","        lstm_out, (hn, cn) = self.lstm(attention_out, (h0, c0))\n","\n","        # Aggregation method\n","        if self.aggregation == 'last_hidden_state':\n","            # Concatenate the last hidden states from both directions\n","            out = torch.cat((hn[-2], hn[-1]), dim=1)  # -2 and -1 refer to the last forward and backward states\n","        elif self.aggregation == 'max_mean_pool':\n","            # Mask out padding tokens for pooling\n","            mask = (x != self.embedding.padding_idx).unsqueeze(-1)\n","\n","            # Max pooling\n","            max_pooled_out = lstm_out.masked_fill(~mask, -100000)  # Mask padding tokens with a large negative value\n","            max_pooled_out, _ = torch.max(max_pooled_out, dim=1)\n","\n","            # Mean pooling\n","            mean_pooled_out = lstm_out * mask\n","            summed = mean_pooled_out.sum(dim=1)\n","            valid_counts = mask.sum(dim=1).clamp(min=1)\n","            mean_pooled_out = summed / valid_counts\n","\n","            # Concatenate max and mean pooling outputs\n","            out = torch.cat((max_pooled_out, mean_pooled_out), dim=1)\n","        elif self.aggregation == 'mhsa':\n","\n","            # Mean pooling on the output of MHSA layer\n","            mask = (x != self.embedding.padding_idx).unsqueeze(-1)\n","            attention_out = attention_out * mask\n","            summed = attention_out.sum(dim=1)\n","            valid_counts = mask.sum(dim=1).clamp(min=1)\n","            out = summed / valid_counts\n","\n","        out = self.dropout(out)  # Apply dropout\n","        #print(f\"Dimension before fully connected layer: {out.shape}\")\n","        out = self.fc(out)  # Final fully connected layer\n","\n","        # Sigmoid activation for binary classification\n","        out = torch.sigmoid(out)\n","\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2000650,"status":"ok","timestamp":1730462446194,"user":{"displayName":"Alex Khoo","userId":"03621717103776378058"},"user_tz":-480},"id":"POG59IXZ6FBb","outputId":"2aeab08a-e741-47d0-e2f4-523649f5a04a"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-11-01 11:06:01,234] A new study created in memory with name: no-name-9650c122-8dc0-452d-8d6a-fbb253287375\n","[I 2024-11-01 11:06:36,935] Trial 0 finished with value: 0.773921200750469 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.773921200750469.\n","[I 2024-11-01 11:08:09,101] Trial 1 finished with value: 0.7814258911819888 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.7814258911819888.\n","[I 2024-11-01 11:09:37,715] Trial 2 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.7823639774859287.\n","[I 2024-11-01 11:10:40,990] Trial 3 finished with value: 0.7851782363977486 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.7851782363977486.\n","[I 2024-11-01 11:11:41,287] Trial 4 finished with value: 0.7898686679174484 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:13:54,433] Trial 5 finished with value: 0.7148217636022514 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:14:46,730] Trial 6 finished with value: 0.575046904315197 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:16:26,119] Trial 7 finished with value: 0.5075046904315197 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:17:40,768] Trial 8 finished with value: 0.7317073170731707 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:18:28,021] Trial 9 finished with value: 0.46810506566604126 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:19:16,474] Trial 10 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:20:19,125] Trial 11 finished with value: 0.7851782363977486 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:21:18,913] Trial 12 finished with value: 0.7898686679174484 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:22:18,707] Trial 13 finished with value: 0.7898686679174484 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:23:17,674] Trial 14 finished with value: 0.5056285178236398 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:24:16,877] Trial 15 finished with value: 0.7786116322701688 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:24:47,604] Trial 16 finished with value: 0.50093808630394 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:25:47,149] Trial 17 finished with value: 0.7898686679174484 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:26:46,499] Trial 18 finished with value: 0.7898686679174484 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:27:55,203] Trial 19 finished with value: 0.7814258911819888 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:28:32,199] Trial 20 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:29:31,753] Trial 21 finished with value: 0.7898686679174484 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:30:31,184] Trial 22 finished with value: 0.7898686679174484 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:31:30,603] Trial 23 finished with value: 0.7898686679174484 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:32:30,064] Trial 24 finished with value: 0.7898686679174484 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:33:28,901] Trial 25 finished with value: 0.5056285178236398 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:34:28,232] Trial 26 finished with value: 0.7898686679174484 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:35:27,784] Trial 27 finished with value: 0.7898686679174484 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7898686679174484.\n","[I 2024-11-01 11:36:36,466] Trial 28 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 28 with value: 0.7945590994371482.\n","[I 2024-11-01 11:37:22,063] Trial 29 finished with value: 0.5056285178236398 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 28 with value: 0.7945590994371482.\n","[I 2024-11-01 11:38:09,676] Trial 30 finished with value: 0.7879924953095685 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 28 with value: 0.7945590994371482.\n","[I 2024-11-01 11:39:18,324] Trial 31 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 28 with value: 0.7945590994371482.\n","[I 2024-11-01 11:40:26,897] Trial 32 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 28 with value: 0.7945590994371482.\n","[I 2024-11-01 11:41:35,409] Trial 33 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 28 with value: 0.7945590994371482.\n","[I 2024-11-01 11:42:43,858] Trial 34 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 28 with value: 0.7945590994371482.\n","[I 2024-11-01 11:44:09,449] Trial 35 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 28 with value: 0.7945590994371482.\n","[I 2024-11-01 11:45:20,692] Trial 36 finished with value: 0.7889305816135085 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 28 with value: 0.7945590994371482.\n","[I 2024-11-01 11:47:01,187] Trial 37 finished with value: 0.5037523452157598 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 28 with value: 0.7945590994371482.\n","[I 2024-11-01 11:48:03,237] Trial 38 finished with value: 0.7954971857410882 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 38 with value: 0.7954971857410882.\n","[I 2024-11-01 11:48:43,537] Trial 39 finished with value: 0.5103189493433395 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 38 with value: 0.7954971857410882.\n","[I 2024-11-01 11:50:01,123] Trial 40 finished with value: 0.7954971857410882 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 38 with value: 0.7954971857410882.\n","[I 2024-11-01 11:51:18,780] Trial 41 finished with value: 0.7954971857410882 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 38 with value: 0.7954971857410882.\n","[I 2024-11-01 11:52:36,467] Trial 42 finished with value: 0.7954971857410882 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 38 with value: 0.7954971857410882.\n","[I 2024-11-01 11:53:54,091] Trial 43 finished with value: 0.7954971857410882 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 38 with value: 0.7954971857410882.\n","[I 2024-11-01 11:54:59,375] Trial 44 finished with value: 0.7851782363977486 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 38 with value: 0.7954971857410882.\n","[I 2024-11-01 11:56:15,273] Trial 45 finished with value: 0.5121951219512195 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 38 with value: 0.7954971857410882.\n","[I 2024-11-01 11:57:05,019] Trial 46 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 38 with value: 0.7954971857410882.\n","[I 2024-11-01 11:58:21,273] Trial 47 finished with value: 0.4906191369606004 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 38 with value: 0.7954971857410882.\n","[I 2024-11-01 11:59:27,706] Trial 48 finished with value: 0.7626641651031895 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'Adam'}. Best is trial 38 with value: 0.7954971857410882.\n","[I 2024-11-01 12:00:45,404] Trial 49 finished with value: 0.7954971857410882 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 38 with value: 0.7954971857410882.\n"]}],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Reset the seed for fairness\n","    torch.manual_seed(42)\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers,aggregation='mhsa',freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","# Start the Optuna study\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)\n","\n","#[I 2024-11-01 11:48:03,237] Trial 38 finished with value: 0.7954971857410882 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 38 with value: 0.7954971857410882."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o7B3lIB3H1HN"},"outputs":[],"source":["# The Best Hyperparameters\n","hidden_size = 128  # Hidden size choices\n","num_layers = 3 # Number of RNN layers choices\n","learning_rate = 0.0001  # Learning rate choices\n","batch_size = 128  # Batch size choices\n","num_epochs = 40  # num epoch choices\n","optimizer_name = 'RMSprop'  # Optimizer choices\n","\n","\n","#[I 2024-11-01 11:48:03,237] Trial 38 finished with value: 0.7954971857410882 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 38 with value: 0.7954971857410882."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68774,"status":"ok","timestamp":1730613551913,"user":{"displayName":"Alex Khoo Shien How","userId":"06086813408012082662"},"user_tz":-480},"id":"JqCjxCppt110","outputId":"e2a9d869-dcc7-4bd9-eb14-b4f092f0cabd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/40], Train Loss: 0.6238, Validation Accuracy: 0.7355\n","Epoch [2/40], Train Loss: 0.5073, Validation Accuracy: 0.7523\n","Epoch [3/40], Train Loss: 0.4767, Validation Accuracy: 0.7542\n","Epoch [4/40], Train Loss: 0.4669, Validation Accuracy: 0.7674\n","Epoch [5/40], Train Loss: 0.4610, Validation Accuracy: 0.7749\n","Epoch [6/40], Train Loss: 0.4572, Validation Accuracy: 0.7542\n","Epoch [7/40], Train Loss: 0.4526, Validation Accuracy: 0.7570\n","Epoch [8/40], Train Loss: 0.4499, Validation Accuracy: 0.7749\n","Epoch [9/40], Train Loss: 0.4468, Validation Accuracy: 0.7608\n","Epoch [10/40], Train Loss: 0.4435, Validation Accuracy: 0.7645\n","Epoch [11/40], Train Loss: 0.4425, Validation Accuracy: 0.7739\n","Epoch [12/40], Train Loss: 0.4390, Validation Accuracy: 0.7683\n","Epoch [13/40], Train Loss: 0.4365, Validation Accuracy: 0.7692\n","Epoch [14/40], Train Loss: 0.4360, Validation Accuracy: 0.7683\n","Epoch [15/40], Train Loss: 0.4341, Validation Accuracy: 0.7739\n","Epoch [16/40], Train Loss: 0.4315, Validation Accuracy: 0.7702\n","Epoch [17/40], Train Loss: 0.4302, Validation Accuracy: 0.7720\n","Epoch [18/40], Train Loss: 0.4301, Validation Accuracy: 0.7739\n","Epoch [19/40], Train Loss: 0.4276, Validation Accuracy: 0.7720\n","Epoch [20/40], Train Loss: 0.4262, Validation Accuracy: 0.7730\n","Epoch [21/40], Train Loss: 0.4250, Validation Accuracy: 0.7739\n","Epoch [22/40], Train Loss: 0.4234, Validation Accuracy: 0.7749\n","Epoch [23/40], Train Loss: 0.4224, Validation Accuracy: 0.7720\n","Epoch [24/40], Train Loss: 0.4199, Validation Accuracy: 0.7683\n","Epoch [25/40], Train Loss: 0.4188, Validation Accuracy: 0.7720\n","Epoch [26/40], Train Loss: 0.4182, Validation Accuracy: 0.7777\n","Epoch [27/40], Train Loss: 0.4172, Validation Accuracy: 0.7777\n","Epoch [28/40], Train Loss: 0.4168, Validation Accuracy: 0.7786\n","Epoch [29/40], Train Loss: 0.4139, Validation Accuracy: 0.7795\n","Epoch [30/40], Train Loss: 0.4132, Validation Accuracy: 0.7805\n","Epoch [31/40], Train Loss: 0.4116, Validation Accuracy: 0.7702\n","Epoch [32/40], Train Loss: 0.4094, Validation Accuracy: 0.7786\n","Epoch [33/40], Train Loss: 0.4091, Validation Accuracy: 0.7795\n","Epoch [34/40], Train Loss: 0.4078, Validation Accuracy: 0.7720\n","Epoch [35/40], Train Loss: 0.4058, Validation Accuracy: 0.7739\n","Epoch [36/40], Train Loss: 0.4042, Validation Accuracy: 0.7749\n","Epoch [37/40], Train Loss: 0.4036, Validation Accuracy: 0.7730\n","Epoch [38/40], Train Loss: 0.4007, Validation Accuracy: 0.7664\n","Epoch [39/40], Train Loss: 0.3995, Validation Accuracy: 0.7683\n","Epoch [40/40], Train Loss: 0.3990, Validation Accuracy: 0.7711\n","Using Best Model, Test Accuracy: 0.7795\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1123: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1410.)\n","  result = _VF.lstm(\n"]}],"source":["# Reset the seed to ensure fairness when comparing performances\n","set_seed()\n","\n","# Load Data\n","train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","\n","# Initialize the model\n","model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation='mhsa')\n","model.to(device)\n","# Use BCELoss for binary classification\n","criterion = nn.BCELoss()\n","# Choose optimizer based on trial suggestion\n","optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","# Initialize the early stopper\n","early_stopper = EarlyStopper()\n","\n","# Lists to store loss and accuracy for visualization\n","train_losses = []\n","val_accuracies = []\n","\n","# Training Loop\n","best_val_accuracy = 0\n","best_model = None\n","for epoch in range(num_epochs):\n","    # Train on train set\n","    train_loss = train_loop(train_loader, model, criterion, optimizer)\n","    train_losses.append(train_loss)\n","\n","    # Evaluate on validation set\n","    val_accuracy = test_loop(val_loader, model)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Get the best model\n","    if val_accuracy > best_val_accuracy:\n","      best_val_accuracy = val_accuracy\n","      best_model = copy.deepcopy(model)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    if early_stopper.early_stop(val_accuracy):\n","       print(\"Early stopping triggered!\")\n","       break\n","\n","# Evaluate on test data\n","test_accuracy = test_loop(test_loader, best_model)\n","\n","# Print out the performance\n","print(f\"Using Best Model, Test Accuracy: {test_accuracy:.4f}\") #0.7795"]},{"cell_type":"markdown","metadata":{"id":"G_PNNcAue8fx"},"source":["# Attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KlNp7OCMe-7O"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class Attention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Attention, self).__init__()\n","        self.attention_weights = nn.Linear(hidden_size * 2, 1, bias=False)  # *2 for bidirectional LSTM\n","\n","    def forward(self, lstm_out):\n","        # lstm_out shape: (batch_size, seq_len, hidden_size * 2)\n","\n","        # Compute attention scores\n","        attn_scores = self.attention_weights(lstm_out)  # (batch_size, seq_len, 1)\n","        attn_weights = torch.softmax(attn_scores, dim=1)  # Softmax over seq_len dimension\n","\n","        # Compute context vector as weighted sum of lstm_out\n","        context_vector = torch.sum(attn_weights * lstm_out, dim=1)  # (batch_size, hidden_size * 2)\n","\n","        return context_vector\n","\n","\n","class SimpleBiLSTM(nn.Module):\n","    def __init__(self, embedding_matrix, hidden_size, num_layers, aggregation='last_hidden_state', freeze_embedding=True):\n","        super(SimpleBiLSTM, self).__init__()\n","        embedding_dim = embedding_matrix.shape[1]\n","\n","        self.aggregation = aggregation\n","        self.hidden_size = hidden_size\n","\n","        # Embedding layer\n","        self.embedding = nn.Embedding.from_pretrained(\n","            torch.tensor(embedding_matrix, dtype=torch.float32),\n","            padding_idx=0,\n","            freeze=freeze_embedding,\n","        )\n","\n","        # BiLSTM layer with bidirectional=True\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","\n","        # Attention layer (optional, if needed)\n","        self.attention = Attention(hidden_size) if aggregation == 'attention' else None\n","\n","        # Batch normalization layers\n","        self.batch_norm_lstm = nn.BatchNorm1d(hidden_size * 2)  # For BiLSTM output (bidirectional)\n","\n","        # Set batch normalization for fully connected layer based on aggregation method\n","        if aggregation == 'max_mean_pool':\n","            self.batch_norm_fc = nn.BatchNorm1d(hidden_size * 4)  # For concatenated max and mean pooling\n","            self.fc = nn.Linear(hidden_size * 4, 1)  # Adjusted output size for concatenated max and mean pooling\n","        else:\n","            self.batch_norm_fc = nn.BatchNorm1d(hidden_size * 2)\n","            self.fc = nn.Linear(hidden_size * 2, 1)\n","\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","\n","        # Initialize hidden and cell states for BiLSTM\n","        h0 = torch.zeros(2 * self.lstm.num_layers, embedded.size(0), self.hidden_size).to(embedded.device)\n","        c0 = torch.zeros(2 * self.lstm.num_layers, embedded.size(0), self.hidden_size).to(embedded.device)\n","\n","        # Forward pass through BiLSTM\n","        out, (hn, cn) = self.lstm(embedded, (h0, c0))\n","        out = self.batch_norm_lstm(out.transpose(1, 2)).transpose(1, 2)\n","\n","        # Aggregation\n","        if self.aggregation == 'last_hidden_state':\n","            out = torch.cat((hn[-2], hn[-1]), dim=1)\n","        elif self.aggregation == 'max_mean_pool':\n","            mask = (x != self.embedding.padding_idx).unsqueeze(-1)\n","            max_pooled_out = out.masked_fill(~mask, -1e9).max(dim=1).values\n","            mean_pooled_out = out.sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n","            out = torch.cat((max_pooled_out, mean_pooled_out), dim=1)\n","        elif self.aggregation == 'attention':\n","            out = self.attention(out)\n","\n","        # Apply batch normalization before the fully connected layer\n","        out = self.batch_norm_fc(out)\n","\n","        # Dropout and final fully connected layer\n","        out = self.dropout(out)\n","        out = self.fc(out).squeeze(1)\n","\n","        # Sigmoid activation for binary classification\n","        out = torch.sigmoid(out)\n","\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4223220,"status":"ok","timestamp":1730551601091,"user":{"displayName":"ALEX KHOO SHIEN HOW","userId":"09575417203520826139"},"user_tz":-480},"id":"mRWLCJBNoDAX","outputId":"08b6d010-2376-4968-a8d2-49ae737b1a4d"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-11-02 11:36:17,774] A new study created in memory with name: no-name-efe1c6eb-c5fc-4896-aa1a-b1d449d8f6ae\n","[I 2024-11-02 11:37:20,088] Trial 0 finished with value: 0.6529080675422139 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.6529080675422139.\n","[I 2024-11-02 11:38:20,904] Trial 1 finished with value: 0.5 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.6529080675422139.\n","[I 2024-11-02 11:42:49,226] Trial 2 finished with value: 0.5028142589118199 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.6529080675422139.\n","[I 2024-11-02 11:47:03,151] Trial 3 finished with value: 0.7363977485928705 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.7363977485928705.\n","[I 2024-11-02 11:47:50,337] Trial 4 finished with value: 0.7786116322701688 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.7786116322701688.\n","[I 2024-11-02 11:48:45,420] Trial 5 finished with value: 0.7908067542213884 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 5 with value: 0.7908067542213884.\n","[I 2024-11-02 11:49:27,969] Trial 6 finished with value: 0.7889305816135085 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 5 with value: 0.7908067542213884.\n","[I 2024-11-02 11:49:56,334] Trial 7 finished with value: 0.7842401500938087 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'SGD'}. Best is trial 5 with value: 0.7908067542213884.\n","[I 2024-11-02 11:50:50,984] Trial 8 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 8 with value: 0.7945590994371482.\n","[I 2024-11-02 11:54:25,927] Trial 9 finished with value: 0.7889305816135085 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 8 with value: 0.7945590994371482.\n","[I 2024-11-02 11:55:22,493] Trial 10 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 8 with value: 0.7945590994371482.\n","[I 2024-11-02 11:56:18,607] Trial 11 finished with value: 0.775797373358349 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 8 with value: 0.7945590994371482.\n","[I 2024-11-02 11:57:00,114] Trial 12 finished with value: 0.7861163227016885 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 8 with value: 0.7945590994371482.\n","[I 2024-11-02 11:57:54,996] Trial 13 finished with value: 0.7870544090056285 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 8 with value: 0.7945590994371482.\n","[I 2024-11-02 11:58:12,287] Trial 14 finished with value: 0.7532833020637899 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 8 with value: 0.7945590994371482.\n","[I 2024-11-02 11:59:09,178] Trial 15 finished with value: 0.774859287054409 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'Adam'}. Best is trial 8 with value: 0.7945590994371482.\n","[I 2024-11-02 12:00:03,874] Trial 16 finished with value: 0.773921200750469 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 8 with value: 0.7945590994371482.\n","[I 2024-11-02 12:00:45,154] Trial 17 finished with value: 0.7954971857410882 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 17 with value: 0.7954971857410882.\n","[I 2024-11-02 12:02:16,344] Trial 18 finished with value: 0.7917448405253283 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 17 with value: 0.7954971857410882.\n","[I 2024-11-02 12:02:51,458] Trial 19 finished with value: 0.7682926829268293 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 17 with value: 0.7954971857410882.\n","[I 2024-11-02 12:03:42,873] Trial 20 finished with value: 0.5 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 17 with value: 0.7954971857410882.\n","[I 2024-11-02 12:05:14,053] Trial 21 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 17 with value: 0.7954971857410882.\n","[I 2024-11-02 12:06:45,165] Trial 22 finished with value: 0.7964352720450282 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 22 with value: 0.7964352720450282.\n","[I 2024-11-02 12:08:16,648] Trial 23 finished with value: 0.7879924953095685 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 22 with value: 0.7964352720450282.\n","[I 2024-11-02 12:09:47,738] Trial 24 finished with value: 0.7964352720450282 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 22 with value: 0.7964352720450282.\n","[I 2024-11-02 12:10:57,440] Trial 25 finished with value: 0.7898686679174484 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 22 with value: 0.7964352720450282.\n","[I 2024-11-02 12:13:51,562] Trial 26 finished with value: 0.7898686679174484 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 22 with value: 0.7964352720450282.\n","[I 2024-11-02 12:15:22,890] Trial 27 finished with value: 0.7851782363977486 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 22 with value: 0.7964352720450282.\n","[I 2024-11-02 12:16:28,395] Trial 28 finished with value: 0.7795497185741088 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 22 with value: 0.7964352720450282.\n","[I 2024-11-02 12:16:52,628] Trial 29 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 22 with value: 0.7964352720450282.\n","[I 2024-11-02 12:17:53,184] Trial 30 finished with value: 0.7786116322701688 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 22 with value: 0.7964352720450282.\n","[I 2024-11-02 12:18:47,993] Trial 31 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 22 with value: 0.7964352720450282.\n","[I 2024-11-02 12:20:00,803] Trial 32 finished with value: 0.7851782363977486 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 22 with value: 0.7964352720450282.\n","[I 2024-11-02 12:21:26,486] Trial 33 finished with value: 0.5 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 22 with value: 0.7964352720450282.\n","[I 2024-11-02 12:24:20,057] Trial 34 finished with value: 0.7823639774859287 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 22 with value: 0.7964352720450282.\n","[I 2024-11-02 12:25:14,771] Trial 35 finished with value: 0.8067542213883677 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 35 with value: 0.8067542213883677.\n","[I 2024-11-02 12:26:45,929] Trial 36 finished with value: 0.801125703564728 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 35 with value: 0.8067542213883677.\n","[I 2024-11-02 12:28:16,950] Trial 37 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 35 with value: 0.8067542213883677.\n","[I 2024-11-02 12:29:48,378] Trial 38 finished with value: 0.7833020637898687 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 35 with value: 0.8067542213883677.\n","[I 2024-11-02 12:32:37,650] Trial 39 finished with value: 0.7166979362101313 and parameters: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'SGD'}. Best is trial 35 with value: 0.8067542213883677.\n","[I 2024-11-02 12:33:27,558] Trial 40 finished with value: 0.7954971857410882 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 35 with value: 0.8067542213883677.\n","[I 2024-11-02 12:34:40,278] Trial 41 finished with value: 0.797373358348968 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 35 with value: 0.8067542213883677.\n","[I 2024-11-02 12:35:52,975] Trial 42 finished with value: 0.8067542213883677 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 35 with value: 0.8067542213883677.\n","[I 2024-11-02 12:37:05,841] Trial 43 finished with value: 0.7945590994371482 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 35 with value: 0.8067542213883677.\n","[I 2024-11-02 12:38:18,767] Trial 44 finished with value: 0.7926829268292683 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 35 with value: 0.8067542213883677.\n","[I 2024-11-02 12:39:31,578] Trial 45 finished with value: 0.7936210131332082 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 35 with value: 0.8067542213883677.\n","[I 2024-11-02 12:43:03,135] Trial 46 finished with value: 0.7917448405253283 and parameters: {'hidden_size': 256, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'Adam'}. Best is trial 35 with value: 0.8067542213883677.\n","[I 2024-11-02 12:44:04,395] Trial 47 finished with value: 0.6425891181988743 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 128, 'num_epochs': 40, 'optimizer': 'SGD'}. Best is trial 35 with value: 0.8067542213883677.\n","[I 2024-11-02 12:45:17,385] Trial 48 finished with value: 0.7917448405253283 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 35 with value: 0.8067542213883677.\n","[I 2024-11-02 12:46:40,612] Trial 49 finished with value: 0.7776735459662288 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 40, 'optimizer': 'RMSprop'}. Best is trial 35 with value: 0.8067542213883677.\n"]}],"source":["import optuna\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","    # Hyperparameters\n","    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n","    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n","    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    num_epochs = trial.suggest_categorical('num_epochs', [30, 40, 50])\n","    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n","\n","    # Create data loaders\n","    train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","    # Initialize the model\n","    model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers,aggregation='attention',freeze_embedding=False).to(device)\n","    criterion = nn.BCELoss()  # Use BCELoss for binary classification\n","\n","    # Choose optimizer based on trial suggestion\n","    optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","\n","    # Evaluation function\n","    def evaluate_model(data_loader):\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in data_loader:\n","                outputs = model(inputs)\n","                predicted = (outputs.squeeze() > 0.5).float()\n","                total += labels.size(0)\n","                correct += (predicted == labels.float()).sum().item()\n","        accuracy = correct / total\n","        return accuracy\n","\n","    # Training Loop\n","    max_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        val_accuracy = evaluate_model(val_loader)\n","        if val_accuracy > max_accuracy:\n","            max_accuracy = val_accuracy\n","\n","    # Return the validation accuracy as the objective value to optimize\n","    return max_accuracy\n","\n","# Start the Optuna study\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=50)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PUmh1tRjqYFB"},"outputs":[],"source":["#[I 2024-11-02 12:25:14,771] Trial 35 finished with value: 0.8067542213883677 and parameters: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 1e-05, 'batch_size': 64, 'num_epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 35 with value: 0.8067542213883677.\n","# The Best Hyperparameters\n","hidden_size = 256  # Hidden size choices\n","num_layers = 1 # Number of RNN layers choices\n","learning_rate = 1e-05  # Learning rate choices\n","batch_size = 64  # Batch size choices\n","num_epochs = 30  # num epoch choices\n","optimizer_name = 'RMSprop'  # Optimizer choices\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47344,"status":"ok","timestamp":1730553020433,"user":{"displayName":"ALEX KHOO SHIEN HOW","userId":"09575417203520826139"},"user_tz":-480},"id":"NALAxTcc_mbi","outputId":"885585b7-1bdb-48bd-9e0e-287a37ff7fdd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/30], Train Loss: 0.6503, Validation Accuracy: 0.6839\n","Epoch [2/30], Train Loss: 0.5733, Validation Accuracy: 0.7270\n","Epoch [3/30], Train Loss: 0.5324, Validation Accuracy: 0.7402\n","Epoch [4/30], Train Loss: 0.5123, Validation Accuracy: 0.7448\n","Epoch [5/30], Train Loss: 0.4927, Validation Accuracy: 0.7467\n","Epoch [6/30], Train Loss: 0.4829, Validation Accuracy: 0.7514\n","Epoch [7/30], Train Loss: 0.4762, Validation Accuracy: 0.7580\n","Epoch [8/30], Train Loss: 0.4705, Validation Accuracy: 0.7552\n","Epoch [9/30], Train Loss: 0.4618, Validation Accuracy: 0.7561\n","Epoch [10/30], Train Loss: 0.4537, Validation Accuracy: 0.7589\n","Epoch [11/30], Train Loss: 0.4469, Validation Accuracy: 0.7636\n","Epoch [12/30], Train Loss: 0.4417, Validation Accuracy: 0.7627\n","Epoch [13/30], Train Loss: 0.4368, Validation Accuracy: 0.7627\n","Epoch [14/30], Train Loss: 0.4336, Validation Accuracy: 0.7636\n","Epoch [15/30], Train Loss: 0.4279, Validation Accuracy: 0.7636\n","Epoch [16/30], Train Loss: 0.4249, Validation Accuracy: 0.7692\n","Epoch [17/30], Train Loss: 0.4218, Validation Accuracy: 0.7674\n","Epoch [18/30], Train Loss: 0.4199, Validation Accuracy: 0.7655\n","Epoch [19/30], Train Loss: 0.4131, Validation Accuracy: 0.7664\n","Epoch [20/30], Train Loss: 0.4117, Validation Accuracy: 0.7692\n","Epoch [21/30], Train Loss: 0.4083, Validation Accuracy: 0.7730\n","Epoch [22/30], Train Loss: 0.4045, Validation Accuracy: 0.7702\n","Epoch [23/30], Train Loss: 0.4023, Validation Accuracy: 0.7739\n","Epoch [24/30], Train Loss: 0.3974, Validation Accuracy: 0.7702\n","Epoch [25/30], Train Loss: 0.3961, Validation Accuracy: 0.7749\n","Epoch [26/30], Train Loss: 0.3962, Validation Accuracy: 0.7702\n","Epoch [27/30], Train Loss: 0.3937, Validation Accuracy: 0.7720\n","Epoch [28/30], Train Loss: 0.3899, Validation Accuracy: 0.7702\n","Epoch [29/30], Train Loss: 0.3863, Validation Accuracy: 0.7749\n","Epoch [30/30], Train Loss: 0.3845, Validation Accuracy: 0.7739\n","Using Best Model, Test Accuracy: 0.7964\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1123: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1410.)\n","  result = _VF.lstm(\n"]}],"source":["# Reset the seed to ensure fairness when comparing performances\n","set_seed()\n","\n","# Load Data\n","train_loader, val_loader, test_loader = dataloader_factory(X_train_sequence, X_val_sequence, X_test_sequence, Y_train, Y_val, Y_test, batch_size, device)\n","\n","# Initialize the model\n","model = SimpleBiLSTM(embedding_matrix, hidden_size, num_layers, aggregation='attention')\n","model.to(device)\n","# Use BCELoss for binary classification\n","criterion = nn.BCELoss()\n","# Choose optimizer based on trial suggestion\n","optimizer = optimizer_factory(optimizer_name, model, learning_rate)\n","# Initialize the early stopper\n","early_stopper = EarlyStopper()\n","\n","# Lists to store loss and accuracy for visualization\n","train_losses = []\n","val_accuracies = []\n","\n","# Training Loop\n","best_val_accuracy = 0\n","best_model = None\n","for epoch in range(num_epochs):\n","    # Train on train set\n","    train_loss = train_loop(train_loader, model, criterion, optimizer)\n","    train_losses.append(train_loss)\n","\n","    # Evaluate on validation set\n","    val_accuracy = test_loop(val_loader, model)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Get the best model\n","    if val_accuracy > best_val_accuracy:\n","      best_val_accuracy = val_accuracy\n","      best_model = copy.deepcopy(model)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    if early_stopper.early_stop(val_accuracy):\n","       print(\"Early stopping triggered!\")\n","       break\n","\n","# Evaluate on test data\n","test_accuracy = test_loop(test_loader, best_model)\n","\n","# Print out the performance\n","print(f\"Using Best Model, Test Accuracy: {test_accuracy:.4f}\") #0.7964"]},{"cell_type":"markdown","source":["# Simple Embedding Matrix Method"],"metadata":{"id":"nt0R_TnsP5g4"}},{"cell_type":"code","source":["def create_embedding_matrix(w2v_model, word2index, embedding_dim=300):\n","    \"\"\"\n","    Creates an embedding matrix based on the word2index mapping and Word2Vec model.\n","\n","    Args:\n","    - w2v_model: Pre-trained Word2Vec model.\n","    - word2index: Dictionary mapping words to their indices.\n","    - embedding_dim: Dimension of the Word2Vec word vectors (default: 300).\n","\n","    Returns:\n","    - embedding_matrix: Embedding matrix where each row corresponds to the vector of a word in the vocabulary.\n","    \"\"\"\n","    vocab_size = len(word2index)  # Number of words in the training vocabulary\n","\n","    # Initialize the embedding matrix with zeros (or any other value)\n","    embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n","\n","    for word, idx in word2index.items():\n","        if word in w2v_model:\n","            # If the word exists in the Word2Vec model, use its pre-trained embedding\n","            embedding_matrix[idx] = w2v_model[word]\n","        else:\n","            # For out-of-vocabulary (OOV) words, initialize a random vector\n","            embedding_matrix[idx] = np.random.normal(size=(embedding_dim,))\n","\n","    return embedding_matrix\n","\n","embedding_dim = w2v_model.vector_size  # Get the dimension of the word vectors in Word2Vec\n","embedding_matrix = create_embedding_matrix(w2v_model, word2index, embedding_dim)\n","print(f\"embedding_matrix shape: {embedding_matrix.shape}\")"],"metadata":{"id":"Vf23ugoMP5yL"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["EsVKHy3K1njx","jTxJ0zFgAzou","SukwM-3EA4ix","_aoHg0KYx4te","rBeJ7A9AjUj9","JizMIPPhiaju"],"gpuType":"T4","provenance":[{"file_id":"1qfasDC4gsXgEIuxzfCnubXhGbAPGLXxy","timestamp":1731065890825}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"02ad52ed3bbc4ba6b8750aa4a2565c0c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"04f6c6ba85f44387953a1c662424ad82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"070f7012059d46e9b3dc250405a354b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0b8a07c019ef4bd4a2c18e87f328a5d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3773abf300dd42a9a1d1f94989aa72ab","placeholder":"​","style":"IPY_MODEL_be75af6ddff54a0088da1453ebc75c7b","value":" 7.46k/7.46k [00:00&lt;00:00, 159kB/s]"}},"0dff2f4e567a4fb398b56c16e199b3f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f73887fedf4402c943b8c8a050bd0a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1463ad0101de4ebdbbc75aefebc4f423":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20e03d4e40d34ff2a6cea59eb7c2281d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"246bc2c957e44828b2dbd0255a5591d0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2584ee548125431a9ec7a60ee385d342":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a90bb6951dd4da5a84a4b26b737fb84":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"366ebc5f241c4cf38406ab9c93280b80":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3717b79035744bfc9bbdbd5f9df5380f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3773abf300dd42a9a1d1f94989aa72ab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"395124e7a7c54beb89a98fd542bdb627":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ca89dd8e3e341e38e83aa642fe77acb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ec88fc8995249f99e24a6a7b8399102":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab4e414fd6194f7ab76712fd71eba09e","placeholder":"​","style":"IPY_MODEL_f6470914dfc7463b90664712d3b946c1","value":" 1066/1066 [00:00&lt;00:00, 11587.08 examples/s]"}},"4111f13dccc74ddfb66c416dd0ce9fd1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42a4ea758bcc4983b28a8d4140879616":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"44a6473408e246b990c1e377a3703c03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4621ac9eb58f462686688936bcbdea53":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bd5ec82a5ee646659add5e9ba1b0e76a","IPY_MODEL_5fc163d9e89e4baeb85d3aa16257c42b","IPY_MODEL_7ac82e55bb4e4868b67b2177b42d28ba"],"layout":"IPY_MODEL_c2164308609d4decacd82fc888625780"}},"478045f86d604fc1bac9d267ca5ad7f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47ea00e33aba43b0bc906adf4256ef8b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f61b2a37c294a5a9bbf3ed0ede587ab","placeholder":"​","style":"IPY_MODEL_f34f7432ec4d459cb03cacdc85db437a","value":"train.parquet: 100%"}},"4c060e5b775d49cf97fa43d8cdacfc1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1463ad0101de4ebdbbc75aefebc4f423","placeholder":"​","style":"IPY_MODEL_3ca89dd8e3e341e38e83aa642fe77acb","value":"README.md: 100%"}},"4ffefb0779fd47c9935887b37afc0c08":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55353da4c2a141dfa5c978d307396f1f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"558d6df7b71d44ae880d518c99a1f1f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff389610170749e2aaaf7bd3fa9c35e8","max":92206,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0f73887fedf4402c943b8c8a050bd0a7","value":92206}},"57cebd549e394a7387d4e6f8d067bf51":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59f4af2615c5408d8b06838747facdfc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f61b2a37c294a5a9bbf3ed0ede587ab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fc163d9e89e4baeb85d3aa16257c42b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3717b79035744bfc9bbdbd5f9df5380f","max":90001,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ee68977fb5634e5d95004a7689686e31","value":90001}},"60a8db739f3442a48e69e41fa2cc02a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62dd83f5fdff457dafe66216fa897934":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"645be1bd8bb54812a2df0ecb447882cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d8bd3534cbe4bcb9f90394526df6862","IPY_MODEL_558d6df7b71d44ae880d518c99a1f1f5","IPY_MODEL_ac59d255e29049868449c3413bff4d2d"],"layout":"IPY_MODEL_cca0901277064588b527facd74a53467"}},"668aea027e4a4e5382b89464ea88104c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2584ee548125431a9ec7a60ee385d342","placeholder":"​","style":"IPY_MODEL_ef5b7c15d9454955b21c7387510d54d6","value":" 699k/699k [00:00&lt;00:00, 11.1MB/s]"}},"6875518f241f4a31be532e63a34f5f5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ceefff4198314b9b9ff797fca58919e0","max":1066,"min":0,"orientation":"horizontal","style":"IPY_MODEL_42a4ea758bcc4983b28a8d4140879616","value":1066}},"68b74e4f668f46fd818835af2457571f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d8bd3534cbe4bcb9f90394526df6862":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf9d82a268cc40f2bad4eb07b0e92252","placeholder":"​","style":"IPY_MODEL_04f6c6ba85f44387953a1c662424ad82","value":"test.parquet: 100%"}},"7085e3d659ac41809963e05d35aeb41d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"766dd2fbf5f1410ca4011be0f6f737a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57cebd549e394a7387d4e6f8d067bf51","placeholder":"​","style":"IPY_MODEL_478045f86d604fc1bac9d267ca5ad7f1","value":" 8530/8530 [00:00&lt;00:00, 74795.10 examples/s]"}},"769bc507904044f9ad42c776573d1237":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77ea8f679b64453190d939de6d2fe81c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ac82e55bb4e4868b67b2177b42d28ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55353da4c2a141dfa5c978d307396f1f","placeholder":"​","style":"IPY_MODEL_395124e7a7c54beb89a98fd542bdb627","value":" 90.0k/90.0k [00:00&lt;00:00, 1.89MB/s]"}},"7cf56f239dcc40f29a1f01b335a90d58":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_366ebc5f241c4cf38406ab9c93280b80","max":7457,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93320f7e8a864d5bb5aa78dfa42597fa","value":7457}},"8248e0447764401a9851e40e3c626354":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c34e46a333724e759604b94a2ae19997","placeholder":"​","style":"IPY_MODEL_59f4af2615c5408d8b06838747facdfc","value":" 1066/1066 [00:00&lt;00:00, 15395.49 examples/s]"}},"86ac5db6431340c8a6e74bb969043ae3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_20e03d4e40d34ff2a6cea59eb7c2281d","max":8530,"min":0,"orientation":"horizontal","style":"IPY_MODEL_02ad52ed3bbc4ba6b8750aa4a2565c0c","value":8530}},"93320f7e8a864d5bb5aa78dfa42597fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"95ac2d742074490d85094aaeafbc41d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d3613dc7a94d4fbda121d28e255424c5","IPY_MODEL_86ac5db6431340c8a6e74bb969043ae3","IPY_MODEL_766dd2fbf5f1410ca4011be0f6f737a3"],"layout":"IPY_MODEL_77ea8f679b64453190d939de6d2fe81c"}},"976816239b5342de97846df39a66294c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab4e414fd6194f7ab76712fd71eba09e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac59d255e29049868449c3413bff4d2d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_246bc2c957e44828b2dbd0255a5591d0","placeholder":"​","style":"IPY_MODEL_f78ebff877a4450c968080425ab05183","value":" 92.2k/92.2k [00:00&lt;00:00, 1.66MB/s]"}},"b0d73abc371d4d9b98412535af6af040":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b39178f5784b4f67a174e5c957c07673":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b81d898e794743ea82e549581da457ae","IPY_MODEL_6875518f241f4a31be532e63a34f5f5e","IPY_MODEL_8248e0447764401a9851e40e3c626354"],"layout":"IPY_MODEL_60a8db739f3442a48e69e41fa2cc02a0"}},"b81d898e794743ea82e549581da457ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ffefb0779fd47c9935887b37afc0c08","placeholder":"​","style":"IPY_MODEL_4111f13dccc74ddfb66c416dd0ce9fd1","value":"Generating validation split: 100%"}},"bcf1bc6748d14056beecb71c395bc97e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_62dd83f5fdff457dafe66216fa897934","max":1066,"min":0,"orientation":"horizontal","style":"IPY_MODEL_070f7012059d46e9b3dc250405a354b8","value":1066}},"bd5ec82a5ee646659add5e9ba1b0e76a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a90bb6951dd4da5a84a4b26b737fb84","placeholder":"​","style":"IPY_MODEL_c1c906e0106746fbaf921178ea8cbc66","value":"validation.parquet: 100%"}},"be75af6ddff54a0088da1453ebc75c7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0e216c195e347cd97f395a66ade2dc3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_47ea00e33aba43b0bc906adf4256ef8b","IPY_MODEL_ffcb6889ef8749ce8db231526ff088ef","IPY_MODEL_668aea027e4a4e5382b89464ea88104c"],"layout":"IPY_MODEL_7085e3d659ac41809963e05d35aeb41d"}},"c1c906e0106746fbaf921178ea8cbc66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2164308609d4decacd82fc888625780":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c34e46a333724e759604b94a2ae19997":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca26c7d60b8340619b72dcc095234522":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4c060e5b775d49cf97fa43d8cdacfc1c","IPY_MODEL_7cf56f239dcc40f29a1f01b335a90d58","IPY_MODEL_0b8a07c019ef4bd4a2c18e87f328a5d6"],"layout":"IPY_MODEL_b0d73abc371d4d9b98412535af6af040"}},"cca0901277064588b527facd74a53467":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ceefff4198314b9b9ff797fca58919e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf9d82a268cc40f2bad4eb07b0e92252":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3613dc7a94d4fbda121d28e255424c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_976816239b5342de97846df39a66294c","placeholder":"​","style":"IPY_MODEL_df33277871204e6a9a4a3ccba3ca8045","value":"Generating train split: 100%"}},"df33277871204e6a9a4a3ccba3ca8045":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed549a4bb4784f72827df85c5d290be6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44a6473408e246b990c1e377a3703c03","placeholder":"​","style":"IPY_MODEL_68b74e4f668f46fd818835af2457571f","value":"Generating test split: 100%"}},"ee68977fb5634e5d95004a7689686e31":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ef5b7c15d9454955b21c7387510d54d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f34f7432ec4d459cb03cacdc85db437a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6470914dfc7463b90664712d3b946c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f78bfa03d5f04ad397050e25cf07246f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f78ebff877a4450c968080425ab05183":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7c3d20305014a6e9745276f02d396c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ed549a4bb4784f72827df85c5d290be6","IPY_MODEL_bcf1bc6748d14056beecb71c395bc97e","IPY_MODEL_3ec88fc8995249f99e24a6a7b8399102"],"layout":"IPY_MODEL_769bc507904044f9ad42c776573d1237"}},"ff389610170749e2aaaf7bd3fa9c35e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffcb6889ef8749ce8db231526ff088ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dff2f4e567a4fb398b56c16e199b3f8","max":698845,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f78bfa03d5f04ad397050e25cf07246f","value":698845}}}}},"nbformat":4,"nbformat_minor":0}